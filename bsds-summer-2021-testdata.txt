 Building Scalable
Distributed Systems

  Ian Gorton

   Copyright  2021 Ian Gorton
   All rights reserved.
   ISBN:

DEDICATION

   Insert dedication text here. Insert dedication text here. Insert dedication text here. Insert
dedication text here. Insert dedication text here. Insert dedication text here. Insert dedication
text here. Insert dedication text here. Insert dedication text here. Insert dedication text here.

TABLE OF CONTENTS
   Introduction to Scalable Systems	10
   What is Scalability?	10
   System scale in early 2020s: Examples	12
   How did we get here? A short history of system growth	13
   The 1980s	13
   1990-1995	14
   1996-2000	14
   2000-2006	14
   2007-2020 (today)	15
   Scalability Basic Design Principles	15
   Scalability and Costs	17
   Summary	19
   References	19
   Distributed Systems Architectures: A Whirlwind Tour	20
   Basic System Architecture	20
   Scale Out	22
   Scaling the Database with Caching	23
   Distributing the Database	25
   Multiple Processing Tiers	26
   Increasing Responsiveness	28
   Summary and Further Reading	30
   An Overview of Concurrent Systems	31
   Why Concurrency?	31
   Threads in Java	33
   Order of Thread Execution	35
   Problems with Thread  Race Conditions	36
   Problems with Thread  Deadlock	39
   Thread States	43
   Thread Coordination	44
   Thread Pools	47
   Barrier Synchronization	49
   Thread-Safe Collections	51
   Summary and Further Reading	52
   Exercises	52
   Distributed Systems Fundamentals	54
   Communications Basics	54
   Communications Hardware	55
   Communications Software	57
   Remote Method Invocation	60
   Partial Failures	64
   Consensus in Distributed Systems	68
   Time in Distributed Systems	70
   Summary and Further Reading	72
   Application Services	73
   Service Design	73
   Application Programming Interface (API)	73
   Designing Services	75
   State Management	77
   Applications Servers	80
   Horizontal Scaling	82
   Load Balancing	83
   Load Distribution Policies	84
   Health Monitoring	84
   Elasticity	84
   Session Affinity	85
   Summary and Further Reading	87
   Caching	88
   Application Caching	88
   Web Caching	92
   Cache-Control	93
   Expires and Last-Modified	93
   Etag	93
   Summary and Further Reading	95

ACKNOWLEDGMENTS

   Insert acknowledgments text here. Insert acknowledgments text here. Insert
acknowledgments text here. Insert acknowledgments text here. Insert acknowledgments text
here. Insert acknowledgments text here. Insert acknowledgments text here. Insert
acknowledgments text here. Insert acknowledgments text here. Insert acknowledgments text
here.

CHAPTER 1
__________________________
Introduction to Scalable Systems
   The last 20 years have seen unprecedented growth in the size, complexity and capacity of
software systems. This rate of growth is hardly likely to slow down in the next 20 years  what
these future systems will look like is close to unimaginable right now. The one thing we can
guarantee is that more and more software systems will need to be built with constant growth -
more requests, more data, more analysis - as a primary design driver.
   Scalable is the term used in software engineering to describe software systems that can
accommodate growth. In this chapter we will explore what precisely is meant by the ability to
scale  known, not surprisingly, as scalability. Well also describe a few examples that put hard
numbers on the capabilities and characteristics of contemporary applications and give a brief
history of the origins of the massive systems we routinely build today. Finally, we will describe
two general principles for achieving scalability that will recur in various forms throughout the
rest of this book and examine the indelible link between scalability and cost.
What is Scalability?
   Intuitively, scalability is a pretty straightforward concept. If we ask Wikipedia for a
definition, it tells us scalability is the property of a system to handle a growing amount of work
by adding resources to the system. We all know how we scale a highway system  we add more
traffic lanes so it can handle a greater number of vehicles. Some of my favorite people know
how to scale beer production  they add more capacity in terms of the number and size of
brewing vessels, the number of staff to perform and manage the brewing process, and the
number of kegs they can fill with tasty fresh brews. Think of any physical system  a transit
system, an airport, elevators in a building  and how we increase capacity is pretty obvious.
   Unlike physical systems, software is somewhat amorphous. It is not something you can point
at, see, touch, feel, and get a sense of how it behaves internally from external observation. Its a
digital artifact. At its core, the stream of 1s and 0s that make up executable code and data are
hard for anyone to tell apart. So, what does scalability mean in terms of a software system?
   Put very simply, and without getting into definition wars, scalability defines a software
systems capability to handle growth in some dimension of its operations. Examples of
operational dimensions are:

*	the number of simultaneous user or external (e.g. sensor) requests a system can process
*	the amount of data a system can effectively process and manage
*	the value that can be derived from the data a system stores

   For example, imagine a major supermarket chain is rapidly opening new stores and
increasing the number of self-checkout kiosks in every store. This requires the core supermarket
software systems to:

*	Handle increased volume from item sale scanning without decreased response time.
Instantaneous responses to item scans are necessary to keep customers happy.
*	Process and store the greater data volumes generated from increased sales. This data is
needed for inventory management, accounting, planning and likely many other
functions.
*	Derive real-time (e.g. hourly) sales data summaries from each store, region and country
and compare to historical trends. This trend data can help highlight unusual events in
regions (e.g. unexpected weather conditions, large crowds at events, etc.) and help the
stores affected quickly respond.
*	Evolve the stock ordering prediction subsystem to be able to correctly anticipate sales
(and hence the need for stock reordering) as the number of stores and customers grow

   These dimensions are effectively the scalability requirements of a system. If, over a year, the
supermarket chain opens 100 new stores and grows sales by 400 times (some of the new stores
are big!), then the software system needs to scale to provide the necessary processing capacity to
enable the supermarket to operate efficiently. If the systems dont scale, we could lose sales as
customers are unhappy. We might hold stock that will not be sold quickly, increasing costs. We
might miss opportunities to increase sales by responding to local circumstances with special
offerings. All these reduce customer satisfaction and profits. None are good for business.
   Successfully scaling is therefore crucial for our imaginary supermarkets business growth, and
is in fact the lifeblood of many modern internet applications. But for most business and
Government systems, scalability is not a primary quality requirement in the early stages of
development and deployment. New features to enhance usability and utility become the drivers
of our development cycles. As long as performance is adequate under normal loads, we keep
adding user-facing features to enhance the systems business value.
   Still, its not uncommon for systems to evolve into a state where enhanced performance and
scalability become a matter of urgency, or even survival. Attractive features and high utility
breed success, which brings more requests to handle and more data to manage. This often
heralds a tipping point, where design decisions that made sense under light loads are now
suddenly technical debt. External trigger events often cause these tipping points  look in the
March/April 2020 media at the many reports of Government Unemployment and supermarket
online ordering sites crashing under demand caused by the coronavirus pandemic.
   Increasing a systems capacity in some dimension by increasing resources is commonly called
scaling up or scaling out  well explore the difference between these later. In addition, unlike
physical systems, it is often equally important to be able to scale down the capacity of a system to
reduce costs. The canonical example of this is Netflix, which has a predictable regional diurnal
load that it needs to process. Simply, a lot more people are watching Netflix in any geographical
region at 9pm than are at 5am. This enables Netflix to reduce its processing resources during
times of lower load. This saves the cost of running the processing nodes that are used in the
Amazon cloud, as well as societally worthy things such as reducing data center power
consumption. Compare this to a highway. At night when few cars are on the road, we dont
retract lanes (except for repairs). The full road capacity is available for the few drivers to go as
fast as they like.
   Theres a lot more to consider about scalability in software systems, but lets come back to
these issues after examining the scale of some contemporary software systems circa 2020.
System scale in early 2020s: Examples
   Looking ahead in this technology game is always fraught with danger. In 2008 I wrote [1]:

   While petabyte datasets and gigabit data streams are today's frontiers for data-intensive applications, no
doubt 10 years from now we'll fondly reminisce about problems of this scale and be worrying about the difficulties
that looming exascale applications are posing.

   Reasonable sentiments, it is true, but exascale? Thats almost commonplace in todays world.
Google reported multiple exabytes of Gmail in 2014 , and by now, do all Google services
manage a yottabyte or more? I dont know. Im not even sure I know what a yottabyte is!
Google wont tell us about their storage, but I wouldnt bet against it. Similarly, how much data
do Amazon store in the various AWS data stores for their clients. And how many requests does,
say, DynamoDB process per second collectively, for all client applications supported. Think
about these things for too long and your head will explode.
   A great source of information that sometimes gives insights into contemporary operational
scales are the major Internet companys technical blogs. There are also Web sites analyzing
Internet traffic that are highly illustrative of traffic volumes. Lets take a couple of point in time
examples to illustrate a few things we do know today. Bear in mind these will look almost quaint
in a year or four.

*	Facebooks engineering blog describes Scribe , their solution for collecting, aggregating,
and delivering petabytes of log data per hour, with low latency and high throughput.
Facebooks computing infrastructure comprises millions of machines, each of which
generates log files that capture important events relating to system and application
health. Processing these log files, for example from a Web server, can give development
teams insights into their applications behavior and performance, and support fault
finding. Scribe is a custom buffered queuing solution that can transport logs from
servers at a rate of several terabytes per second and deliver them to downstream analysis
and data warehousing systems. That, my friends, is a lot of data!
*	You can see live Internet traffic for numerous services at www.internetlivestats.com. Dig
around and youll find statistics like Google handles around 3.5 billion search requests a
day, Instagram uploads about 65 million photos per day, and there is something like 1.7
billion web sites. It is a fun site with lots of information to amaze you. Note the data is
not really live, just estimates based on statistical analyses of multiple data sources.
*	In 2016 Google published a paper describing the characteristics of their code base .
Amongst the many startling facts reported is: The repository contains 86TBs of data, including
approximately two billion lines of code in nine million unique source files. Remember, this was
2016.

   Still, real, concrete data on the scale of the services provided by major Internet sites remain
shrouded in commercial-in-confidence secrecy. Luckily, we can get some deep insights into the
request and data volumes handled at Internet scale through the annual usage report from one
tech company. You can browse their incredibly detailed usage statistics here from 2019 here .
Its a fascinating glimpse into the capabilities of massive scale systems. Beware though, this is
Pornhub.com. The report is not for the squeamish. Heres one PG-13 illustrative data point 
they had 42 billion visits in 2019! Ill let interested readers browse the data in the report to their
hearts content. Some of the statistics will definitely make your eyes bulge!
How did we get here? A short history of system
growth
   I am sure many readers will have trouble believing there was civilized life without Internet
search, YouTube and social media. By coincidence, the day I type this sentence is the 15 year
anniversary of the first video being uploaded to YouTube . Only 15 years. Yep, it is hard for
even me to believe. Theres been a lot of wine under the bridge since then. I cant remember
how we survived!
   So, lets take a brief look back in time at how we arrived at the scale of todays systems. This
is from a personal perspective  one which started at college in 1981 when my class of 60 had
access to a shared lab of 8 state-of-the-art so-called microcomputers . By todays standards, micro
they were not.
  The 1980s
   An age dominated by mainframe and minicomputers. These were basically timeshared
multiuser systems where users interacted with the machines via dumb terminals. Personal
computers emerged in the early 1980s and developed throughout the decade to become useful
business and (relatively) powerful development machines. They were rarely networked however,
especially early in the decade. The first limited incarnation of the Internet emerged during this
time . By the end of the 1980s, development labs, universities and increasingly businesses had
email and access to exotic internet-based resources such as Usenet discussion forums   think of
a relatively primitive and incredibly polite reddit.
  1990-1995
   Personal computers and networking technology, both LANs and WANS, continued to
improve dramatically through this period. This created an environment ripe for the creation of
the World Wide Web (WWW) as we know it today. The catalyst was the HTTP/HTML
technology that had been pioneered at CERN by Tim Berners-Lee  during the 1980s. In 1993
CERN made the WWW technology available on a royalty-free basis. And the rest is history  a
platform for information sharing and money-making had been created. By 1995, the number of
web sites was tiny, but the seeds of the future were planted with companies like Yahoo! in 1994
and Amazon and eBay in 1995
  1996-2000
   During this period, the number of web sites grew from around 10,000 to 10 million , a truly
explosive growth period. Networking bandwidth and access also grew rapidly, with initially dial-
up modems for home users (yep, dial-up) and then early broadband technologies becoming
available.
   This surge in users with Internet access heralded a profound change in how we had to think
about building systems. Take for example a retail bank. Before providing online services, it was
possible to accurately predict the loads the banks business systems would experience. You
knew how many people worked in the bank and used the internal systems, how many
terminals/PCs were connected to the banks networks, how many ATMs you had to support,
and the number and nature of connections to other financial institutions. Armed with this
knowledge, we could build systems that support say a maximum of 3000 concurrent users, safe
in the knowledge that this number could not be exceeded. Growth would also be relatively slow,
and probably most of the time (eg outside business hours) the load would be a lot less than the
peak. This made our software design decisions and hardware provisioning a lot easier.
   Now imagine our retail bank decides to let all customers have Internet banking access. And
the bank has 5 million customers. What is our maximum load now? How will load be dispersed
during a business day? When are the peak periods? What happens if we run a limited time
promotion to try and sign up new customers? Suddenly our relatively simple and constrained
business systems environment is disrupted by the higher average and peak loads and
unpredictability you see from Internet-based user populations.
   During this period, companies like Amazon, eBay, Google, Yahoo! and the like were
pioneering many of the design principles and early versions of advanced technologies for highly
scalable systems. They had to, as their request loads and data volumes were growing
exponentially.
  2000-2006
   The late 1990s and early 2000s saw massive investments in, and technological innovations
from so called dot com companies, all looking to provide innovative and valuable online
businesses. Spending was huge, and not all investments were well targeted. This led to a little
event called the dot com crash  during 2000/2001. By 2002 the technology landscape was
littered with failed investments  anyone remember Pets.Com? Nope. Me neither. About 50%
of dot coms disappeared during this period. Of those that survived, albeit with much lower
valuations, many have become the staples we all know and use today.
   The number of web sites grew from around 10 to 80 million during this period, and new
service and business models emerged. In 2005, YouTube was launched. 2006 saw Facebook
become available to the public. In the same year, Amazon Web Services, which had low key
beginnings in 2004, relaunched with its S3 and EC2 services. The modern era of Internet-scale
computing and cloud-hosted systems was born.
  2007-2020 (today)
   We now live in a world with nearly 2 billion web sites, of which about 20% are active. There
are something like 4 billion Internet users . Huge data centers operated by public cloud
operators like AWS, GCP and Azure, along with a myriad of private data centers, for example
Twitters operational infrastructure , are scattered around the planet. Clouds host millions of
applications, with engineers provisioning and operating their computational and data storage
systems using sophisticated cloud management portals. Powerful, feature-rich cloud services
make it possible for us to build, deploy and scale our systems literally with a few clicks of a
mouse. All you must do is pay your cloud provider bill at the end of the month.
   This is the world that this book targets. A world where our applications need to exploit the
key principles for building scalable systems and leverage highly scalable infrastructure platforms.
Bear in mind, in modern applications, most of the code executed is not written by your
organization. It is part of the containers, databases, messaging systems and other components
that you compose into your application through API calls and build directives. This makes the
selection and use of these components at least as important as the design and development of
your own business logic. They are architectural decisions that are not easy to change.
Scalability Basic Design Principles
   As we have already discussed, the basic aim of scaling a system is to increase its capacity in
some application-specific dimension. A common dimension is increasing the number of
requests that a system can process in a given time period. This is known as the systems
throughput. Lets use an analogy to explore two basic principles we have available to us for
scaling our systems and increasing throughput.
   In 1932, one of the worlds great icons, the Sydney Harbor Bridge , was opened. Now it is a
fairly safe assumption that traffic volumes in 2020 are somewhat higher than in 1932. If you
have driven over the bridge at peak hour in the last 30 years, then you know that its capacity is
exceeded considerably every day. So how do we increase throughput on physical infrastructures
such as bridges?
   This issue became very prominent in Sydney in the 1980s, when it was realized that the
capacity of the harbor crossing had to be increased. The solution was the rather less iconic
Sydney Harbor tunnel , which essentially follows the same route underneath the harbor. This
provides 4 more lanes of traffic, and hence added roughly 1/3rd more capacity to harbor
crossings. In not too far away Auckland, their harbor bridge  also had a capacity problem as it
was built in 1959 with only 4 lanes. In essence, they adopted the same solution as Sydney,
namely, to increase capacity. But rather than build a tunnel, they ingeniously doubled the
number of lanes by expanding the bridge with the hilariously named  Nippon Clipons , which
widened the bridge on each side. Ask a Kiwi to say Nippon Clipons and you will understand
why this is funny.
   These examples illustrate the first strategy we have in software systems to increase capacity.
We basically replicate the software processing resources to provide more capacity to handle
requests and thus increase throughput, as shown in Figure 1. These replicated processing
resources are analogous to the lane ways on bridges, providing a mostly independent processing
pathway for a stream of arriving requests. Luckily, in cloud-based software systems, replication
can be achieved at the click of a mouse, and we can effectively replicate our processing
resources thousands of times. We have it a lot easier than bridge builders in that respect.

   Figure 1 Increasing Capacity through Replication
   The second strategy for scalability can also be illustrated with our bridge example. In Sydney,
some observant person realized that in the mornings a lot more vehicles cross the bridge from
north to south, and in the afternoon we see the reverse pattern. A smart solution was therefore
devised  allocate more of the lanes to the high demand direction in the morning, and sometime
in the afternoon, switch this around. This effectively increased the capacity of the bridge
without allocating any new resources  we optimized the resources we already had available.
   We can follow this same approach in software to scale our systems. If we can somehow
optimize our processing, by maybe using more efficient algorithms, adding extra indexes in our
databases to speed up queries, or even rewriting our server in a faster programming language,
we can increase our capacity without increasing our resources. The canonical example of this is
Facebooks creation of (the now discontinued)  HipHop for PHP , which increased the speed
of Facebooks web page generation by up to 6 times by compiling PHP code to C++.
   Well revisit these two design principles  namely replication and optimization - many times
in the remainder of this book. You will see that there are many complex implications of
adopting these principles that arise from the fact that we are building distributed systems.
Distributed systems have properties that make building scalable systems interesting, where
interesting in this context has both positive and negative connotations.
Scalability and Costs
   Lets take a trivial hypothetical example to examine the relationship between scalability and
costs. Assume we have a Web-based (e.g. web server and database) system that can service a
load of 100 concurrent requests with a mean response time of 1 second. We get a business
requirement to scale up this system to handle 1000 concurrent requests with the same response
time. Without making any changes, a simple load test of this system reveals the performance
shown in Figure 2 (left). As the request load increases, we see the mean response time steadily
grow to 10 seconds with the projected load. Clearly this is not scalable and cannot satisfy our
requirements in its current deployment configuration.

   Figure 2 Scaling an application. (Left)  non-scalable performance. (Right)  scalable performance
   Clearly some engineering effort is needed in order to achieve the required performance.
Figure 2 (right) shows the systems performance after it has been modified. It now provides the
specified response time with 1000 concurrent requests. Hence, we have successfully scaled the
system. Party time!
   A major question looms however. Namely, how much effort and resources were required to
achieve this performance? Perhaps it was simply a case of scaling up by running the Web server
on a more powerful (virtual) machine. Performing such reprovisioning on a cloud might take 30
minutes at most. Slightly more complex would be reconfiguring the system to scale out and run
multiple instances of the Web server to increase capacity. Again, this should be a simple, low
cost configuration change for the application, with no code changes needed. These would be
excellent outcomes.
   However, scaling a system isnt always so easy. The reasons for this are many and varied, but
heres some possibilities:
*	the database becomes less responsive with 1000 requests per second, requiring an
upgrade to a new machine
*	the Web server generates a lot of content dynamically and this reduces response time
under load. A possible solution is to alter the code to more efficiently generate the
content, thus reducing processing time per request.
*	the request load creates hot spots in the database when many requests try to access and
update the same records simultaneously. This requires a schema redesign and
subsequent reloading of the database, as well as code changes to the data access layer.
*	the Web server framework that was selected emphasized ease of development over
scalability. The model it enforces means that the code simply cannot be scaled to meet
the request load requirements, and a complete rewrite is required. Another framework?
Another programming language even?

   Theres a myriad of other potential causes, but hopefully these illustrate the increasing effort
that might be required as we move from possibility (1) to possibility (4).
   Now lets assume option (1), upgrading the database server, requires 15 hours of effort and a
thousand dollars extra cloud costs per month for a more powerful server. This is not
prohibitively expensive. And lets assume option (4), a rewrite of the Web application layer,
requires 10,000 hours of development due to implementing in a new language (e.g. Java instead
of Ruby). Options (2) and (3) fall somewhere in between options (1) and (4). The cost of 10,000
hours of development is seriously significant. Even worse, while the development is underway,
the application may be losing market share and hence money due to its inability to satisfy client
requests loads. These kinds of situations can cause systems and businesses to fail.
   This simple scenario illustrates how the dimensions of resource and effort costs are
inextricably tied to scalability. If a system is not designed intrinsically to scale, then the
downstream costs and resources of increasing its capacity to meet requirements may be massive.
For some applications, such as Healthcare.gov , these (more than $2 billion) costs are borne
and the system is modified to eventually meet business needs. For others, such as Oregons
health care exchange , an inability to scale rapidly at low cost can be an expensive ($303million)
death knell.
   We would never expect someone would attempt to scale up the capacity of a suburban home
to become a 50 floor office building. The home doesnt have the architecture, materials and
foundations for this to be even a remote possibility without being completely demolished and
rebuilt. Similarly, we shouldnt expect software systems that do not employ scalable
architectures, mechanisms and technologies to be quickly evolved to meet greater capacity
needs. The foundations of scale need to be built in from the beginning, with the recognition
that the components will evolve over time. By employing design and development principles
that promote scalability, we can more rapidly and cheaply scale up systems to meet rapidly
growing demands.
   Software systems that can be scaled exponentially while costs grow linearly are known as
hyperscale systems, defined as:
   Hyper scalable systems exhibit exponential growth in computational and storage capabilities while
exhibiting linear growth rates in the costs of resources required to build, operate, support and evolve the required
software and hardware resources.
    You can read more about hyperscale systems in this article  [3].

Summary
   The ability to scale an application quickly and cost-effectively should be a defining quality of
the software architecture of contemporary Internet-facing applications. We have two basic ways
to achieve scalability, namely increasing system capacity, typically through replication, and
performance optimization of system components. The rest of this book will delve deeply into
how these two basic principles manifest themselves in constructing scalable distributed systems.
Get ready for a wild ride.
References

1.	Ian Gorton, Paul Greenfield, Alex Szalay, and Roy Williams. 2008. Data-Intensive
Computing in the 21st Century. Computer 41, 4 (April 2008), 3032.
2.	Rachel Potvin and Josh Levenberg. 2016. Why Google stores billions of lines of code
in a single repository. Commun. ACM 59, 7 (July 2016), 7887.
3.	Ian Gorton (2017). Chapter 2. Hyperscalability  The Changing Face of Software
Architecture. 10.1016/B978-0-12-805467-3.00002-8.
?

CHAPTER 2
__________________________

Distributed Systems Architectures:
A Whirlwind Tour
   In this chapter well introduce some of the fundamental approaches to scaling a software
system. The type of systems this book is oriented towards are the internet-facing systems we all
utilize every day. Ill let you name your favorite. These systems accept requests from users
through Web and mobile interfaces, store and retrieve data based on user requests or events
(e.g. a GPS-based system), and have some intelligent features such as providing recommendations
or providing notifications based on previous user interactions.
   Well start with a simple system design and show how it can be scaled. In the process, several
concepts will be introduced that well cover in much more detail later in this book. Hence this
chapter just gives a broad overview of these concepts and how they aid in scalability  truly a
whirlwind tour!
Basic System Architecture
   Virtually all massive scale systems start off small and grow due to their success. Its common,
and sensible, to start with a development framework such as Ruby on Rails or Django or
equivalent, which promotes rapid development to get a system quickly up and running.  A
typical, very simple software architecture for starter systems which closely resembles what you
get with rapid development frameworks is shown in Figure 3. This comprises a client tier,
application service tier, and a database tier. If you use Rails or equivalent, you also get a
framework which hardwires a Model-View-Controller (MVC) pattern for Web application
processing and an Object-Relational Mapper (ORM) that generates SQL queries.
   With this architecture, users submit requests to the application from their mobile app or
Web browser. The magic of Internet networking (see Chapter 4) delivers these requests to the
application service which is running on a machine hosted in some corporate or commercial
cloud data center. Communications uses a standard network protocol, typically HTTP.
   The application service runs code that supports an application programming interface (API)
that clients use to format data and send HTTP requests to. Upon receipt of a request, the
service executes the code associated with the requested API. In the process, it may read from or
write to a database, depending on the semantics of the API. When the request is complete, the
service sends the results to the client to display in their app or browser.

   Figure 3 Basic Multi-Tier Distributed Systems Architecture
   Many systems conceptually look exactly like this. The application service code exploits a
server execution environment that enables multiple requests from multiple users to be
processed simultaneously. Theres a myriad of these application server technologies  JEE and
Spring for Java, Flask for Python  that are widely used in this scenario. This approach leads to
what is generally known as a monolithic architecture . Monoliths grow in complexity as the
application becomes more feature rich. This eventually makes it hard to modify and test rapidly,
and the execution footprint can become extremely heavyweight as all the API implementations
run in the same application service.
   Still, if request loads stay relatively low, this application architecture can suffice. The service
has the capacity to process requests with consistently low latency. But if request loads keep
growing, this means latencies will grow as the service has insufficient CPU/memory capacity for
the concurrent request volume and hence requests will take longer to process. In these
circumstances, our single server is overloaded and has become a bottleneck.
   In this case, the first strategy for scaling is usually to scale up the application service
hardware. For example, if your application  is running on AWS, you might upgrade your server
from a modest t3.xlarge instance with 4 (virtual) CPUs and 16GBs of memory to a t3.2xlarge
instance which doubles the number of CPUs and memory available for the application .
   Scale up is simple. It gets many real-world applications a long way to supporting larger
workloads. It obviously just costs more money for hardware, but thats scaling for you.
   Its inevitable however for many applications the load will grow to a level which will swamp
a single server node, no matter how many CPUs and how much memory you have. Thats when
you need a new strategy  namely scaling out, or horizontal scaling, that we touched on in
Chapter 1.
Scale Out
   Scaling out relies on the ability to replicate a service in the architecture and run multiple
copies on multiple server nodes. Requests from clients are distributed across the replicas so that
in theory, if we have N replicas, each server node processes {#requests/N}. This simple
strategy increases an applications capacity and hence scalability.
   To successfully scale out an application, we need two fundamental elements in our design.
As illustrated in Figure 4, these are:

1)	Load balancer: All user requests are sent to a load balancer, which chooses a service
replica to process the request. Various strategies exist for choosing a target service, all
with the core aim of keeping each resource equally busy. The load balancer also relays
the responses from the service back to the client. Most load balancers belong to a class
of Internet components known as reverse proxies , which control access to server
resources for client requests. As an intermediary, reverse proxies add an extra network
hop for a request, and hence need to be extremely low latency to minimize the
overheads they introduce. There are many off-the-shelf load balancing solutions as well
as cloud-provider specific ones, and well cover the general characteristics of these in
much more detail in Chapter XXX.
2)	Stateless services: For load balancing to be effective and share requests evenly, the
load balancer must be free to send consecutive requests from the same client to different
service instances for processing. This means the API implementations in the services
must retain no knowledge, or state, associated with an individual clients session. When a
user accesses an application, a user session is created by the service and a unique session
identified is managed internally to identify the sequence of user interactions and track
session state. A classic example of session state is a shopping cart. To use a load balancer
effectively, the data representing the current contents of a users cart must be stored
somewhere  typically a data store  such that any service replica can access this state
when it receives a request as part of a user session. In Figure 4 this is labeled as a Session
Store.

   Figure 4 Scale out Architecture
   Scale out is attractive as, in theory, you can keep adding new (virtual) hardware and services
to handle increased request loads and keep request latencies consistent and low. As soon as you
see latencies rising, you deploy another server instance. This requires no code changes and
hence is relatively cheap  you just pay for the hardware you deploy.
   Scale out has another highly attractive feature. If one of the services fails, the requests it is
processing will be lost. But as the failed service manages no session state, these requests can be
simply reissued by the client and sent to another service instance for processing. This means the
application is resilient to failures in the service software and hardware, thus enhancing the
applications availability. Availability is a key feature of distributed systems, and one we will
discuss in depth in Chapter XXX.
   Unfortunately, as with any engineering solution, simple scaling out has limits. As you add
new service instances, the request processing capacity grows, potentially infinitely. At some
stage however, reality will bite and the capability of your single database to provide low latency
query responses will diminish. Slow queries will mean longer response times for clients. If
requests keep arriving faster than they are being processed, some system component will fail
due to resource exhaustion and clients will see exceptions and request timeouts. Essentially your
database has become a bottleneck that you must engineer away in order to scale your
application further.
Scaling the Database with Caching
   Scaling up by increasing the number of CPUs, memory and disks in a database server can go
a long way to scaling a system. For example, at the time of writing Google Cloud Platform can
provision a SQL database on a db-n1-highmem-96 node, which has 96 vCPUs, 624GB of memory,
30TBs of disk and can support 4000 connections. This will cost somewhere between $6K and
$16K per year, which sounds a good deal to me! Scaling up is a very common database
scalability strategy.
   Large databases need constant care and attention from highly skilled database administrators
to keep them tuned and running fast. Theres a lot of wizardry in this job  e.g. query tuning,
disk partitioning, indexing, on-node caching  and hence database administrators are valuable
people that you want to be very nice to. They can make you application services highly
responsive indeed.
   In conjunction with scale up, a highly effective approach is querying the database as
infrequently as possible in your services. This can be achieved by employing distributed caching
in the service tier. Caching stores recently retrieved and commonly accessed database results in
memory so they can be quickly retrieved without placing a burden on the database. For data that
is frequently read and changes rarely, your processing logic can be modified to first check a
distributed cache, such as a Redis  or memcached  store. These cache technologies are
essentially distributed Key-Value stores with very simple APIs. This scheme is illustrated in
Figure 5. Note that the Session Store from Figure 4 has disappeared. This is because we can use a
general-purpose distributed cache to store session identifiers along with application data.
   Accessing the cache requires a remote call from your service, but if the data you need is in
the cache, on a fast network this is far less expensive than querying the database instance.
Introducing a caching layer also requires your processing logic to be modified to check for
cached data. If what you want is not in the cache, your code must still query the database and
load the results into the cache as well as return it to the caller. You also need to decide when to
remove or invalidate cached results  this depends on your applications tolerance to serving
stale results to clients and the volume of data you cache.

   Figure 5 Introducing Distributed Caching
   A well-designed caching scheme can be absolutely invaluable in scaling a system. Caching
works great for data that rarely changes and is accessed frequently, such as inventory, event and
contact data. If you can handle a large percentage, like 80% or more, of read requests from your
cache, then you effectively buy extra capacity at your databases as they are not involved in
handling requests.
   Still, many systems need to rapidly access terabyte and larger data stores that make a single
database effectively prohibitive. In these systems, a distributed database is needed.
Distributing the Database
   There are more distributed database technologies around in 2020 than you probably want to
imagine. Its a complex area, and one well cover extensively later in Chapter XXX. In very
general terms, there are two major categories:

1)	Distributed SQL stores from major vendors such as Oracle and IBM. These enable
organizations to scale out their SQL database relatively seamlessly by storing the data
across multiple disks that are queried by multiple database engine replicas. These
multiple engines logically appear to the application as a single database, hence
minimizing code changes.
2)	Distributed so-called NoSQL stores from a whole array of vendors. These products use
a variety of data models and query languages to distribute data across multiple nodes
running the database engine, each with their own locally attached storage. Again, the
location of the data is transparent to the application, and typically controlled by the
design of the data model using hashing functions on database keys. Leading products in
this category are Cassandra, MongoDB and Neo4j.

   Figure 6 Scaling the Data Tier using a Distributed Database
   Figure 6 shows how our architecture incorporates a distributed database. As the data
volumes grow, a distributed database has features to enable the number of storage nodes to be
increased.  As nodes are added (or removed), the data managed across all nodes is rebalanced to
attempt to ensure the processing and storage capacity of each node is equally utilized.
   Distributed databases also promote availability. They support replicating each data storage
node so if one fails or cannot be accessed due to network problems, another copy of the data is
available. The models utilized for replication and the trade-offs these require (spoiler 
consistency) are covered in later chapters.
   If you are utilizing a major cloud provider, there are also two deployment choices for your
data tier. You can deploy your own virtual resources and build, configure, and administer your
own distributed database servers. Alternatively, you can utilize cloud-hosted databases. The
latter simplifies the administrative effort associated with managing, monitoring and scaling the
database, as many of these tasks essentially become the responsibility of the cloud provider you
choose. As usual, the no free lunch principle applies.
Multiple Processing Tiers
   Any realistic system that we need to scale will have many different services that interact to
process a request. For example, accessing a Web page on the Amazon.com web site can require
in excess of 100 different services being called before a response is returned to the user .
   The beauty of the stateless, load balanced, cached architecture we are elaborating in this
chapter is that we can extend the core design principles and build a multi-tiered application. In
fulfilling a request, a service can call one or more dependent services, which in turn are
replicated and load-balanced. A simple example is shown in Figure 7. There are many nuances
in how the services interact, and how applications ensure rapid responses from dependent
services. Again, well cover these in detail in later chapters.

   Figure 7 Scaling Processing Capacity with Multiple Tiers
   This design also promotes having different, load balanced services at each tier in the
architecture. For example, Figure 8 illustrates two replicated Internet-facing services that both
utilized a core service that provides database access. Each service is load balanced and employs
caching to provide high performance and reliability. This design is often used to provide a
service for Web clients and a service for mobile clients, each of which can be scaled
independently based on the load they experience. Its commonly called the Backend For
Frontend (BFF) pattern .

   Figure 8 Scalable Architecture with Multiple Services
   In addition, by breaking the application into multiple independent services, we can scale each
based on the service demand. If for example we see an increasing volume of requests from
mobile users and decreasing volumes from Web users, we can provision different numbers of
instances for each service to satisfy demand. This is a major advantage of refactoring monolithic
applications into multiple independent services, which can be separately built, tested, deployed
and scaled.
Increasing Responsiveness
   Most client application requests expect a response. A user might want to see all auction
items for a given product category or see the real estate that is available for sale in a given
location. In these examples, the client sends a request and waits until a response is received.
This time interval between sending the request and receiving the result is the latency of the
request. We can decrease latencies by using caching and precalculated responses, but many
requests will still result in a database access.
   A similar scenario exists for requests that update data in an application. If a user updates
their delivery address immediately prior to placing an order, the new delivery address must be
persisted so that the user can confirm the address before they hit the purchase button. The
latency in this case includes the time for the database write, which is confirmed by the response
the user receives.
   Some update requests however can be successfully responded to without fully persisting the
data in a database. For example, the skiers and snowboarders amongst you will be familiar with
lift ticket scanning systems that check you have a valid pass to ride the lifts that day. They also
record which lifts you take, the time you get on, and so on. Nerdy skier/snowboarders can then
use the resorts mobile app to see how many lifts they ride in a day.
   As a person waits to get on a lift, a scanner device validates the pass using an RFID chip
reader. The information about the rider, lift, and time are then sent over the Internet to a data
capture service operated by the ski resort. The lift rider doesnt have to wait for this to occur, as
the latency could slow down their loading process. Theres also no expectation from the lift
rider that they can instantly use their app to ensure this data has been captured. They just get on
the lift, talk smack with their friends, and plan their next run.
   Service implementations can exploit this type of scenario to improve responsiveness. The
data about the event is sent to the service, which acknowledges receipt and concurrently stores
the data in a remote queue for subsequent writing to the database. Writing a message to a queue
is much faster than writing to a database, and this enables the request to be successfully
acknowledged much more quickly. Another service is deployed to read messages from the
queue and write the data to the database. When the user checks their lift rides  maybe 3 hours
or 3 days later  the data has been persisted successfully. The basic architecture to implement
this approach is illustrated in Figure 9.

   Figure 9 Increasing Responsiveness with Queueing
   Whenever the results of a write operation are not immediately needed, an application can use
this approach to improve responsiveness and hence scalability. Many queueing technologies
exist that applications can utilize, and well discuss how these operate in later chapters. These
queueing platforms all provide asynchronous communications. A producer service writes to the
queue, which acts as temporary storage, while another consumer service removes messages from
the queue and makes the necessary updates to, in our example, a database that stores skier lift
ride details.
   The key is that the data eventually gets persisted. Eventually typically means a few seconds at
most but use cases that employ this design should be resilient to longer delays without
impacting the user experience.
Summary and Further Reading
   This chapter has provided a whirlwind tour of the major approaches we can utilize to scale
out a system as a collection of communicating services and distributed databases. Much detail
has been brushed over, and as you no doubt know, in software systems the devil is in the detail.
Subsequent chapters will therefore progressively start to explore these details, starting with
concurrent and distributed systems fundamentals. Next there are sections on the application
service and data management tiers as portrayed in the basic distributed systems architecture
blueprint described in this chapter.
   Another area this chapter has skirted around is the subject of software architecture. Weve
used the term services for distributed components in an architecture that implement application
business logic and database access. These services are independently deployed processes that
communicate using remote communications mechanisms such as HTTP. In architectural terms,
these services are most closely mirrored by those in the Service Oriented Architecture (SOA)
pattern, an established architectural approach for building distributed systems. A more modern
evolution of this approach revolves around microservices. These tend to be more cohesive,
encapsulated services that promote continuous development and deployment.
   Well touch on these differences in a later chapter. If youd like a much more in depth
discussion of these, and software architecture issues in general, then Mark Richards and Neal
Fords book is an excellent place to start.

Mark Richards and Neal Ford, Fundamentals of Software Architecture: An Engineering
Approach 1st Edition, OReilly Media, 2020

   Finally, theres a class of big data software architectures that address some of the issues that
come to the fore with very large data collections. One of the most prominent is data
reprocessing. This occurs when data that has already been stored and analyzed needs to be re-
analyzed due to code changes. This reprocessing may occurs due to software fixes, or the
introduction of new algorithms that can derive more insights from the original raw data. Theres
a good discussion of the Lambda and Kappa architectures, both of which are prominent in this
space, in this article.

   Jay Krepps, Questioning the Lambda Architecture,
https://www.oreilly.com/radar/questioning-the-lambda-architecture/
?
CHAPTER 3
__________________________

An Overview of Concurrent
Systems

   Scaling a system naturally involves adding multiple independently moving parts. We run our
servers on multiple machines, our databases across multiple storage nodes, all in the quest of
adding more capacity. Consequently, our solutions are distributed across multiple locations, with
each processing events concurrently.
   Any distributed system is hence by definition a concurrent system, even if each node is
processing events one at a time. The behavior of the various nodes has to be coordinated in
order to make the application behave as desired. As well see later, coordinating nodes in a
distributed system is fraught with dangers. Luckily, our industry has matured sufficiently to
provide complex, powerful software frameworks that hide many of these distributed system
nasties (most of the time!) from our applications.
   This chapter is concerned with concurrent behavior in our systems on a single node. By
explicitly writing our software to perform multiple actions concurrently, we can optimize the
processing on a single node, and hence increase our processing capacity both locally and system
wide. Well use the Java 7.0 concurrency capabilities for examples, as these are at a lower level of
abstraction than those introduced in Java 8.0. Knowing how concurrent systems operate closer
to the machine is useful foundational knowledge when building concurrent and distributed
systems.
   A final point. This chapter is a concurrency primer. It wont teach you everything you need
to know to build complex, high performance concurrent systems. It will also be useful if your
experience writing concurrent programs is rusty, or you have written concurrent code in another
programming language. The further reading section points you to more comprehensive
coverage of this topic for those who wish to delve deeper.
Why Concurrency?
   Think of a busy coffee shop. If everyone orders a simple coffee, then the barista can quickly
and consistently deliver each drink. Suddenly, the person in front of you orders a soy, vanilla, no
sugar, quadruple shot iced brew. Everyone in line sighs and starts reading their social media, In
two minutes the line is out of the door.
    Processing requests in Web applications is analogous to our coffee example. In a coffee
shop, we enlist the help of a new barista to simultaneously make coffees on a different machine
to keep the line length in control and serve customers quickly. In software, to make applications
responsive, we need to somehow process requests in our server an overlapping manner.
   In the good old days of computing, each CPU was only able to execute a single machine
instruction at any instant. If our server application runs on such a CPU, why do we need to
structure our software systems to execute multiple instructions concurrently? It all seems slightly
pointless.
   There is actually a very good reason. Virtually every program does more than just execute
machine instructions. For example, when a program attempts to read from a file or send a
message on the network, it must interact with the hardware subsystem (disk, network card) that
is peripheral to the CPU. Reading data from a modern hard disk takes around 10 milliseconds
(ms). During this time, the program must wait for the data to be available for processing.
   Now, even an ancient  CPU such as a circa 1988 Intel 80386  can execute more than 10
million instructions per second (mips). 10ms is 1/100th of a second. How many instructions
could our 80386 execute in 1/100th second. Do the math. Its a lot! A lot of wasted processing
capacity, in fact.
   This is how operating systems such as Linux can run multiple programs on a single CPU.
While one program is waiting for an input-output (I-O) event, the operating system schedules
another program to execute. By explicitly structuring our software to have multiple activities
that can be executed in parallel, the operating system can schedule tasks that have work to do
while others wait for I-O. Well see in more detail how this works with Java later in this chapter.
   In 2001, IBM introduced the worlds first multicore processor, a chip with two CPUs  see
Figure 10. Today, even my laptop has 16 CPUs, or cores as they are commonly known. With a
multicore chip, a software system that is structured to have multiple parallel activities can be
executed in parallel. Well, up the number of available cores, anyway. In this way, we can fully
utilize the processing resources on a multicore chip, and hence increase our applications
capacity.

   Figure 10 Simplified view of a multicore processor
   The primary way to structure a software system as concurrent activities is to use threads.
Every programming language has its own threading mechanism. The underlying semantics of all
these mechanisms are similar  there are only a few primary threading models in mainstream use
 but obviously the syntax varies by language. In the following sections, well see how threads
are supported in Java.
Threads in Java
   Every software process has a single thread of execution by default. This is the thread that the
operating system manages when it schedules the process for execution. In Java, the main()
function you specify as the entry point to your code defines the behavior of this thread. This
single thread has access to the programs environment and resources such as open file handles
and network connections. As the program calls methods in objects instantiated in the code, the
runtime stack is used to pass parameters and manage variable scopes. Standard programming
language run time stuff, basically. This is a sequential process.
   In your systems, you can use programming language features to create and execute additional
threads. Each thread is an independent sequence of execution and has its own runtime stack to
manage local object creation and method calls. Each thread also has access to the process
global data and environment. A simple depiction of this scheme is shown in Figure 11.

   Figure 11 Comparing a single and multithreaded process
   In Java, we can define a thread using a class that implements the Runnable interface and
defines a run() method. A simple example is depicted in Figure 12.
1.	class NamingThread implements Runnable {
2.
3.	  private String name;
4.
5.	 	public NamingThread(String threadName) {
6.		  name = threadName ;
7.	    System.out.println("Constructor called: " + threadName) ;
8.	  }
9.
10.	  public void run() {
11.		  //Display info about this  thread
12.	    System.out.println("Run called : " + name);
13.	    System.out.println(name + " : " + Thread.currentThread());
14.	    // and now terminate  ....
15.		}
16.	}
17.
   Figure 12 A Simple Java Thread class
   To execute the thread, we need to construct a Thread object using an instance of our
Runnable and call the start() method to invoke the code in its own execution context. This is
shown in Figure 13, along with the output of running the code. Note this example has two
threads  the main() thread and our own NamingThread. The main thread starts the
NamingThread, which executes asynchronously, and then waits for 1 second to give our
run() method in NamingThread ample time to complete. This order of execution can be seen
from examining the outputs in Figure 13.
   For illustration, we also call the static currentThread() method, which returns a string
containing:

*	The system generated thread identifier
*	The thread priority, which by default is 5 for all threads. Well cover priorities later.
*	The identifier of the parent thread  in this example both parent threads are main

   Note to instantiate a thread, we call the start() method, not the run() method we define
in the Runnable. The start() method contains the internal system magic to create the
execution context for a separate thread to execute (e.g. see Figure 11). If we call run() directly,
the code will execute, but no new thread will be created. The run() method will execute as part
of the main thread, just like any other Java method invocation that you know and love. You will
still have a single threaded code.
1.	public static void main(String[] args) {
2.
3.	  NamingThread name0 = new NamingThread("My first thread");
4.
5.	  //Create the thread
6.	  Thread t0 = new Thread (name0);
7.
8.	  // start the threads
9.	  t0.start();
10.
11.	  //delay the main thread for a second (1000 miliiseconds)
12.	  try {
13.	    Thread.currentThread().sleep(1000);
14.	  } catch (InterruptedException e) {
15.	      }
16.
17.	      //Display info about the main thread and terminate
18.	      System.out.println(Thread.currentThread());
19.	    }
20.
21.	===EXECUTION OUTPUT===
22.	Constructor called: My first thread
23.	Run called : My first thread
24.	My first thread : Thread[Thread-0,5,main]
25.	Thread[main,5,main]

Figure 13 Creating and executing a thread
   In the example, we use sleep() to make sure the NamimgThread will terminate before the
main thread. Coordinating two threads by delaying for an absolute time period (e.g. 1 second in
our example) is not a very robust mechanism. What if for some reason - a slower CPU, a long
delay reading disk, additional complex logic in the method  our thread doesnt terminate in the
expected time frame? In this case, main will terminate first  this is not we intend. In general, if
you are using absolute times for thread coordination, you are doing it wrong. Almost always.
Like 99.99999% of the time.
   A simple and robust mechanism for one thread to wait until another has completed its work
is to use the join() method. In Figure 13, we could replace the try-catch block with:

   t0.join();

   This method causes the calling thread (in our case, main) to block until the thread referenced
by t0 terminates. If the referenced thread has terminated before the call to join(), then the
method call returns immediately. In this way we can coordinate, or synchronize, the behavior of
multiple threads. Synchronization of multiple threads is in fact the major focus of rest of this
chapter.
Order of Thread Execution
   TL;DR The system scheduler (in Java, this lives in the JVM) controls the order of thread
execution. From the programmers perspective, the order of execution is non-deterministic. Get
used to that term, well use it a lot. The concept of non-determinism is fundamental to
understanding multithreaded code.
   Lets illustrate this by building on our earlier example. Instead of creating a single
NamingThread, lets create and start up a few. 3 in fact, as shown in Figure 14:
1.	NamingThread name0 = new NamingThread("thread0");
2.	      NamingThread name1 = new NamingThread("thread1");
3.	      NamingThread name2 = new NamingThread("thread2");
4.
5.	      //Create the threads
6.	      Thread t0 = new Thread (name0);
7.	      Thread t1 = new Thread (name1);
8.	      Thread t2 = new Thread (name2);
9.
10.	      // start the threads
11.	      t0.start();
12.	      t1.start();
13.	      t2.start();
14.
15.	===EXECUTION OUTPUT===
16.	Run called : thread0
17.	thread0 : Thread[Thread-0,5,main]
18.	Run called : thread2
19.	Run called : thread1
20.	thread1 : Thread[Thread-1,5,main]
21.	thread2 : Thread[Thread-2,5,main]
22.	Thread[main,5,main]
   Figure 14 Multiple threads exhibiting non-deterministic behavior
   The output shown in Figure 14 is a sample from just one execution. You can see the code
starts three threads sequentially, namely t0, t1 and t2 (lines 11-13). Looking at the execution
trace, we see thread t0 completes (line 17) before the others start. Next t2s run() method is
called (line 18) followed by t1s run() method, even though t1 was started before t2. Thread t1
then runs to completion (line 20) before t1, and eventually the main thread and the program
terminate.
   This is just one possible order of execution. If we run this program again, we will almost
certainly see a different execution trace. This is because the JVM scheduler is deciding which
thread to execute, and for how long. Simply, once the scheduler has given a thread an execution
time slot on the CPU, it interrupts the thread and schedules another one to run. This ensures
each threads is given an opportunity to make progress. Hence the threads run independently
and asynchronously until completion, and the scheduler decides which thread runs when based
on a scheduling algorithm.
   We will examine the basic scheduling algorithm used later in this chapter. But for now, there
is a major implication for programmers, namely, regardless of the order of thread execution,
your code should produce correct results. Sounds easy?
   Read on.
Problems with Thread  Race Conditions
   Non-deterministic execution of threads implies that the code statements that comprise the
threads:

*	Will execute sequentially as defined for each thread
*	Can be overlapped in any order across threads, as how many statements are executed for
each thread execution slot is the scheduler.

   Hence, when many threads are executed on a single processor, their execution is interleaved.
The executes some steps from one thread, then performs some steps from another, and so on.
If we are executing on a multicore CPU, then we can execute one thread per core. The
statements of each thread execution are still however interleaved in a non-deterministic manner.
   Now, if every thread simply does its own thing, and is completely independent, this is not a
problem. Each thread executes until it terminates, as in our trivial example in Figure 14. Piece of
cake! Why are these thread things meant to be complex?
   Unfortunately, totally independent threads are not how most multithreaded systems behave.
If you refer back to Figure 11, you will see that multiple threads share the global data within a
process. In Java this is both global and static data.
   Threads can use shared data structures to coordinate their work and communicate status
across threads. For example, we may have threads handling requests from a Web clients, one
thread per request. We also want to keep a running total of how many requests we process each
day. When a thread completes a request, it increments a global RequestCounter object that all
threads share and update after each request. At the end of the day, we know how many requests
were processed. A lovely solution indeed.
   Figure 15 shows a very simple implementation that mimics our example scenario by creating
50k threads to update a shared counter. Note we use a lambda function for brevity to create the
threads, and a dont do this at home 5 second delay in main to allow the threads to finish.
   What you can do at home is run this code a few times and see what results you get. In 10
executions my mean was 49995. I didnt once get the correct answer, namely 50000. Weird.
   Why?
1.	public class RequestCounter {
2.	  final static private int NUMTHREADS = 50000;
3.	  private int count = 0;
4.
5.	  public  void inc() {
6.	    count++;
7.	  }
8.
9.	  public int getVal() {
10.	    return this.count;
11.	  }
12.
13.	  public static void main(String[] args) throws InterruptedException
{
14.	    final RequestCounter counter = new RequestCounter();
15.
16.	    for (int i = 0; i < NUMTHREADS; i++) {
17.	      // lambda runnable creation
18.	      Runnable thread = () -> {counter.inc(); };
19.		    new Thread(thread).start();
20.	    }
21.
22.	    Thread.sleep(5000);
23.	    System.out.println("Value should be " + NUMTHREADS + "It is: " +
counter.getVal());
24.	  }
25.	}
   Figure 15 Example of a race condition
   The answer lies in how abstract, high-level programming language statements, in Java in this
case, are executed on a machine. To perform an increment of a counter, the CPU must (1) load
the current value into a register, (2) increment the register value, and (3) write the results back to
the original memory location.
   As Figure 16 shows, at the machine level these three operations are not indistinguishable, or
as more commonly known, atomic. One thread can load the value into the register, but before it
writes the incremented value back, the scheduler interrupts it and allows another thread to start.
This thread loads the old value of the counter and writes back the incremented value.
Eventually the original thread executes again, and writes back its incremented value, which just
happens to be the same as what is already in memory.
   Weve lost an update. From our 10 tests of the code in Figure 15, we see this is happening
on average 5 times in 50000 increments.  Hence such events are rare, but even if it happens 1
time in 10 million, you still have an incorrect result.

   Figure 16 Increments are not atomic at the machine level
   This is called a race condition. Race conditions can occur whenever multiple threads make
changes to some shared state, in this case a simple counter. Essentially, different interleaving sof
the threads can produce different results. Race conditions are insidious, evil errors, because their
occurrence is typically rare, and they can be hard to detect as most of the time the answer will
be correct. Try running the code in Figure 15 with 1000 threads instead of 50000, and you will
see this in action. I got the correct answer four times out of 5.
   Same code. Occasionally different results. Like I said  race conditions evil! Luckily,
eradicating them is straightforward if you take a few precautions.
   The key is to identify and protect critical sections. A critical section is a section of code that
updates shared data structures, and hence must be executed atomically if accessed by multiple
threads. Our example of incrementing a shared counter is an example of a critical section. What
about removing an item from a list? We need to delete the head node of the list, and move the
reference to the head of the list from the removed node to the next node in the list. Both
operations must be performed atomically to maintain the integrity of the list. Its a critical
section.
   In Java, the synchronized keyword defines a critical section. If use to decorate a method,
then when multiple threads attempt to call that method on the same shared object, only one is
permitted to enter the critical section. All others block until the thread exits the synchronized
method, at which point the scheduler chooses the next thread to execute the critical section. We
say the execution of the critical section is serialized, as only one thread at a time can be
executing the code inside it.
   To fix the example in Figure 15, we therefore just need to identify the inc() method as a
critical section, ie:

   synchronized public void inc() {
       count++;
     }

   Test it out as many times as you like. Youll always get the correct answer. Slightly more
formally, this means any interleaving of the threads thar the scheduler throws at us will always
produce the correct results.
   The synchronized keyword can also be applied to blocks of statements within a method. For
example, we could rewrite the above example as:

   public void inc() {
        synchronized(this){
              count++;
           }
   }

   Underneath the covers, every Java object has a monitor lock, sometimes known as an
intrinsic lock, as part of its runtime representation. To enter a synchronized method or block of
statements as shown above, a thread must acquire the monitor lock. Only one thread can own
the lock at any time, and hence execution is serialized. This, very basically, is how Java
implements critical sections.
   Aa a rule of thumb, we should keep critical sections as small as possible so that the serialized
code is minimized. This can have positive impacts on performance and hence scalability. Well
return to this topic later, but if you are interested, have a read about Amdahls Law  for an
explanation on why this is desirable.
Problems with Thread  Deadlock
   To ensure correct results in multithreaded code, we have seen we have to restrict non-
determinism to serialize access to critical sections. This avoids race conditions. However, if we
are not careful, we can write code that restricts non-determinism so much that our program
stops. This is formally known as a deadlock.
   A deadlock occurs when two threads acquire exclusive access to a resource that the other
thread also needs to make progress. The scenario below shows how this can occur:

   Two threads sharing access to two shared variables via synchronized blocks
1.	thread 1: enters critical section A
2.	thread 2: enters critical section B
3.	thread 1: blocks on entry to critical section B
4.	thread 2: blocks on entry to critical section A
5.	Both threads wait forever ?

   A deadlock, also known as a deadly embrace, causes a program to stop. It doesnt take a
vivid imagination to realize that this can cause all sorts of undesirable outcomes. Im happily
texting away while my autonomous vehicle drives me to the bar. Suddenly, the code deadlocks.
It wont end well.
   Deadlocks occur in more subtle circumstances than the simple example above. The classic
example is the Dining Philosophers  problem. The story goes like this.
   Five philosophers sit around a shared table. Being philosophers, they spend a lot of time
thinking deeply. In between bouts of deep thinking, they replenish their brain functions by
eating from a plate of food that sits in front of them. Hence a philosopher is either eating or
thinking, or transitioning between these two states.
   In addition, the philosophers must all be very close, highly dexterous and all COVID19
vaccinated friends, as they share chop sticks to eat with. Only five chopsticks are on the table,
placed between each philosopher. When one wishes to eat, they follow a protocol of picking up
their left chopstick first, then their right chopstick. Once they are ready to think again, they first
return the right chopstick, then the left.

   Figure 17 The Dining Philosophers Problem
   Figure 17 depicts our all-female philosophers, each identified by a unique number. As each is
either concurrently eating or thinking, we can model each philosopher as a thread. The code is
shown in Figure 18. The shared chopsticks are represented by instances of the Java Object
class. As only one object can hold the monitor lock on an object, they are used as entry
conditions to the critical sections in which the philosophers acquire the chopsticks they need to
eat. After eating, the chopsticks are returned to the table and the lock is released on each so that
neighboring philosophers can eat whenever they are ready.
1.	public class Philosopher implements Runnable {
2.
3.	  private final Object leftChopStick;
4.	  private final Object rightChopStick;
5.
6.	  Philosopher(Object leftChopStick, Object rightChopStick) {
7.	    this.leftChopStick = leftChopStick;
8.	    this.rightChopStick = rightChopStick;
9.	  }
10.	  private void LogEvent(String event) throws InterruptedException {
11.	    System.out.println(Thread.currentThread()
12.	                                  .getName() + " " + event);
13.	    Thread.sleep(1000);
14.	  }
15.
16.	  public void run() {
17.	    try {
18.	      while (true) {
19.	        LogEvent(": Thinking deeply");
20.	        synchronized (leftChopStick) {
21.	          LogEvent( ": Picked up left chop stick");
22.	          synchronized (rightChopStick) {
23.	            LogEvent(": Picked up right chopstick  eating");
24.	            LogEvent(": Put down right chopstick");
25.	          }
26.	          LogEvent(": Put down left chopstick. Ate too much");
27.	        }
28.	      } // end while
29.	    } catch (InterruptedException e) {
30.	       Thread.currentThread().interrupt();
31.	  }
32.	 }
33.	}
   Figure 18 A Philosopher thread
   To bring our philosophers to life, we must instantiate a thread for each and give each
philosopher access to its neighboring chopsticks. This is done through the thread constructor
call on line 16 in Figure 19. In the for loop we create five philosophers and start these as
independent threads, where each chopstick is accessible to two threads, one as a left chopstick,
and one as a right.
1.	private final static int NUMCHOPSTICKS = 5 ;
2.	private final static int NUMPHILOSOPHERS = 5;
3.	public static void main(String[] args) throws Exception {
4.
5.	  final Philosopher[] ph = new Philosopher[NUMPHILOSOPHERS];
6.	  Object[] chopSticks = new Object[NUMCHOPSTICKS];
7.
8.	  for (int i = 0; i < NUMCHOPSTICKS; i++) {
9.	    chopSticks[i] = new Object();
10.	  }
11.
12.	  for (int i = 0; i < NUMPHILOSOPHERS; i++) {
13.	    Object leftChopStick = chopSticks[i];
14.	    Object rightChopStick = chopSticks[(i + 1) % chopSticks.length];
15.
16.	    ph[i] = new Philosopher(leftChopStick, rightChopStick);
17.	            }
18.
19.	    Thread th = new Thread(ph[i], "Philosopher " + (i + 1));
20.	    th.start();
21.	  }
22.	}
   Figure 19 Dining Philosophers - deadlocked version
   Running this code produces the following output on my first attempt. This was lucky. If you
run the code you will almost certainly see different outputs, but the final outcome will be the
same.

   Philosopher 4 : Thinking deeply
   Philosopher 5 : Thinking deeply
   Philosopher 1 : Thinking deeply
   Philosopher 2 : Thinking deeply
   Philosopher 3 : Thinking deeply
   Philosopher 4 : Picked up left chop stick
   Philosopher 1 : Picked up left chop stick
   Philosopher 3 : Picked up left chop stick
   Philosopher 5 : Picked up left chop stick
   Philosopher 2 : Picked up left chop stick

   10 lines of output, then  nothing! We have a deadlock. This is a classic circular waiting
deadlock. Imagine the following scenario:

1.	Each philosopher indulges in a long thinking session
2.	Simultaneously, they all decide they are hungry and reach for their left chop stick.
3.	No philosopher can eat (proceed) as none can pick up their right chop stick

   The philosophers in this situation would figure out some way to proceed by putting down a
chop stick or two until one or more can eat. We can sometimes do this is our software by using
timeouts on blocking operations. When the timeout expires a thread releases the critical section
and retries, allowing other blocked threads a chance to proceed. This is not optimal though, as
blocked threads hurt performance, and setting timeout values in an inexact science.
   Its much better however to design a solution to be deadlock free. This means that one or
more threads will always be able to make progress. With circular wait deadlocks, this can be
achieved by imposing a resource allocation protocol on the shared resources, so that threads will
not always request resources in the same order.
   In the Dining Philosophers problem, we can do this by making sure one of our philosophers
picks up their right chop stick first. Lets assume we instruct Philosopher 4 to do this.  This
leads to a possible sequence of operations such as below:

   Philosopher 0 picks up left chopstick (chopStick[0]) then right (chopStick[1])
   Philosopher 1 picks up left chopstick (chopStick[1]) then right (chopStick[2])
   Philosopher 2 picks up left chopstick (chopStick[2]) then right (chopStick[3])
   Philosopher 3 picks up left chopstick (chopStick[3]) then right (chopStick[4])
   Philosopher 4 picks up left chopstick (chopStick[0]) then right (chopStick[4])

   In this example, Philosopher 4 must block, as Philosopher 0 already has acquired access to
chopstick[0]. With Philosopher 4 blocked, Philosopher 3 is assured access to chopstick[4] and
can then proceed to satisfy their appetite.
   The fix for the Dining Philosophers solution is shown in Figure 20.
1.	if (i == NUMPHILOSOPHERS - 1) {
2.	  // The last philosopher picks up the right fork first
3.	  ph[i] = new Philosopher(rightChopStick, leftChopStick);
4.	} else {
5.	  // all others pick up the left chop stick first
6.	  ph[i] = new Philosopher(leftChopStick, rightChopStick);
7.	}
8.	            }
    Figure 20 Solving the Dining Philosophers deadlock
   More formally we are imposing an ordering on the acquisition of shared resources, such that:

   chopStick[0]< chopStick[1]< chopStick[2]< chopStick[3]< chopStick[4]

   This means each thread will always attempt to acquire chopstick[0] before chopstick[1], and
chopstick[1] before chopstick[2], and so on. For philosopher 4, this means it will attempt to
acquire chopstick[0] before chopstick[4], thus breaking the potential for a circular wait deadlock.
   Deadlocks are a complicated topic and this section has just scratched the surface. Youll see
deadlocks in many deployed systems, and well revisit them later when discussing concurrent
database accessed and locking.

Thread States
   Lets briefly describe the various states that a thread can have during its lifecycle.
Multithreaded systems have a system scheduler that decides which threads to run when. In Java,
the scheduler is known as a preemptive, priority-based scheduler. In short this means it chooses
to execute the highest priority thread which wishes to run.
   Every thread has a priority (by default 5, range 0 to 10). A thread inherits its priority from its
parent thread. Higher priority threads get scheduled more frequently than lower priority threads,
but in most applications having all threads as the default priority suffices.
   Scheduling is a JVM-specific implementation detail based on the Java specification. But in
general schedulers behave as follows.
   The scheduler cycles threads through four states, based on their behavior. These are:
   Created: A thread object has been created but its start() method has not been invoked.
Once start() in invoked, the thread enters the runnable state.
   Runnable: A thread is able to run. The scheduler will choose which thread(s) to execute in a
FIFO manner. Threads then execute until they block (e.g. on a synchronized statement),
execute a yield(), suspend() or sleep() statement,  or the run() method terminates, or
are preempted by the scheduler. Preemption occurs when a higher priority thread becomes
runnable, or when a system-specific timeslice value expires. Timeslicing allows the scheduler to
ensure that all threads eventually get chance to execute  no execution hungry threads can hog
the CPU.
   Blocked: A thread is blocked if it is waiting for a lock, a notification event to occur (e.g.
sleep timer to expire, resume() method executed), or is waiting for a network or disk request to
complete. When the event a blocked thread is waiting for occurs, it moves back to the runnable
state.
   Terminated: A threads run() method has completed or it has called the stop() method. The
thread will no longer be scheduled.
   An illustration of this scheme is in Figure 21. The scheduler effectively maintains FIFO
queue in the Runnable state for each thread priority. High priority threads are used typically to
respond to events (eg an emergency timer), and execute for a short period of time. Low priority
threads are used for background, ongoing tasks like checking for corruption of files on disk
through recalculating checksums.

   Figure 21 Threads states and transitions

Thread Coordination
   There are many problems that require threads with different roles to coordinate their
activities. Imagine a collection of threads that accept a document, do some processing on that
document (e.g. generate a pdf), and then send the processed document to a shared printer pool.
Each printer can only print one document at a time, so it reads from a shared print queue,
printing documents in the order they arrive.
   This printing problem is an illustration of the classic producer-consumer problem. Producers
generate and send messages via a shared FIFO buffer to consumers. Consumers retrieve these
messages, process them, and then go to try get more work from the buffer. A simple illustration
of this problem is in Figure 22. Its a bit like a 24 hour, 365 day restaurant in New York, the
kitchen keeps producing and the wait staff collect the food and deliver to hungry diners.
Forever.
   Like virtually all real things, the buffer has a limited capacity. Producers generate new items,
but if the buffer is full, they must wait until some item(s) have been consumer before they can
add the new item to the buffer.  Similarly, if the consumers are consuming faster then the
producers are producing, them they must wait if there are no items in the buffer, and somehow
get alerted when new items arrive.

   Figure 22 The Producer Consumer Problem
   One way for a producer to wait for space in the buffer, or a consumer to wait for an item, is
to keep retrying an operation. A producer could sleep for a second, and then retry the put
operation until it succeeds. A consumer could do likewise.
   This solution is called polling, or busy waiting. It works fine, but as the second name implies,
each producer and consumer are using resources (CPU, memory, maybe network?) each time it
retries and fails. If this is not a concern, then cool, but in scalable systems were always aiming
to optimize resource usage, and polling can be wasteful.
   A better solution is for producers and consumers to block until their desired operation, put
or get respectively, can succeed. Blocked threads consume no resources and hence provide an
efficient solution. To facilitate this, thread programming models provide blocking operations
that enable threads to signal to other threads when an event occurs. With the producer-
consumer problem, the basic scheme is as follows:

*	When a producer adds an item to the buffer, it sends a signal to any blocked consumers
to notify them that there is an item in the buffer
*	When a consumer retrieves an item from the buffer, it sends a signal to any blocked
producers to notify them there is capacity in the buffer for new items.

   In Java, the two basic primitives are wait() and notify(). Briefly, they work like this:
*	A thread may call wait() within a synchronized block if some condition it requires to
hold is not true. For example, a thread may attempt to retrieve a message from a buffer,
but if the buffer has no messages to retrieve, it calls wait() and blocks until another
thread adds a message,  sets the condition to true, and calls notify() on the same object.
*	notify() wakes up a thread that has called wait() on the object.

   These Java primitives are used to implement guarded blocks. Guarded blocks use a condition
as a guard that must hold before a thread resumes the execution. The code snippet below shows
how the guard condition, empty, is used to block a thread that is attempting to retrieve a message
from an empty buffer.
1.	while (empty) {
2.	  try {
3.	    System.out.println("Waiting for a message");
4.	    wait();
5.	  } catch (InterruptedException e) {}
6.	}
    When another thread adds a message to the buffer, it executes notify() as in the code
fragment below.
1.	// Store message.
2.	this.message = message;
3.	empty = false;
4.	// Notify consumer that message is available
5.	notify();
    The full implementation of this example is given in the code examples. There are a number
of variations of the wait() and notify() methods, but these go beyond the scope of what we can
cover in this overview. And luckily, Java provides us with abstractions that hide this complexity
from out code.
   An example that is pertinent to the producer-consumer problem is the BlockingQueue
interface in java.util.concurrent.BlockingQueue. A BlockingQueue implementation
provides a thread-safe object that can be used as the buffer in a producer-consumer scenario.
There are 5 different implementations of the BlockingQueue interface. Lets use one of these,
the LinkedBlockingQueue, to implement the producer-consumer. This is shown in Figure 23.
1.	class ProducerConsumer {
2.	   public static void main(String[] args)
3.	     BlockingQueue buffer = new LinkedBlockingQueue();
4.	     Producer p = new Producer(buffer);
5.	     Consumer c = new Consumer(buffer);
6.	     new Thread(p).start();
7.	     new Thread(c).start();
8.	   }
9.	 }
10.
11.	class Producer implements Runnable {
12.	   private boolean active = true;
13.	   private final BlockingQueue buffer;
14.	   public Producer(BlockingQueue q) { buffer = q; }
15.	   public void run() {
16.
17.	     try {
18.	       while (active) { buffer.put(produce()); }
19.	     } catch (InterruptedException ex) { // handle exception}
20.	   }
21.	   Object produce() { // details omitted, sets active=false }
22.	 }
23.
24.	 class Consumer implements Runnable {
25.	   private boolean active = true;
26.	   private final BlockingQueue buffer;
27.	   public Consumer(BlockingQueue q) { buffer = q; }
28.	   public void run() {
29.
30.	     try {
31.	       while (active) { consume(buffer.take()); }
32.	     } catch (InterruptedException ex) { // handle exception }
33.	   }
34.	   void consume(Object x) {  // details omitted, sets active=false }
35.	 }
36.
   Figure 23 Producer-Consumer with a LinkedBlockingQueue
   This solution absolves the programmer from having to be concerned with the
implementation of coordinating access to the shared buffer, and greatly simplifies the code.
   The java.util.concurrent  package is a treasure trove for building multithreaded Java
solutions. The following sections will briefly highlight a few more powerful and extremely useful
capabilities.
Thread Pools
   Many multithreaded systems need to create and manage a collection of threads that perform
similar tasks. For example, in the producer-consumer problem, we can have a collection of
producer threads and a collection of consumer threads, all simultaneously adding and removing
items, with coordinated access to the shared buffer.
   These collections are known as thread pools. Thread pools comprise several worker threads,
which typically perform a similar purpose and are managed as a collection. We could create a
pool of producer threads which wait for an item to process, write the final product to the
buffer, and then wait to accept another item to process. When we stop producing items, the
pool can be shutdown in a safe manner, so no partially processed items are lost through an
unanticipated exception.
   In java.util.concurrent, thread pools are supported by the ExecutorService interface.
This extends the base Executor interface with a set of methods to manage and terminate
threads in pool. A simple producer-consumer example using a fixed size thread pool  is shown
in Figure 24 and Figure 25. The Producer class in Figure 24 is a Runnable that sends a single
message to the buffer and then terminates. The Consumer simply takes messages from the
buffer until an empty string is received, upon which it terminates.

1.	class Producer implements Runnable {
2.
3.	  private final BlockingQueue buffer;
4.
5.	  public Producer(BlockingQueue q) { buffer = q; }
6.
7.	  @Override
8.	  public void run() {
9.
10.	  try {
11.	    sleep(1000);
12.	    buffer.put("hello world");
13.
14.	  } catch (InterruptedException ex) {
15.	    // handle exception
16.	  }
17.	 }
18.	}
19.
20.	class Consumer implements Runnable {
21.	  private final BlockingQueue buffer;
22.
23.	  public Consumer(BlockingQueue q) { buffer = q; }
24.
25.	  @Override
26.	   public void run() {
27.	      boolean active = true;
28.	      while (active) {
29.	          try {
30.	             String  s = (String) buffer.take();
31.	             System.out.println(s);
32.	             if (s.equals("")) active = false;
33.	          } catch (InterruptedException ex) {
34.	              / handle exception
35.	          }
36.	      } /
37.	      System.out.println("Consumer terminating");
38.	    }
39.	 }
40.
   Figure 24 Producer and Consumer for thread pool implementation
   In Figure 25, we create a single consumer to take messages from the buffer. We then create
fixed size thread pool of size 5 to manage our producers. This causes the JVM to pre-allocate
five threads that can be used to execute any Runnable objects that are executed by the pool.
   In the for() loop, we then use the ExecutorService to run 20 producers. As there are
only 5 threads available in the thread pool, only a maximum of 5 producers will be executed
simultaneously. All others are placed in wait queue which is managed by the thread pool. When
a producer terminates, the next Runnable in the wait queue is executed using any an available
thread in the pool.
   Once we have requested all the producers to be executed by the thread pool, we call the
shutdown() method on the pool. This tells the ExecutorService not to accept any more tasks
to run. We next call  the awaitTermination() method, which blocks the calling thread until
all the threads managed by the thread pool are idle and no more work is waiting in the wait
queue. Once awaitTermination() returns, we know all messages have been sent to the
buffer, and hence send an empty string to the buffer which will act as a termination value for
the consumer.
1.	public static void main(String[] args) throws
InterruptedException
2.	  {
3.	    BlockingQueue buffer = new LinkedBlockingQueue();
4.
5.	    //start a single consumer
6.	    (new Thread(new Consumer(buffer))).start();
7.
8.	    ExecutorService producerPool = Executors.newFixedThreadPool(5);
9.	    for (int i = 0; i < 20; i++)
10.	      {
11.	        Producer producer = new Producer(buffer) ;
12.	        System.out.println("Producer created" );
13.	        producerPool.execute(producer);
14.	      }
15.
16.	      producerPool.shutdown();
17.	      producerPool.awaitTermination(10, TimeUnit.SECONDS);
18.
19.	      //send termination message to consumer
20.	      buffer.put("");
21.	    }
   Figure 25 Thread pool-based Producer Consumer solution
   Like most topics in this chapter, theres many more sophisticated features in the Executor
framework that can be used to create multithreaded programs. This description has just covered
the basics. Thread pools are important as they enable our systems to rationalize the use of
resources for threads. Every thread consumes memory, for example the stack size for a thread is
typically around 1MB. Also, when we switch execution context to run a new thread, this
consumes CPU cycles. If our systems create threads in an undisciplined manner, we will
eventually run out of memory and the system will crash. Thread pools allow us to control the
number of threads we create, and utilize them efficiently.
   Well revisit thread pools throughout the remainder of this book, as they are a key concept
for efficient and scalable management of ever the increasing request loads that servers must
satisfy.

Barrier Synchronization
   I had a high school friend whose family, at dinner times, would not allow anyone to start
eating until the whole family was seated at the table. I thought this was weird, but many years
later it serves as a good analogy for the concept known as barrier synchronization. Eating
commenced only after all family members arrived at the table.
   Multithreaded systems often need to follow such a pattern of behavior. Imagine a
multithreaded image processing system. An image arrives and a non-overlapping segment of the
image is passed to each thread to perform some transformation upon  think Instagram filters
on sterioids. The image is only fully processed when all threads have completed. In software
systems, we use a mechanism called barrier synchronization to achieve this style of thread
coordination.
   The general scheme is shown in Figure 26. In this example, the main() thread creates four
new threads and all proceed independently until they reach the point of execution defined by
the barrier. As each thread arrives, it blocks. When all threads have arrived at this point, the
barrier is released, and each thread can continue with its processing.

   Figure 26 Barrier Synchronization
   Java providers three primitives for barrier synchronization. Lets see how one of the three,
namely the CountDownLatch, works.
   When you create a CountDownLatch, you pass a value to its constructor that represents the
number of threads that must bock at the barrier before they are all allowed to continue. This is
called in the thread which is managing the barrier points for the system  in Figure 26this would
be main().

   CountDownLatch  nextPhaseSignal = new CountDownLatch(numThreads);

   Next we create the worker threads that will perform some actions and then block at the
barrier until they all complete. To do this, we need to pass each thread a reference to
CountDownLatch.

   for (int i = 0; i < numThreads; i++) {
               Thread worker = new Thread(new
WorkerThread(nextPhaseSignal));
               worker.start();
           }

   After launching the worker threads, the main() thread will call the .await() method to
block until the latch is triggered by the worker threads.

   nextPhaseSignal.await();

   Each worker thread will complete its task and before exiting call the .countDown() method
on the latch. This decrements the latch value. When the last thread calls .countDown() and the
latch value becomes zero, all threads that have called .await() on the latch transition from the
blocked to the runnable state. In our example this would be the main()  thread. At this stage we
are assured that all workers have completed their assigned task.

   nextPhaseSignal.countDown();

   Any subsequent calls to .countDown() will return immediately as the latch has been
effectively triggered. Note .countDown() is non-blocking, which is a useful property for
applications in which threads have more work to do after reaching the barrier.
   Our example illustrates using a CountDownLatch to block a single thread until a collection
of threads have completed their work. We can invert this use case with a latch however if we
initialize its value to one. Multiple threads could call .await() and block until another thread
calls .countDown()  to release all waiting threads. This example is analogous to a simple gate,
which one thread opens to allow a collection of others to continue.
   CountDownLatch is a simple barrier synchronizer. Its a single use tool, as the initializer
value cannot be reset. More sophisticated features are provided by the CyclicBarrier and
Phaser classes in Java and hopefully armed with the knowledge of how barrier synchronization
works from this section, these will be straightforward to understand.
Thread-Safe Collections
   Many programmers, once they delve into the wonders of multithreaded programs, are
surprised to discover that the collections in the java.util package  are not thread safe. Why, I
hear you ask? The answer, luckily, is simple. Its to do with performance. Calling synchronized
methods incurs overheads. Hence to attain faster execution for single threaded programs, the
collections are not thread-safe.
   If you want to share an ArrayList, Map or your favorite data structure from java.util
across multiple threads, you must ensure modifications to the structure are placed in critical
sections. This approach places the burden on the client of the collection to safely make updates,
and hence is error prone  a programmer might forget to make modifications in a
synchronized block.
   Its always safer to use inherently thread-safe collections in out multithreaded code. For this
reason, the Java collections framework provides a factory method that creates a thread-safe
version of java.util collections. Heres an example of creating a thread-safe list.

   List<String> list = Collections.synchronizedList(new ArrayList<>());

   Whats really happening here is that we are creating a wrapper around the base collection
class, which has synchronized methods. These delegate the actual work to the original class, in
a thread-safe manner of course. You can use this approach for any collection in the java.util
package, and the general form is:

   Collections.synchronized.(collection)

   where . is List, Map, Set, and so on.
   Of course, when using the synchronized wrappers, you pay the performance penalty for
acquiring the monitor lock and serializing access from multiple threads. This means the whole
collection is locked while a single thread makes a modification, greatly limiting concurrent
performance (remember Amdahls Law?). For this reason, Java 5.0 included the concurrent
collections package, namely java.util.concurrent. It contains a rich collection of classes
specifically designed for efficient multithreaded access.
   In fact weve already seen one of these classes  the LinkedBlockingQueue. This uses a
locking mechanism that enables items to be added to and removed from the queue in parallel.
This finer grain locking mechanism utilizes the java.util.concurrent.lock.Lock class
rather than the monitor lock approach. This allows multiple locks to be utilized on the same
collection, hence enabling safe concurrent access.
   Another extremely useful collection that provides this finer-grain locking is the
ConcurrentHashMap. This provides the same methods as the non-thread safe Hashtable, but
allows non-blocking reads and concurrent writes based on a concurrencyLevel value you can
pass to the constructor (the default value is 16).

   ConcurrentHashMap (int initialCapacity, float loadFactor,
                        int concurrencyLevel)

   Internally, the hash table is divided into individually lockable segment, often known as
shards. This means updates can be made concurrently to hash table entries in different shards of
the collection, increasing performance.
   There are a number of subtle issues concerning iterators and collections, but these are
beyond the scope of this chapter. Well however investigate some of these in the exercises at the
end of the chapter.
Summary and Further Reading
   This chapter has only brushed the surface of concurrency in general and its support in Java.
The best book to continue learning more about the basic concepts of concurrency in is the
classic Java Concurrency in Practice by Brian Goetz et al. If you understand everything in this
book, youll be writing pretty great concurrent code.
   Java concurrency support has moved on considerably however since Java 5.  In the world of
Java 12 (or whatever version is current when you read this), there are new features such as
CompleteableFutures, lambda expressions and parallel streams. The functional programming
style introduced in Java 8.0 makes it easy to create concurrent solutions without directly creating
and managing threads. A good source of knowledge for Java 8.0 features is Mastering
Concurrency Programming with Java 8 by Javier Fernndez Gonzlez.

   Other excellent sources include:
*	Doug Lea, Concurrent Programming in Java: Design Principles and Patterns, 2nd
Edition
*	Raoul-Gabriel Urma, Mario Fusco, and Alan Mycroft, Java 8 in Action: Lambdas,
Streams, and functional-style programming, manning Publications, 1st Edition, 2014.

Exercises

   volatile
   thread safe v non thread safe collection  performance, including CHP sharding
   copyonwrite example  immutable, slow

   collections, iterators

CHAPTER 4
__________________________

Distributed Systems Fundamentals

   Scaling a system naturally involves adding multiple independently moving parts. We run our
software components on multiple machines and our databases across multiple storage nodes, all
in the quest of adding more capacity. Consequently, our solutions are distributed across multiple
machines in multiple locations, with each machine processing events concurrently, and
exchanging messages over a network.
   This fundamental nature of distributed systems has some profound implications on the way
we design, build and operate our solutions. This chapter provides the basic nature of the beast
information you need to know to appreciate the issues and complexities of distributed software
systems. We briefly cover communications networks hardware and software, how to deal with
the implications of communications failures, distributed coordination, and the complex issue of
time in distributed systems.
Communications Basics
   Every distributed system has software components that communicate over a network. If a
mobile banking app requests the users current bank account balance, a (simplified) sequence of
communications occurs along the lines of:

1.	The cell phone app sends a request over the cellular network addressed to the bank to
retrieve the users bank balance
2.	The request is routed across the internet to where the banks web servers are located.
3.	The banks web server authenticates the request (checks its really the supposed user)
and sends a request to a database server for the account balance.
4.	The database server reads the account balance from disk and returns it to the web server
5.	The web server sends the balance in a reply message addressed to the app, which is
routed over the internet and the cellular network until the balance magically appears on
the screen of the mobile device

   It almost sounds simple when you read the above, but in reality, theres a huge amount of
complexity hidden beneath this sequence of communications. Lets examine some of these in
the following sections.
  Communications Hardware
   The bank balance request will inevitably traverse multiple different networking technologies
and devices. The global internet is a heterogeneous machine, comprising different types of
network communications channels and devices that shuttle many millions of messages a second
across networks to their intended destinations.
   Different types of communications channels exist. The most obvious categorization is wired
versus wireless. For each category there are multiple network transmission hardware
technologies that can ship bits from one machine to another. Each technology has different
characteristics, and the ones we typically care about are speed and range.
   For physically wired networks, the two most common types are local area networks (LANs)
and wide area networks (WANs). LANs are networks that can connect devices at building
scale, being able to transmit data over a small number (e.g. 1-2) of kilometers. Contemporary
LANs can transport between 100 megabits per second (Mbps) to 1 gigabits per second (Gbps).
This is known as the networks bandwidth, or capacity. The time taken to transmit a message
across a LAN  the networks latency  is sub-millisecond with modern LAN technologies.
   WANs are networks that traverse the globe and make up what we collectively call the
internet. These long-distance connections are the high speed data pipelines connecting cites
and countries and continents with fiber optic cables. These cables support a networking
technology known as wavelength division multiplexing  which makes it possible to transmit up
171 Gbps over 400 different channels, giving more than 70 Terabits per second (Tbps) of total
bandwidth for a single fiber link. The fiber cables that span the world normally comprise four or
more strands of fiber, giving bandwidth capacity of hundreds of Tbps for each cable.
   Latency is more complicated with WANs however. WANs transmit data over 100s to 1000s
of kilometers, and the maximum speed that the data can travel is the theoretical speed of light.
In reality, fiber optic cables cant reach the speed of light, but do get pretty close to it as we can
see in Table 1.

Path
Distance
Time -
Speed of
Light
Time - Fiber
Optic Cable
New York to
San Francisco
4,148 km
14 ms
21 ms
New York to
London
5,585 km
19 ms
28 ms
New York to
Sydney
15,993 km
53 ms
80 ms
   Table 1 WAN Speeds
   Actual times will be slower than this as the data needs to pass through networking
equipment known as routers . Routers are responsible for transmitting data on the physical
network connections to ensure data is transmitted across the internet from source to
destination. Routers are specialized, high speed devices that can handle several hundred Gbps of
network traffic, pulling data off incoming connections and sending the data out to different
outgoing network connections based on their destination. Routers at the core of the internet
comprise racks of these devices and hence can process 10s to hundreds of Tbps. This is how
you and 1000s of your friends get to watch a steady video stream on Netflix.
   Wireless technologies have different range and bandwidth characteristics. Wi-Fi routers that
we are all familiar with in our homes and offices are wireless ethernet networks and use 802.11
protocols to send and receive data. The most widely used Wi-Fi protocol, 802.11ac, allows for
maximum (theoretical) data rates of up to 5,400Mbps. The most recent 802.11ax protocol, also
known as Wi-Fi 6, is an evolution of 802.11ac technology that promises increased throughput
speeds of up to 9.6Gbps. The range of Wi-Fi routers is of the order of 10s of meters, and of
course is affected by physical impediments like walls and floors.
   Cellular wireless technology uses radio waves to send data from our phones to routers
mounted on cell towers, which are generally connected by wires to the core internet for message
routing. Each cellular technology introduces improved bandwidth and other dimensions of
performance. The most common technology at the time of writing is 4G LTE wireless
broadband. 4G LTE is around 10 times faster than the older 3G, able to handle sustained
download speeds around 10 Mbps (peak download speeds are nearer 50 Mbps) and upload
speeds between 2 and 5 Mbps.
   Emerging 5G cellular networks promise 10x bandwidth improvements over existing 4G,
with 1-2 millisecond latencies between devices and cell towers. This is a great improvement over
4G latencies which are in the 20-40 millisecond range. The trade-off is range. 5G base station
range operates at about 500m maximum, whereas 4G provides reliable reception at distances of
10-15kms.
   This whole collection of different hardware types for networking comes together in the
global internet. The internet is heterogeneous network, with many different operators globally
and every type of hardware imaginable. Figure 27 (from Wikipedia ) shows a simplified view of
the major components that comprise the internet. Tier 1 networks are the global high-speed
internet backbone. There are around 20 Tier 1 Internet Service Providers (ISPs) who manage
and control global traffic. Tier 2 ISPs are typically regional (e.g. one country), have lower
bandwidth than Tier 1 ISPs, and deliver content to customers through Tier 3 ISPs. Tier 3 ISPs
are the ones that charge your exorbitant fees for your home internet every month.

   Figure 27 Simplified view of the Internet (from Wikipedia)
   Theres a lot more complexity to how the internet works than described here. That
complexity is beyond the scope of this chapter. From a distributed systems software
perspective, we need to understand more about the magic that enables all this hardware to
route messages from say my cell phone, to my bank and back. This is where the IP protocol
comes in.

  Communications Software
   Software systems on the internet communicate using the Internet Protocol suite. The
Internet Protocol suite specifies host addressing, data transmission formats, message routing
and delivery characteristics. There are four abstract layers, which contain related protocols that
support the functionality required at that layer. These are, from lowest to highest:

1)	the data link layer, specifying communication methods for data across a single network
segment. This is implemented by the device drivers and network cards that live inside
your devices.
2)	the internet layer specifies addressing and routing protocols that make it possible for
traffic to traverse the independently managed and controlled networks that comprise the
internet. This is the IP protocol in the internet protocol suite.
3)	the transport layer, specifying protocols for reliable and best-effort host-to-host
communications. This is where the well-known TCP and UDP protocols live.
4)	the application layer, which comprises several application level protocols such as HTTP
and SCP.

   Each of the higher layer protocols builds on the features of the lower layers. The enables the
IP suite to support end to end data communications across the internet. In the following, well
briefly cover the IP protocol for host discovery and message routing, and the TCP and UDP
transport protocols that can be utilized by distributed applications.
   Internet Protocol (IP)
   IP defines how hosts are assigned addresses on the internet and how messages are
transmitted between two hosts who know each others addresses.
   Every device on the internet has its own address. These are known as Internet Protocol (IP)
addresses. The location of an IP address can be found using an internet wide directory service
known as Domain Naming Service (DNS). DNS is a widely distributed, hierarchical database
that acts as the address book of the internet.
   The technology currently used to assign IP addresses, known as Internet Protocol version 4
(IPv4), will eventually be replaced by its successor, IPv6. IPv4 is a 32-bit addressing scheme that
before long will run out of addresses due to the number of devices connecting to the internet.
IPv6 is a 128-bit scheme that will offer an (almost) infinite number of IP addresses. As an
indicator, in July 2020 about 33% of the traffic processed by Google.com  is IPv6.
   DNS servers are organized hierarchically. A small number of root DNS servers, which are
highly replicated, are the starting point for resolving and an IP address. When an internet
browser tries to find a web site, a network host known as the local DNS server that is managed
by your employer or ISP, will contact a root server with the requested host name. The root
server replies with a referral to a so-called authoritative DNS server that manages name resolution
for, in our banking example, .com addresses. There is an authoritative name server for each top-
level internet domain (e.g. .com, .org, .net, etc).
   Next the local DNS server will query the .com DNS server, which will reply with the address
of the DNS server which knows about all the IP addresses managed by mybank.com. This DNS
is queried, and it returns the actual IP address we need to communicate with the application.
The overall scheme is illustrated in Figure 28.

   Figure 28 Example DNS Lookup for mybank.com
   The whole DNS database is highly geographically replicated so there are no single points of
failure, and requests are spread across multiple physical servers. Local DNS servers also
remember the IP addresses of recently contacted hosts, which is possible as IP addresses dont
change very often. This means the complete name resolution process doesnt occur for every
site we contact.
   Armed with a destination IP address, a host can start sending data across the network as a
series of IP packets. IP has the task of delivering data from the source to the destination host
based on the IP addresses in the packet headers. IP defines a packet structure that contains the
data to be delivered, along with header data including source and destination IP addresses. Data
sent by an application is broken up into a series of packets which are independently transmitted
across the Internet.
   IP is known as a best-effort delivery protocol. This means it does not attempt to compensate
for the various error conditions that can occur during packet transmission. Possible
transmission errors include data corruption, packet loss and duplication. In addition, as every
packet is routed from source to destination independently, different packets may be delivered to
the same destination via different network paths, resulting in out-of-order delivery to the
receiver. Treating every packet independently is known as packet-switching. This allows the
network to dynamically respond to conditions such as link failure and congestion, and hence is a
defining characteristic of the internet.
   Because of this design, the IP is unreliable. If two hosts require reliable data transmission,
they need to add additional features to make this occur. This is where the next layer in the IP
protocol suite, the transport layer, enters the scene.

   Transmission Control Protocol (TCP)
   Once an application or browser has discovered the IP address of the server it wishes to
communicate with, it can send messages using the transport protocol API. This is achieved
using Transmission Control Protocol (TCP) or User Datagram Protocol (UDP), which are the
established standard transport protocols for the IP network stack.
   Distributed applications can choose which of these protocols to use. APIs for both TCP and
UDP are widely available in mainstream programming languages such as Java, Python and C++.
In reality, use of these APIs is not common as higher-level programming abstractions hide the
details from most applications. In fact, the IP protocol suite application layer contains several of
these application level APIs, including HTTP, which is very widely used in mainstream
distributed systems. Still, its important to understand TCP, UDP and their differences.
   Most requests on the internet are sent using TCP. TCP is:

*	connection-oriented
*	stream-oriented
*	reliable

   TCP is a connection-oriented protocol. Before any messages are exchanged between
applications, TCP uses a 3-step handshake to establish a two-way connection between the client
and server applications. The connection stays open until the TCP client calls close to terminate
the connection with the TCP server. The server responds by acknowledging the close request
before the connection is dropped.
   Once a connection is established, a client sends a sequence of requests to the server as a data
stream. When a data stream is sent over TCP, it is broken up into individual network packets,
with a maximum packet size of 65535 bytes. Each packet contains a source and destination
address, which is used by the underlying IP protocol to route the messages across the network.
   The internet is a packet-switched network, which means every packet is individually routed
across the network. The route each packet traverses can vary dynamically based on the
conditions in the network, such as link congestion or failure. This means the packets may not
arrive at the server in the same order they are sent from the client. To solve this problem, a TCP
sender includes a sequence number in each packet so the receiver can reassemble packets into a
stream that is identical to the order they were sent.
   Reliability is needed as network packets can be lost or delayed during transmission between
sender and receiver. To achieve reliable packet delivery, TCP uses a cumulative
acknowledgement mechanism. This means a receiver will periodically send an acknowledgement
packet that contains the highest sequence number of the received packets. This implicitly
acknowledges all packets sent with a lower sequence number, meaning all have been successfully
received. If a sender doesnt receive an acknowledgement within a timeout period, the packet is
resent.
   TCP has many other features, such as checksums to check packet integrity, and dynamic
flow control to ensure a sender doesnt overwhelm a slow receiver by sending data too quickly.
Along with connection establishment and acknowledgments, this makes TCP a relatively
heavyweight protocol, which trades off reliable over efficiency.
   This is where UDP comes into the picture. UDP is a simple connectionless protocol, which
exposes the user's program to any unreliability of the underlying network. There is no guarantee
of in order delivery, or even delivery for that matter. It can be thought of as a thin veneer (layer)
on top of the underlying IP protocol, and deliberately trades off raw performance over
reliability. This however is highly appropriate for many modern applications where the odd lost
packet has very little effect. Think streaming movies, video conferencing and gaming, where one
lost packet is unlikely to be perceptible by a user.

   Figure 29 Comparing TCP and UDP
   Figure 29 depicts some of the major differences between TCP and UDP. TCP incorporates
a connection establishment 3-packet handshake, and piggybacks acknowledgements (ACK) of
packets so that any packet loss can be handled by the protocol. Theres also a TCP connection
close phase involving a 4-way handshake that is not shown in the diagram. UDP dispenses with
connection establishment, tear down, acknowledgements and retries. Hence applications using
UDP need to be tolerant of packet loss and client or server failures and behave accordingly.
Remote Method Invocation
   Its perfectly feasible to write our distributed applications using APIs that interact directly
with transport layer protocols like TCP. If we use for example the TCP sockets library, we can
create a connection, known as a socket, between a client and a server and exchange data over
that connection.
   A socket is basically a pipe between the client and server. Once the socket is created, the
client sends data to the server in a stream. In our bank example, the client might request a
balance for the users checking account. Ignoring specific language issues (and security!!), the
client might send a message payload as follows over a socket to the server:

   {balance, 000169990}

   In this message, balance represents the operation we want the server to execute, and
000169990 is the bank account number.
   In the server, we need to know that the first string in the message is the operation identifier,
and based on this value being balance, the second is the bank account number. The server
then uses these values to presumably query a database, retrieve the balance and send back the
results, perhaps as a message formatted with the account number and current balance, as below:

   {000169990, 220.77}

   In any complex system, the server will support many operations. In mybank.com, we might
have for example login, transfer, address, statement, transactions, and so on. Each
will be followed by different message payloads that the server needs to interpret correctly to
fulfill the clients request.
   What we are defining here is an application specific protocol. As long as we send the necessary
values in the correct order for each operation, the server will be able to respond correctly. If we
have an erroneous client that doesnt adhere to our application protocol, well, our server needs
to do thorough error checking.
   Stepping back, if we were defining the mybank.com server in an object-oriented language
such as Java, we would have each operation it can process as a method, which is passed an
appropriate parameter list for that operation, as shown in Figure 30.
1.	// Simple mybank.com server interface
2.	public interface MyBank {
3.	    public float balance  (String accNo);
4.	    public boolean  statement(String month) ;
5.	    // other operations
6.	 }
   Figure 30 Simple mybank.com server interface
   There are several advantages of having such an interface, namely:
*	Calls from the client to the server can be statically checked by the compiler to ensure
they are of the correct type
*	Changes in the server interface (e.g. add a new parameter) force changes in the client
code to adhere to the new method signature
*	The interface is clearly defined by the class definition, and hence straightforward for a
client programmer to utilize

   These benefits of an explicit interface are of course well known in sequential programming.
The whole discipline of object-oriented design is pretty much based upon these foundations,
where an interface defines a contract between the caller and callee. Compared to the implicit,
application protocol we need to program to with sockets, the advantages are significant.
   This fact was recognized reasonably early in the evolution of distributed systems. Since the
early 1990s, we have seen an evolution of technologies that enable us to define explicit server
interfaces and call these across the network using essentially the same syntax as we would in a
sequential program. A summary of the major approaches is given in Table 2. Collectively they
are known as Remote Procedure Call (RPC) or Remote Method Invocation (RMI) technologies.

Technology
Dates
Main features
Distributed Computing
Environment (DCE)
Early
1990s
DCE RPC provides a standardized approach for
client-server systems. Primary languages were
C/C++.
Common Object Request
Broker Architecture
(CORBA)
Early
1990s
Facilitates language-neutral client-server
communications based on an object-oriented
Interface Definition Language (IDL). Primary
language support in C/C++, Java, Python, Ada.
Java Remote Method
Invocation (RMI)
Late
1990s
A pure Java-based remote method invocation
that facilitates distributed client-server systems
with the same semantics as Java objects.
XML Web Services
2000
Supports client-server communications based on
HTTP and XML. Servers define their remote
interface in the Web Services Description
Language (WSDL)
   Table 2 Summary of major RPC/RMI Technologies
   While the syntax and semantics of these RPC/RMI technologies vary, the essence of how
each operates is the same. Lets continue with our Java example of mybank.com to use this as
an example of the whole classification.
   Using Java RMI, we can trivially make our MyBank interface in Figure 30 a remote interface,
as shown in Figure 31.
1.	import java.rmi.*;
2.	// Simple mybank.com server interface
3.	public interface MyBank extends Remote{
4.	    public float balance  (String accNo)
5.	         throws RemoteException;
6.	    public boolean  statement(String month)
7.	         throws RemoteException ;
8.	    // other operations
9.	 }
   Figure 31 Java RMI Example
   The empty java.rmi.Remote interface serves as a marker to inform the Java compiler we
are creating an RMI server. In addition, each method must throw
java.rmi.RemoteException. These exceptions represent errors that can occur when a
distributed call between two objects is invoked over a network. The most common reasons for
such an exception would be a communications failure or the server object having crashed.
   We then must provide a class that implements this remote interface. Figure 32 shows an
extract of the server implementation - the complete code for this example is in the Chapter 4
code repository. Points to note are:
*	The server extends the UnicastRemoteObject class. This essentially provides the
functionality to instantiate remotely callable object.
*	Once the server object is constructed, its availability must be advertised to remote
clients. This is achieved by storing a reference to the object in the RMI Registry, and
associating a logical name with it  in this example, MyBankServer. The registry is a
simple directory service that enables clients to look up the location (network address and
object reference) of and obtain a reference to an RMI server.
1.	public class MyBankServer extends UnicastRemoteObject
2.	                          implements MyBank  {
3.	            // constructor/method implementations omitted
4.	   public static void main(String args[]){
5.	        try{
6.	          MyBankServer server=new MyBankServer();
7.	          // create a registry in local JVM on default port
8.	          Registry registry = LocateRegistry.createRegistry(1099);
9.	          registry.bind("MyBankServer", server);
10.	          System.out.println("server ready");
11.	        }catch(Exception e){
12.	                 // code omitted for brevity}
13.	        }
14.	   }
    Figure 32 Implementing an RMI Server
   An extract from the client code to connect to the server is shown in Figure 33. It obtains a
reference to the remote object by performing a lookup operation (line 3) in the RMI Registry
and specifying the logical name that identifies the server. The reference returned by the lookup
operation can then be used to call the server object in the same manner a local object. However
there is a difference  the client must be ready to catch a RemoteException that will be thrown
by the Java runtime when the server object cannot be reached.
1.	 // obtain a remote reference to the server
2.	 MyBank bankServer=
3.	        (MyBank)Naming.lookup("rmi://localhost:1099/MyBankServer");
4.	 //now we can call the server
5.	 System.out.println(bankServer.balance("00169990"));
    Figure 33 Implementing an RMI Client
   Figure 34 depicts the call sequence amongst the components that comprise a RMI system.
The Stub and Skeleton are compiler-generated objects that facilitate the remote communications.
The skeleton is a TCP network endpoint (host, port) that listens for calls to the associated
server. The sequence of operations is as follows:

1.	When the server reference is stored in the RMI Registry, the entry contains the
client stub that can be used to make remote calls to the server.
2.	The client queries the registry, and the stub for the server is returned.
3.	The client stub accepts a method call to the server interface from the client
4.	The stub transforms the request into a network request to the server host. This
transformation process is known as marshalling.
5.	The skeleton accepts network requests from the client, and unmarshalls the network
packet data into a valid call to the server  object implementation
6.	The skeleton waits for the method to return a result
7.	The skeleton marshalls the method results into a network reply packet that is sent
the client
8.	The stub unmarshalls the data passes the result to the client call site

   Figure 34 Schematic depicting the call sequence for establishing a connection and making a call to a RMI
server object
   This Java RMI example illustrates the basics that are used for implementing any RPC/RMI
mechanism, even in modern languages like Erlang  and Go . Regardless of implementation,
the basic attraction of these approaches to is to provide an abstract calling mechanism that
provides  location transparency for clients making remote server calls.
   RPC/RMI is not without its flaws. Marshalling and unmarshalling can become inefficient for
complex object parameters. Cross language marshalling  client in one language, server in
another  can cause problems due to types being represented differently in different languages,
causing subtle incompatibilities. And if a remote method signature changes, all clients need to
obtain a new compatible stub which can be cumbersome in large deployments.
   For these reasons, most modern systems are built around simpler protocols based on HTTP
and using JSON for parameter representation. Instead of operation names, HTTP verbs (PUT,
GET, POST, etc) have associated semantics that are mapped to a specific URL. This approach
originated in the work by Roy Fielding on the REST approach . REST has a set of semantics
that comprise a RESTful architecture style, and in reality, most systems do not adhere to these.
Well discuss REST and HTTP API mechanisms in the Chapter 5.
Partial Failures
   The components of distributed systems communicate over a network. In communications
technology terminology, the shared local and wide area networks that our systems communicate
over are known as asynchronous networks. With asynchronous networks:

*	nodes can choose to send data to other nodes at any time
*	The network is half-duplex, meaning that one node sends a request and must wait for a
response from the other. These are two separate communications.
*	The time for data to be communicated between nodes is variable, due to reasons like
network congestion, dynamic packet routing and transient network connection failures
*	The receiving node may not be available due to a software or machine crash
*	Data can be lost. In wireless networks, packets can be corrupted and hence dropped due
to weak signals or interference. Internet routers can drop packets during congestion .
*	Nodes do not have identical internal clocks, hence they are not synchronized

   What does this mean for our applications? Well, put simply, when a client sends a request to
server, how long does it wait until it receives a reply? Is the server node just being slow? Is the
network congested and the packet has been dropped? If the client doesnt get a reply, what
should it do?
   Lets explore these scenarios in detail. The core problem is known as handling partial
failures, and the general situation is depicted in Figure 35.

   Figure 35 Handling Partial Failures
   When a client node sends a network request to a server node and expects a response, the
following outcomes may occur:

1.	The request succeeds and a rapid response is received. All is good.
2.	The destination IP address lookup may fail. In this case the client rapidly receives a
error message.
3.	The IP address is valid but the destination node or target server process has failed.
Again the sender will receive an error message.
4.	The request is received by the target server, which fails while processing the request and
no response is ever sent.
5.	The request is received by the target server, which is heavily loaded. It processes the
request but takes a long time (e.g 24 seconds!) to respond
6.	The request is received by the target server and a response is sent. However, the
response is not received by the client due to a network failure.

   Numbers (1) to (3) are easy for the client to handle, as a response is received rapidly. A result
from the server or an error message  either allows the client to proceed. Failures that can be
detected quickly are easy to deal with.
   Numbers (4) to (6) pose a problem for the client. It has no insight into the reason why a
response has not been received. From the clients perspective, these three outcomes look the
same. It cannot know, without waiting forever, whether the response will arrive eventually, or
never arrive. And waiting forever doesnt get much work done. More insidiously, the client
cannot know if the operation succeeded and server or network failure caused the result to never
arrive, or if the request is on its way - delayed simply due to congestion in the network/server.
These faults are collectively known as crash faults .
   The typical solution that clients adopt to handle crash faults is to resend the request after a
configured timeout period. This however is fraught with danger, as Figure 36 illustrates.  The
client sends a request to the server to deposit money in a bank account. When it receives no
response after a timeout period, it resends the request. What is the resulting balance? The server
may have applied the deposit, and it may not, depending on the partial failure scenario.

   Figure 36 Client retries a request after timeout
   The chance that the deposit may occur twice is a fine outcome for the customer. The bank
though is unlikely to be amused by this possibility. Hence we need a way to ensure in our server
operations that retried, duplicate requests from clients only result in the request being applied
once. This is necessary to maintain correct application semantics. This property is known as
idempotence. Idempotent operations can be applied multiple times without changing the result
beyond the initial application. This means that for the example in Figure 36, the client can retry
the request as many times as it likes, and the account will only be increased by $100.
   Requests that make no persistent state changes are naturally idempotent. This means all read
requests are inherently safe and no extra work is needed in the server. Updates are a different
matter. The system needs to devise a mechanism such that duplicate client requests can be
detected by the server, and they do not cause any state changes. In API terms, these endpoints
cause mutation of the server state and must be idempotent.
   The general approach to building idempotent operations is as follows:
*	Clients include a unique idempotence-key in all requests that mutate state. The key identifies
a single operation from the specific client or event source. It is usually a composite of a
user identifier, such as the session key, and a unique value such as a local timestamp,
UUID or a sequence number.
*	When the server receives a request, it checks to see if it has previously seen the
idempotence key value by reading from a database that is uniquely designed for
implementing idempotence. If the key is not in the database, this is a new request. The
server therefore performs the business logic to update the application state. It also stores
the idempotence key in a database to indicate that the operation has been successfully
applied.
*	If the idempotence key is in the database, this indicates that this request is a retry from
the client and hence should not be processed. In this case the server returns a valid
response for the operation so that (hopefully) the client wont retry again.

   The database used to store idempotence keys can be implemented in, for example:
*	a separate database table or collection in the transactional database used for the
application data
*	a dedicated database that provides very low latency lookups, such as a simple key-value
store

   Unlike application data, idempotence keys dont have to be retained forever. Once a client
receives an acknowledgement of a success for an individual operation, the idempotence key can
be discarded. The simplest way to achieve this is to automatically remove idempotence keys
from the store after a specific time period, such as 60 minutes or 24 hours, depending on
application needs and request volumes.
   In addition, an idempotent API implementation must ensure that the application state is
modified, and the idempotence key is stored. Both must occur for success. If the application
state is modified and, due to some failure, the idempotent key is not stored, then a retry will
cause the operation to be applied twice. If the idempotence key is stored but for some reason
the application state is not modified, then the operation has not been applied. If a retry arrives,
it will be filtered out as duplicate as the idempotence key already exists, and the update will be
lost.
   The implication here is that the updates to the application state and idempotence key store
must both occur, or neither must occur. If you know your databases, youll recognize this as a
requirement for transactional semantics. (Well discuss how distributed transaction are achieved
in Chapter XXX) This will ensure exactly-once semantics, which guarantees that all messages will
always be processed exactly once  what we need for idempotence.
   Exactly once does not mean that there are no message transmission failures, retries and no
application crashes. These are all inevitable. The important thing is that the retries eventually
succeed and the result is the always the same.
   Well return to the issue of communications delivery guarantees in later chapters. As Figure
37 illustrates, theres a spectrum of semantics, each with different guarantees and performance
characteristics. At most once delivery is fast and unreliable  this is what the UDP protocol
provides. At least once delivery is the guarantee provided by TCP/IP, meaning duplicates are
inevitable. Exactly-once delivery, as weve discussed here, requires guarding against duplicates and
hence trades off reliability against slower performance.
   As well see, some advanced communications mechanisms can provide our applications with
exactly once semantics. However, these dont operate at Internet scale because of the
performance implications. That is why, as our applications are built on the at least once
semantics of TCP/IP, we must implement exactly once semantics in our APIs that cause state
mutation.

   Figure 37 Communications Delivery Guarantees
Consensus in Distributed Systems
   Crash faults have another implication for the way we build distributed systems. This is best
illustrated by the Two Generals Problem , which is illustrated in Figure 38.
   Imagine a city under siege by two armies. The armies lie on opposite sides of the city, and
the terrain surrounding the city is difficult to travel through and visible to snipers in the city. In
order to overwhelm the city, its crucial that both armies attack at the same time. This will
stretch the citys defenses and make victory more likely for the attackers. If only one army
attacks, then they will likely be repelled. How can the two generals reach agreement on the exact
time to attack, such that both generals know for certain that agreement has been reached? They
both need certainty that the other army will attack at the agreed time.
   To coordinate an attack, the first general sends a messenger to the other, with instructions to
attack at a specific time. As the messenger may be captured or killed by snipers, the sending
general cannot be certain the message has arrived unless they get an acknowledgement
messenger from the second general. Of course, the acknowledgement messenger may be
captured or killed, so even if the original messenger does get through, the first general may
never know. And even if the acknowledgement message arrives, how does the second general
know this, unless they get an acknowledgement from the first general?
   Hopefully the problem is apparent. With messengers being randomly captured or
extinguished, there is no guarantee the two generals will ever reach consensus on the attack
time. In fact, it can be proven that it is not possible to guarantee agreement will be reached. There
are solutions that increase the likelihood of reaching consensus. For example, Game of Thrones
style, each general may send 100 different messengers every time, thus increasing the probability
that at least one will make the perilous journey to the other friendly army and successfully
deliver the message.

   Figure 38 The Two Generals Problem
   The Two Generals problem is analogous to two nodes in a distributed system wishing to
reach agreement on some state, such as the value of a data item that can be updated at either.
Partial failures are analogous to losing messages and acknowledgements. Messages may be lost
or delayed for an indeterminate period of time.
   In fact it can be demonstrated that consensus on an asynchronous network in the presence
of crash faults, where messages can be delayed but not lost, is impossible to achieve within
bounded time. This is known as the FLP Impossibility Theorem .
   Luckily, this is only theoretical limitation, demonstrating its not possible to guarantee
consensus will be reached with unbounded message delays on an asynchronous network. In
reality, distributed systems reach consensus all the time. This is possible because while our
networks are asynchronous, we can establish sensible practical bounds on message delays and
retry after a timeout period. FLP is therefore a worst-case scenario, and as well discuss
algorithms for establishing consensus when we discuss distributed databases.
   Finally, we should note the issue of Byzantine failures. Imagine extending the Two Generals
problem to N Generals, who need to agree on a time to attack. However, in this scenario,
traitorous messengers may change the value of the time of the attack, or a traitorous general
may sense false information to other generals. This class of malicious failures are known as
Byzantine faults and are particularly sinister in distributed systems. Luckily, the systems we
discuss in this book typically live behind well-protected, secure enterprise networks and
administrative environments. This means we can practically exclude handling Byzantine faults.
Algorithms that do address malicious behavior exist, and if you are interested in a practical
example, take a look at Blockchain technologies  and BitCoin .
Time in Distributed Systems
   Every node in a distributed system has its own internal clock. If all the clocks on every
machine were perfectly synchronized, we could always simply compare the timestamps on
events across nodes to determine the precise order they occurred in. If this were reality, many of
the problems well discuss with distributed systems would pretty much go away.
   Unfortunately, this is not the case. Clocks on individual nodes drift due to environmental
conditions like changes in temperature or voltage. The amount of drift varies on every machine,
but values like 10-20 seconds a day are not uncommon. Or with my current coffee machine,
about 15 minutes a day!
   If left unchecked, clock drift would render the time on a node meaningless  like my coffee
machine if I dont correct it every few days. To address this problem, a number of time services
exist. A time service represents an accurate time source, such as a GPS or atomic clock, which
can be used to reset the clock on a node to correct for drift on packet-switched, variable-latency
data networks.
   The most widely used time service is NTP , which provides a hierarchically organized
collection of time servers spanning the globe. The root servers, of which there are around 300
worldwide, are the most accurate. Time servers in the next level of the hierarchy (approximately
20,000) synchronize to within a few milliseconds of the root server periodically, and so on
throughout the hierarchy, a with a maximum of 15 levels. Globally there are more than 175,000
NTP servers.
   Using the NTP protocol, a node in an application running the NTP client can synchronize
to a NTP server. The time on a node is set by a UDP message exchange with one or more NTP
servers. Messages are timestamped and through the message exchange the time taken for transit
is estimated. This becomes a factor in the algorithm used by NTP to establish what the time on
the client should be reset to.  A simple NTP configuration is shown in Figure 39. On a LAN,
machines can synchronize to an NTP server within a small number of milliseconds accuracy.

   Figure 39 Illustrating using the NTP Service
   One interesting effect of NTP synchronization for our applications is that the resetting of
the clock can move the local node time forwards or backwards. This means that if our
application is measuring the time taken for events to occur (e.g. to calculate event latencies), it is
possible that the end time of the event may be earlier than the start time if the ntp protocol has
set the local time backwards.
   In fact, a node has two clocks. These are:
	Time of Day Clock: This represents the number of milliseconds since midnight
January 1970. In Java, you can get the current time using
System.currentTimeMillis(). This is the clock that can be reset by NTP, and
hence may jump forwards or backwards if it is a long way ahead of NTP time.
	Monotonic Clock: This represents the amount of time (in seconds and nanoseconds)
since an unspecified point in the past, such as the last time the system was restarted. It
will only ever move forward, however it again may not be a totally accurate measure of
elapsed time because it stalls during a virtual machine suspension. In Java, you can get
the current monotonic clock time using System.nanoTime().
.
   Applications can use an NTP service to ensure the clocks on every node in the system are
closely synchronized. Its typical for an application to resynchronize clocks on anything from a
one hour to one day time interval. This ensures the clocks remain close in value. Still, if an
application really needs to precisely know the order of events that occur on different nodes,
clock drift is going to make this fraught with danger.
   There are other time services that provide higher accuracy than NTP. Chrony  supports the
NTP protocol but provides much higher accuracy and greater scalability than NTP  the reason
it has recently been adopted by Facebook . Amazon has built the Amazon Time Sync Service
by installing GPS and atomic clocks in its data centers. This service is available for free to all
Amazon cloud customers.
   The take away from this discussion is that our applications cannot rely on timestamps of
events on different node s to represent the actual order of these events. Clock drift even by a
second or two makes cross-node timestamps meaningless to compare. The implications of this
will become clear when we start to discuss distributed databases in detail.

Summary and Further Reading
   This chapter has covered a lot of ground to explain some of the fundamental issues
communications and time in distributed systems. The key issues that should resonate with the
you, dear reader, are as follows:
1.	Communications in distributed systems can transparently traverse many different types
of underlying physical networks  e.g. Wi-Fi, wireless, WANs and LANs.
2.	Communication latencies are highly variable, and influenced by the physical distance
between nodes, and transient network congestion.
3.	Communications can fail due to network communications fabric and router failures
that make nodes unavailable, and individual node failure.
4.	The sockets library has two variants, one that supports TCP/IP for reliable connection-
based communications (SOCK_STREAM), and one for unreliable UDP datagram-based
communications (SOCK_DGRAM).
5.	RMI/RPC technologies build on TCP/IP sockets to provide abstractions for client-
server communications that mirror making local method/procedure calls.
6.	Achieving agreement, or consensus on state across multiple nodes in the presence of
crash faults is not possible in bounded time on asynchronous networks. Luckily, real
networks, especially LANs, are fast and mostly reliable, meaning we can devise
algorithms that achieve consensus in practice.
7.	There is no reliable global time source that nodes in an application can rely upon to
synchronize their behavior. Clocks on individual nodes vary and cannot be used for
meaningful comparisons.

   These issues will pervade the discussions in the rest of this book. Many of the unique
problems and solutions that are adopted in distributed systems stem from these fundamentals.
Theres no escaping them!
   An excellent source for more detailed, more theoretical coverage of all aspects of distributed
systems is George Colouris et at, Distributed Systems: Concepts and Design, 5th Edition, Pearson, 2011.
   Likewise for computer networking, youll find out all you wanted to know and no doubt
more in James Kurose, Keith Ross, Computer Networking: A Top-Down Approach, 7th Edition, Pearson
2017.

CHAPTER 5
__________________________

Application Services
   In this chapter, were going to focus on the pertinent issues in achieving scalability for the
services tier in an application. Well explore API and service design and describe the salient
features of application servers that provide the execution environment for services. Well also
elaborate on topics such as horizontal scaling, load balancing and state management that we
introduced in Chapter 2.
Service Design
   In the simplest case, an application comprises one Internet facing service that persists data to
a local data store, as shown in Figure 40. Clients interact with the service through its published
API, which is accessible across the Internet.
   Lets look at the API and service implementation in more detail.

   Figure 40 A Simple Service
  Application Programming Interface (API)

   An API defines a contract between the client and server. The API specifies the types of
requests that are possible, the data that is needed to accompany the requests, and the results that
will be obtained. APIs have many different variations, as we explored in RPC/RMI discussions
in Chapter 4. While there remains some API diversity in modern applications, the predominant
style relies on HTTP APIs. These are typically, although not particularly accurately, classified as
RESTful.
   REST is actually an architectural style that was defined by Roy Fielding in his PhD thesis .
A great source of knowledge on RESTful APIs and the various degrees to which Web
technologies can be exploited is REST in Practice by Webber, Parastatidis, and Robinson. Here well
just briefly touch on the HTTP CRUD API pattern. This pattern does not fully implement the
principles of REST, but it is widely adopted in Internet systems today.
   CRUD stands for Create, Read, Update, Delete. A CRUD API specifies how clients perform
these operations in a specific business context. For example a user might create a profile, read
catalog items, update their shopping cart and delete items from their order. A HTTP CRUD API
makes these operations possible using four core HTTP verbs, as shown in Table 3.

Verb
Uniform Resource Identifier
Example
Purpose
POST
/skico.com/skiers/{skierID}/{date}
Create a new ski day record for a skier
GET
/skico.com/skiers/(skierID)
Get the profile information for a skier,
returned in a JSON response payload
PUT
/skico.com/skiers/{skierID}
Update skier profile
DELETE
/skico.com/skiers/{skierID}
Delete a skiers profile as they didnt
renew their pass!

   Table 3 HTTP CRUD Verbs
   A HTTP CRUD API applies HTTP verbs on resources identified by Uniform Resource
Identifiers (URIs). In Table 3 for example, a URI that identifies skier 768934 would be:

   /skico.com/skiers/768934

   A HTTP GET request to this resource would return the complete profile information for a
skier in the response payload, such as name, address, number of days visited, and so on. If a
client subsequently sends a HTTP PUT request to this URI, we are expressing the intent to
update the resource for skier 768934  in this example it would be the skiers profile. The PUT
request would provide the complete representation for the skiers profile as returned by the
GET request. Again, this would be as a payload with the request. Payloads are typically
formatted as JSON, although XML and other formats are also possible. If a client sends a
DELETE request to the same URI, then the skiers profile will be deleted.
   Hence the combination of the HTTP verb and URI define the semantics of the API
operation. Resources, represented by URIs, are conceptually like objects in Object Oriented
Design (OOD) or entities in Entity-Relationship (ER) model. Resource identification and
modeling hence follows similar methods to OOD and ER modeling. The focus however is on
resources that need to be exposed to clients in the API. The Further Reading section at the end
of this chapter points to useful sources of information for resource design.
   HTTP APIs can be specified using a notation called OpenAPI . At the time of writing the
latest version is 3.0. A tool called SwaggerHub  is the de facto standard to specify APIs in
OpenAPI. The specification is defined in YAML, and an example is show in Figure 41. It
defines the GET operation on the URI /resorts. If the operation is successful, a 200 response
code is returned along with a list of resorts in a format defined by a JSON schema that appears
later in the specification. If for some weird reason, the query to get a list of resorts operated by
skico.com returns no entries, a 404 response code is returned along with an error message that
is also defined by a JSON schema.
1.	paths:
2.	  /resorts:
3.	    get:
4.	      tags:
5.	        - resorts
6.	      summary: get a list of ski resorts in the database
7.	      operationId: getResorts
8.	      responses:
9.	        '200':
10.	          description: successful operation
11.	          content:
12.	            application/json:
13.	              schema:
14.	                $ref: '#/components/schemas/ResortsList'
15.	        '404':
16.	          description: Resorts not found. Unlikely unless we go broke
17.	          content:
18.	            application/json:
19.	              schema:
20.	                $ref: '#/components/schemas/responseMsg'
21.	         500:
22.	            $ref: "#/responses/Standard500ErrorResponse"
    Figure 41 OpenAPI Example
   It is also possible for the client to see a 500 Internal Error response code. This indicates
that the server encountered an unexpected condition that prevented it from fulfilling the
request. This error code is a generic "catch-all" response. It may be thrown if the server has
crashed, or if the server is temporarily unavailable due to a transient network error.

  Designing Services
   An application server container receives requests and routes them to the appropriate handler
function to process the request. The handler is defined by the application service and
implements the business logic required to generate results from the request. As multiple
simultaneous requests arrive at a service instance, each is typically  allocated a thread context to
execute the request.
   The sophistication of the routing functionality varies widely by technology platform and
language. For example, in Express.js, the container calls a specified function for requests that
match an API signature  known as a route path - and HTTP method. Figure 42 illustrates this
with a method that will be called when the client sends a GET request for a specific skiers
profile, as identified by the value of :skierID.
1.	app.get('/skiers/:skierID', function (req, res) {
2.	  // process the GET request
3.	  ProcessRequest(req.params)
4.	}
    Figure 42 Express.js request routing example
In Java, the Spring framework provides an equally sophisticated method routing technique. It
leverages a set of annotations that define dependencies and implement dependency injection to
simplify the service code. Figure 43 shows an example of annotations usage, namely:

	@RestController  identifies the class as a controller that implements an API and
automatically serializes the return object into the HttpResponse returned from the API.
	@GetMapping  maps the API signature to the specific method, and defines the format of
the response body
	@PathVariable  identifies the parameter as value that originates in the path for URI that
maps to this method
1.	@RestController
2.	public class SkierController {
3.
4.	    @GetMapping("/skiers/{skierID}",
5.	                produces = application/json)
6.	    public Profile GetSkierProfile(
7.	                        @PathVariable String skierID,
8.	                        ) {
9.	          // DB query method omitted for brevity
10.	        return GetProfileFromDB(skierID);
11.	    }
12.	}
    Figure 43 Spring method routing example
   Another Java technology, JEE servlets, also provide annotations, as shown in Figure 44, but
these are simplistic compared to Spring and other higher-level frameworks. The @WebServlet
annotation identifies the base pattern for the URI which should cause a particular servlet to be
invoked. This is /skiers in our example. The class that implements the API method must
extend the HttpServlet abstract class from the javax.servlet.http package and override
at least one method that implements a HTTP request.  The HTTP verbs map to methods as
follows:

	doGet: HTTP GET requests
	doPost: HTTP POST requests
	doPut: HTTP PUT requests
	doDelete: for HTTP DELETE requests

   Each method is passed as parameters a HttpServletRequest and HttpServletResponse
object. The servlet container creates the HttpServletRequest object, which contains members
that represent the components of the incoming HTTP request. This object contains the
complete URI path for the call, and it is the servlets responsibility to explicitly parse and
validate this, and extract path and query parameters if valid. Likewise, the servlet must explicitly
set the properties of the response using the HttpServletResponse object.
   Servlets therefore require more code from the application service programmer to implement.
However, they are likely to provide a more efficient implementation as there is less plumbing
involved compared to the more powerful annotation approaches of Spring et al. A good rule of
thumb is that if a framework is easier to use, it is likely to be less efficient. This is a classic
performance versus ease-of-use trade-off. Well see lots of these in this book
1.	import javax.servlet.http.*;
2.	@WebServlet(
3.	    name = SkiersServlet,
4.	    urlPatterns = /skiers
5.	)
6.	public class SkierServlet extends HttpServlet (
7.
8.	protected void doGet(HttpServletRequest request,
9.	                     HttpServletResponse response) {
10.	  // handles requests to /skiers/{skierID}
11.	  try {
12.	     // extract skierID from the request URI (not shown for brevity)
13.	     String skierID  = getSkierIDFromRequest(request);
14.	     if(skierID == null) {
15.	        // request was poorly formatted, return error code
16.	        response.setStatus(HttpServletResponse.SC_BAD_REQUEST);    }
17.	     else {
18.	        // read the skier profile from the database
19.	        Profile profile = GetSkierProfile (skierID);
20.	        // add skier profile as JSON to HTTP response and return 200
21.	        response.setContentType("application/json");
22.	        response.getWriter().write(gson.toJson(Profile);
23.	        response.setStatus(HttpServletResponse.SC_OK);
24.	     } catch(Exception ex) {
25.	         response.setStatus
26.	           (HttpServletResponse.SC_INTERNAL_SERVER_ERROR);    }
27.
28.	       }
29.	} }
    Figure 44 JEE Servlet Example
  State Management

   State management is a tricky, nuanced topic. The bottom line is that service implementations
that need to scale should avoid storing conversational state. What on earth does that mean?
Lets start by examining the topic of state management with HTTP.
   HTTP is known as stateless protocol. This means each request is executed independently,
without any knowledge of the requests that were executed before it from the same client.
Statelessness implies that every request needs to be self-contained, with sufficient information
provided by the client for the Web server to satisfy the request regardless of previous activity
from that client.
   The picture is a little more complicated that this simple description portrays, however. For
example:
	The underlying socket connection between a client and server is kept open so that the
overheads of connection creation are amortized across multiple requests from a client.
This is the default behavior for versions HTTP/1 and above.
	HTTP supports cookies, which are known as the HTTP State Management
Mechanism . Gives it away really!
	HTTP/2 supports streams, compression, and encryption, all of which require state
management

   So, originally HTTP was stateless, but perhaps not anymore? Armed with this confusion (!),
lets look at application services APIs built on top of HTTP.
   When a user or application connects to a service, it will typically send a series of requests to
retrieve and update information. Conversational state represents any information that is retained
between requests such that the subsequent request can assume the service has retained
knowledge about the previous interactions. Lets explore a simple example.
   In our skier service API, a user may request their profile by submitting a GET request to the
following URI:

   /skico.com/skiers/768934

   They may then use their app to modify their phone number and send a PUT request to a
URI designed for updating this field:

   /skico.com/skiers/phoneno/4123131169

   As this URI does not identify the skier, the service must know the unique identifier of the
skier, namely 768934. Hence for this PUT operation to succeed, the service must have retained
conversational state from the previous GET request.
   Implementing this approach is relatively straightforward. When the service receives the initial
GET request, it creates a session state object that uniquely identifies the client connection. In
reality, this is often performed when a user first connects to or logs in to a service. The service
can then read the skier profile from the database and utilize the session state object to store
conversational state  in our example this would be skierID. When the subsequent PUT
request arrives from the client it uses the session state object to look up the skierID associated
with this session and uses that to update the skiers phone number.
   Services that maintain conversational state are known as stateful services. Stateful services
are attractive from a design perspective as they can minimize the number of times a service
retrieves data (state) from the database and reduce the amount of data that is passed between
clients and the services. For services with light request loads they can make eminent sense and
are promoted by many frameworks to make services easy to build and deploy. For example, JEE
servlets support session management using the HttpSession object, and similar capabilities are
offered by the Session object in ASP.NET.
   As we scale our service implementations however, the stateful approach becomes
problematic. For a single service instance, we have two problems to consider:

1.	If we have multiple client sessions all maintaining session state, this will utilize available
service memory. The amount of memory utilized will be proportional the number of
clients we are maintaining state for. If a sudden spike of requests arrive, how can we be
certain we will not exhaust available memory and cause the service to fail?
2.	We also must be mindful about how long to keep session state available. A client may
stop sending requests but not cleanly close their connection to allow the state to be
reclaimed. All session management approaches support a default session time out. If we
set this to a short time interval, clients may see their state disappear unexpectedly. If we
set the session time out period to be too long, we may degrade service perform as it
runs low on resources.

   In contrast, stateless services do not assume that any conversational state from previous calls
has been preserved. The service should not maintain any knowledge from earlier requests, so
that each request can be processed individually. This requires the client to provide all the
necessary information for the service to process the request and provide a response.
   If we consider our example above, we could transform the PUT request to be stateless by
incorporating the skierID in the URI:

   /skico.com/skiers/768934/phoneno/4123131169

   In reality, this is a pretty dumb API design. As we discussed in the service API section, the
API should in fact transfer the complete resource  the skier profile  with the GET response
and PUT request. This means the GET request for the skier should retrieve and return the
complete skier profile so that the client app can display and modify any data items in the profile
as the user wishes. The complete updated profile is then sent as the payload in the subsequent
PUT operation, and used by the service to update the resource, that is persisted in a database.
This is shown in Figure 45.

   Figure 45 Stateless API Example
   Any scalable service will need stateless APIs. If a service needs to retain state pertaining to
client sessions  the classic shopping cart example - it must be stored externally to the service.
This invariably means an external data store.
   Well revisit this topic later in this chapter during the discussion of horizontal scaling. Thats
when stateless services really come to the fore.
Applications Servers
   Application servers are the heart of a scalable application, hosting the business services that
comprise an application. Their basic role is to accept requests from clients, apply application
logic to the requests, and reply to the client with the request results. Clients may be external or
internal, as in other services in the application that require to use the functionality of a specific
service.
   The technological landscape of application servers is broad and complex, depending on the
language you want to use and the specific capabilities that each offer. In Java, the Java
Enterprise Edition (JEE)  defines a comprehensive, feature rich standards-based platform for
application servers, with multiple different vendor and open source implementations.
   In other languages, the Express.js  server supports Node, Flask supports Python , and in
GoLang a service can be created by incorporating the net/http package. These
implementations are much more minimal and lightweight than JEE and are typically classified
are Web application frameworks. In Java, the Apache Tomcat server  is a somewhat equivalent
technology. Tomcat is open source implementation of a subset of the JEE platform, namely the
Java Servlet, JavaServer Pages, Java Expression Language and Java WebSocket technologies.
   Figure 46 depicts a simplified view of the anatomy of Tomcat. Tomcat implements a servlet
container, which is an execution environment for application-defined servlets. Application
defined Servlets are loaded into this container, which provides lifecycle management and a
multithreaded runtime environment.
   Request arrive at the IP address of the server, which is listening for traffic on specific ports.
For example, by default Tomcat listens on port 8080 for HTTP requests and 8443 for HTTPS
requests. Incoming requests are processed by one or more listener threads. These create a
TCP/IP socket connection between the client and server. If network requests arrive at a
frequency that cannot be processed by the TCP listener, pending requests are queued up in the
Sockets Backlog. The size of the backlog is operating system dependent. In most Linux versions
the default is 100.
   Once a connection is established, the TCP requests are marshalled by, in this example, a
HTTP Connector which generates the HTTP request that the application can process. The HTTP
request is then dispatched to an application service thread to process. Application container
threads are managed in a thread pool, essentially a Java Executor, which by default in Tomcat
is a minimum size of 25 threads and a maximum of 200. If there are no available threads to
handle a request, the container maintains them in a queue of runnable tasks and dispatches these
as soon as a thread becomes available. This queue by default is size Integer.MAX_VALUE  that
is, essentially unbounded . If a thread remains idle for by default, 60 seconds, it is killed to free
up resources in the JVM.

   Figure 46 Anatomy of a Web application server
   For each request, the method that corresponds with the HTTP request is invoked in a
thread. The servlet method processes the HTTP request headers, executes the business logic,
and constructs a response that is marshalled by the container back to a TCP/IP packet and sent
over the network to the client.
   In processing the business logic, servlets often need to query an external database. This
requires each thread executing the servlet methods to obtain a database connection and execute
database queries. As database connections are limited resources and consume resources in both
the client and database server, a fixed size connection pool is typically utilized. The pool hands
out open connection to requesting threads on demand.
   When a servlet wishes to submit a query to the database, it therefore requests an open
connection from the pool. If one is available, access to the connection is granted to the servlet
until it indicates it has completed its work. At that stage the connection is returned to the pool
and made available for another servlet to utilize. As the thread pool is typically larger than the
connection pool, a servlet may request a connection when none are available. The connection
pool maintains a request queue and hands out open connections on a FIFO basis, and threads
in the queue are blocked until there is availability.
   An application server framework such as Tomcat is hence highly configurable to different
handle different workloads. For example, the size of the thread and database connection pools
can be specified in configuration files that are read at startup.
   The complete Tomcat container environment runs within a single JVM, and hence
processing capacity is limited by the number of vCPUs available and the amount of memory
allocated as heap size. Each allocated thread consumes memory, and the various queues in the
request processing pipeline consume resources while requests are waiting. This means that
request latency will be governed by both the request processing time in the servlet and the time
spent waiting in queues for threads and connections to become available.
   In a heavily loaded server with many threads, context switching may start to degrade
perform, and available memory may be become limited. If perform degrades, queues grow as
requests wait for resources. This consumes more memory. If more requests are received than
can be queued up and processed by the server, then new TCP/IP connections will be refused,
and clients will see errors. Eventually, an overloaded server will run out of resources and start
throwing exceptions and crash.
   Time spent tuning configuration parameters to efficiently handle anticipated loads is rarely
wasted. A rule-of-thumb is that CPU utilization that consistently exceeds the 70-80% range is a
signal of overload. Similar insights exist for memory usage. Once any resource gets close to full
utilization, systems tend to exhibit less predictable performance, as more time is spent, for
example thread context switching and garbage collecting. This inevitably effects latencies and
throughput.
   Monitoring tools available with Web application frameworks enable engineers to gather a
range of important metrics, including latencies, active requests, queue sizes and so on. These are
invaluable for carrying out data-driven experiments that lead to performance optimization.
   Java-based application frameworks such as Tomcat will invariably support the JMX  (Java
Management Extensions) framework, which is a standard part of the Java Standard Edition
platform. JMX enables frameworks to expose monitoring information based on the capabilities
of MBeans (Managed Beans), which represent a resource of interest (e.g. thread, database
connections usage). This enables an eco-system of tools to offer capabilities for monitoring
JMX-supported. These range from JConsole  which is available in the JDK by default, to
powerful open source technologies such as JavaMelody  and many expensive commercial
offerings.
Horizontal Scaling
   A core principle of scaling a system is being able to easily add new processing capacity to
handle increased load. For most systems, a simple and effective approach is deploying multiple
instances of stateless server resources and using a load balancer to distribute the requests across
these instances. This is known as horizontal scaling and illustrated in Figure 47.

   Figure 47 Simple Load Balancing Example
   These two ingredients, namely stateless service replicas and a load balancer, are both
necessary. Lets explain why.
   Service replicas are deployed on their own (virtual) hardware. Hence if we have two replicas,
we double our processing capacity. If we have ten replicas, we have 10x capacity. This enables
our system to handle increased loads. The aim of horizontal scaling is to create a system
processing capacity that is the sum of the total resources available
   The servers need to be stateless, so that any request can be sent to any service replica to
handle. This decision is made by the load balancer, which can use various policies to distribute
requests. If the load balancer can keep each service replica equally busy, then we are effectively
using the processing capacity provided by the service replicas.
   Horizontal scaling also increases availability. With one service instance, if it fails, the service
is unavailable. This is known as a single point of failure (SPoF)  a bad thing, and one to avoid
in any scalable distributed system. Multiple replicas increase availability. If one replica fails,
requests can be directed to any  they are stateless, remember  replica. The system will have
reduced capacity until the failed server is replaced, but it will still be available. Which is
important. The ability to scale is crucial, but if a system is unavailable, then the most scalable
system ever built is still somewhat ineffective!
Load Balancing
   Load balancing aims to effectively utilize the capacity of a collection of services to optimize
the response time for each request. This is achieved by distributing requests across the available
services as evenly as possible and avoiding overloading some services while underutilizing
others. Clients send requests to the IP address of the load balancer, which redirects requests to
target services, and relays the results back to the client. This means clients never contact the
services directly, which is also beneficial for security as the services can live behind a security
perimeter and not be exposed to the Internet.
   Load balancers may act at the network level or the application level. These are often called Layer 4
and Layer 7 load balancers respectively. These names refer to network transport layer at Layer 4
in the Open Systems Interconnection (OSI) Reference Model , and the application layer at
Layer 7. The OSI model defines network functions in seven abstract layers. Each layer defines
standards for how data is packaged and transported. Lets explore the differences between the
two techniques.
   Network level load balancers distribute requests at the network connection level, operating
on individual TCP or UDP packets.  Routing decisions are made on the basis of client IP
addresses. Once a target service is chosen, the load balancer uses a technique called Network
Address Translation (NAT). This changes the destination IP address in the client request packet
from that of the load balancer to that of the chosen target. When a response is received from
the target, the load balancer changes the source address recorded in the packet header from the
targets IP address to its own. Network load balancers are relatively simple as they operate on
the individual packet level. This means they are extremely fast, as they provide few features
beyond choosing a target service and performing NAT functionality.
   In contrast, application level load balancers reassemble the complete HTTP request and base
their routing decisions on the values of the HTTP headers and on the actual contents of the
message. For example, a load balancer can be configured to send all POST requests to a subset
of available services, or distribute requests based on a query string in the URI. Application load
balancers are sophisticated reverse proxies. The richer capabilities they offer means they are
slightly slower than network load balancers, but the powerful features they offer can be utilized
to more than make up for the overheads incurred.
   In general, a load balancer has the following features which are explained in the following
sections:

1.	Load distribution policies
2.	Health monitoring
3.	Elasticity
4.	Session affinity

  Load Distribution Policies
   Load distribution policies dictate how the load balancer chooses a target service to process a
request. Any load balancer worth its salt will offer several load distribution policies  HAProxy
offers 10 in fact . The following are probably the most commonly supported across all load
balancers:
	round-robin: the load balancer distributes requests to available servers in a round-robin
fashion
	least connections:  the load balancer distributes new requests to the server with the least
open connections
	HTTP header field: the load balancer directs requests based on the contents of a specific
HTTP header field. For example all requests with the header field X-Client-
Location:US,Seattle could be routed to a specific set of servers.
	HTTP operation: the load balancer directs requests based on the HTTP verb in the
request

   Load balancers will also allow services to be allocated weights. For example, standard service
instances in the load balancing pool may have 4 vCPUs and each is allocated a weight of 1. If a
service with 8 vCPUs is added, it can be assigned a weight of 2 so the load balancer will send
twice as many requests its way.

  Health Monitoring
   A load balancer will periodically sends pings and attempts connections to test the health of
each service in the load balancing pool. These tests are called health checks. If a service
becomes unresponsive or fails connection attempts, it will be removed from the load balancing
pool and  no requests will be sent to that host. If the connection to the service has experienced
a transient failure, the load balancer will reincorporate the service once it becomes available and
healthy. If, however it has failed, the service will be removed from the load balancer target pool.
  Elasticity
   Spikes in request loads can cause the service capacity available to a load balancer to become
saturated, leading to longer response times and eventually request and connection failures.
Elasticity is the capability of a load balancer to dynamically provision new service capacity to
handle an increase in requests. As load increases, the load balancer starts up new resources and
directs requests to these, and as load decreases the load balancer stops services that are no
longer needed.
   An example of elastic load balancing is the Amazon Web Services (AWS) Auto-Scaling
groups. An Auto Scaling group is a collection of services available to a load balancer that is
defined with a minimum and maximum size. The load balancer will ensure the group always has
the minimum numbers of services available, and the group will never exceed the maximum
number. The actual number available at any time will depends on the client request load the
load balancer is handling for the services in the group. This scheme is illustrated in Figure 48.

   Figure 48 Elastic Load Balancing
   Typically, there are two ways to control the number of services in a group. The first is based
on a schedule, when the request load increases and decreases are predictable. For example, you
may have an online entertainment guide and publish the weekend events for a set of major cities
at 6pm on Thursday. This generates a higher load until Sunday at noon. An Auto Scaling group
could easily be configured to provision new services at 6pm Thursday and reduce the group size
to the minimum at noon Sunday.
   If increased load spikes are not predictable, elasticity can be controlled dynamically with a set
of configuration parameters. The most commonly used parameter is average CPU utilization
across the active services. For example, an Auto Scaling group can be defined to maintain
average CPU utilization at 70%. If this value is exceeded, the load balancer will start one or
more new services instances until the 70% threshold is reached. Instances need time to start 
often a minute or more - and hence a warmup period can be defined until the new instance is
considered to be contributing to the groups capacity. When the group average CPU utilization
drops below 70%, scale in or scale down will start and instances will be automatically stopped and
removed from the pool.
   Elasticity is a key feature that allows services to scale dynamically as demand grows. For
highly scalable systems, it is a mandatory capability. Like all advanced capabilities however, it
brings new issues for us to consider in terms of downstream capacity and costs. Well discuss
these in Chapter XXXX.

  Session Affinity
   Session affinity, or sticky sessions, are a load balancer feature for stateful services. With
sticky sessions, the load balancer sends all requests from the same client to the same service
instance. This enables the service to maintain in-memory state about each specific client session.
   There are various ways to implement sticky sessions. For example, HAProxy provides a
comprehensive set of capabilities to maintain client requests on the same service in the face of
service additions, removals and failures . AWS Elastic Load Balancing generates an HTTP
cookie that identifies the service a clients session is associated with. This cookie is returned to
the client, which must send it in subsequent request to ensure session affinity is maintained.
   Sticky sessions can be problematic for highly scalable systems. They lead to a load imbalance
problem, in which, over time, clients are not evenly distributed across services. This is illustrated
in Figure 49, where two clients are connected to one service while another service remains idle.

   Figure 49 Load Imbalance with Sticky Sessions
   Load imbalance occurs because client sessions last for varying amounts of time. Even if
sessions are evenly distributed initially, some will terminate quickly while others will persist. In a
lightly loaded system, this tends to not be an issue. However, in a system with millions of
sessions being created and destroyed constantly, load imbalance is inevitable. This will lead to
some services being underutilized, while others are overwhelmed and may potentially fail due to
resource exhaustion.
   Stateful services have other downsides. When a service inevitably fails, how do the clients
connected to that server recover the state that was being managed? If a service instance
becomes slow due to high load, how do clients respond? In general stateful servers create
problems that in large scale systems are difficult to design around and manage.
   Stateless services have none of these downsides. If one fails, clients get an exception and
retry, with their request routed to another live service. If a service is slow due to a transient
network outage, the load balancer takes it out of the service group until it passes health checks
of fails. All state is either externalized or provided by the client in each request, so service
failures can be handled easily by the load balancer.
   Stateless services enhance scalability, simplify failure scenarios and ease the burden of service
management. For scalable applications, these advantages far outweigh the disadvantages, and
hence their adoption in most major Internet sites such as Netflix.
Summary and Further Reading
   Services are the heart of a scalable software system. They define the contract defined as an
API that specifies their capabilities to clients. Services are defined and execute in an application
server container environment that hosts the service code and routes incoming API requests to
the appropriate processing logic. Application servers are highly programming language
dependent, but in general provide a multithreaded programming model that allows services to
process many requests simultaneously. If the threads in the container thread pool are all utilized,
the application server queues up requests until a thread becomes available.
   As request loads grow on a service, we can scale it out horizontally using a load balancer to
distribute requests across multiple instances. This architecture also provides high availability as
the multiple service configuration means the application can tolerate failures of individual
instances. The service instances are managed as pool by the load balancer, which utilizes a load
distribution policy to choose a target service for each request. Stateless services scale easily and
simplify failure scenarios by allowing the load balancer to simply resend requests to responsive
targets. Although most load balancers will support stateful services using a feature called sticky
sessions, stateful services make load balancing and handling failures more complex. Hence, they
are not recommended for highly scalable services.
   API design is a topic of great complexity and debate. An excellent overview of basic API
design and resource modeling is https://www.thoughtworks.com/insights/blog/rest-api-
design-resource-modeling.
   The Java Enterprise Edition (JEE) is an established and widely deployed server-side
technology. It has a wide range of abstractions for building rich and powerful services. The
Oracle tutorial is an excellent starting place for appreciating this platform -
https://docs.oracle.com/javaee/7/tutorial/.
   Much of the knowledge and information about load balancers is buried in the
documentation provided by the technology suppliers. You choose your load balancer and then
dive into the manuals. For an excellent, broad perspective on the complete field of load
balancing, this is a good resource.
   Tony Bourke, Server Load Balancing, OReilly Publishing?
CHAPTER 6
__________________________

Caching
   Caching is an essential ingredient of a scalable system. Caching makes the results of
expensive queries and computations available for reuse by subsequent requests at low cost. By
not having to reconstruct the cached results for every single request, the capacity of the system
is increased, and it is hence able to scale to handle greater workloads.
   Caches exist in many places in an application. The CPUs that run applications have multi-
level, fast hardware caches to reduce relatively slow main memory accesses. Database engines
can make use of main memory to cache the contents of the data store in memory so that in
many cases queries do not have to touch relatively slow disks.
   This chapter covers:
	application based caching, in which service business logic incorporates the caching and
access of precomputed results
	Web based caching, which exploits mechanisms built into the HTTP protocol to enable
caching of results within the infrastructure provided by the Internet.
Application Caching
   Application caching is designed to improve request responsiveness by storing the results of
queries and computations in memory so they can be subsequently served by later requests. For
example, think of an online newspaper site. Once posted, articles change infrequently and hence
can be cached and on first access and reused on all subsequent requests until the article is
updated.
   In general, caching relieves databases of heavy read traffic, as many queries can be served
directly from the cache. It also reduces computation costs for objects that are expensive to
construct, for example those needing queries that span several different databases. The net
effect is to reduce the computational load on our services and databases and create head room
for more requests.
   Caching requires additional resources, and hence cost, to store cached results. However, well
designed caching schemes are low cost compared to upgrading database and service nodes to
cope with higher request loads. As an indication of the value of caches, approximately 3% of
infrastructure at Twitter is dedicated to application level caches.  At Twitter scale, that is a lot
of infrastructure!
   Application level caching exploits dedicated distributed cache engines. The two predominant
technologies in this area are memcached  and Redis . Both are essentially distributed in-
memory key-value stores designed for arbitrary data (strings, objects) from the results of
database queries or downstream service API calls. The cache appears to services as a single
store, and objects are allocated to individual cache servers using hashing on the object key.
   The basic scheme is shown in Figure 50. The service first checks the cache to see if the data
it requires is available. If so, it returns the cached contents as the results  we have what is
known as a cache hit. If the data is not in the cache  a cache miss - the service retrieves the
requested data from the database and writes the query results to the cache so it is available for
subsequent client requests without querying the database.

   Figure 50 Application Level Caching
   Lets return to our mythical winter resort business for a use case. At a busy resort, skiers and
boarders can use their mobile app to get an estimate of the lift wait times across the resort. This
enables them to plan and avoid congested areas where they will have to wait to ride a lift for say
15 minutes (or sometimes more!).
   Every time a skier loads a lift, a message is sent to the companys service that collects data
about skier traffic patterns. Using this data, the system can estimate lift wait times from the
number of skiers who ride a lift and the rate they are arriving. This is an expensive calculation as
it requires aggregating potentially 10s of thousands of lift ride records and performing the wait
time calculation. For this reason, once the results are calculated, they are deemed valid for five
minutes. Only after this time has elapsed is a new calculation performed and results produced.
   Figure 51 shows an example of how a stateless LiftWaitService might work. When a
request arrives, the service first checks the cache to see if the latest wait times are available. If
they are, the results are immediately returned to the client. If the results are not in the cache, the
service calls a downstream service which can perform the lift wait calculations and returns them
as a List. These results are then stored in the cache and then returned to the client.
   Cache access requires a key with which to associate the results with. In this example we
construct the key with the string liftwaittimes: concatenated with the resort identifier that
is passed by the client to the service. This key is hashed by the cache to identify the server where
the cached value resides. When we write a new value to the cache (line 8), we pass a value of
300 seconds as a parameter to the put operation. This is known as a time to live, or TTL value. It
tells the cache that after 300 seconds this key-value pair should be evicted from the cache as the
value is no longer relevant.
   When the cache value is evicted, the next request will calculate the new values and store
them in the cache. But while the cache value is valid, all requests will utilize it, meaning there is
no need to perform the expensive lift wait time calculation for every call. Hence if we get N
requests in a 5 minute period, N-1 are served from the cache. Imagine if N is 20,000. This is a
lot of expensive calculations saved.
1.	public class LiftWaitService {
2.
3.	  public List getLiftWaits(String resort) {
4.	    List liftWaitTimes = cache.get(liftwaittimes: + resort);
5.	      if (liftWaitTimes == null) {
6.	         liftWaitTimes = skiCo.getLiftWaitTimes(resort);
7.	         // add result to cache, expire in 300 seconds
8.	         cache.put("liftwaittimes:" + resort, liftWaitTimes, 300);
9.	      }
10.	    return liftWaitTimes;
11.	  }
12.	}
    Figure 51 Caching Example
   Using an expiry time like the TTL is a common way to invalidate cache contents. It ensures a
service doesnt deliver stale, out of date results to a client. It also enables the system to have
some control over cache contents, which are typically limited. If cached items are not flushed
periodically, the cache may fill up. In this case, a cache will adopt a policy such as least recently
used or least accessed to choose cached values to evict and create space for more current, timely
results.
   Application caching can provide significant throughput boosts, reduced latencies, and
increased client application responsiveness. The key to achieving these desirable qualities is to
satisfy as many requests as possible from the cache. This is known as the cache hit rate. The
general design principle is to maximize the cache hit rate and minimize the cache miss rate 
when a request cannot be satisfied by cached items. When a cache miss occurs, the request must
be satisfied through querying databases or downstream services. The results of the request can
then be written to the cache and hence be available for further accesses.
   Theres no hard and fast rule on what the cache hit rate should be, as it depends on the cost
of constructing the cache contents and the update rate of cached items. Ideal cache designs have
many more reads than updates. This is because when an item must be updated, the application
needs to invalidate cache entries that are now stale because of the update. This means the next
request will result in a cache miss.
   When items are updated regularly, the cost of cache misses can negate the benefits of the
cache. Service designers therefore need to carefully consider query and update patterns an
application experiences, and construct caching mechanisms that yield the most benefit. It is also
crucial to monitor the cache usage once a service is in production to ensure the hit and miss
rates are in line with design expectations. Caches will provide both management utilities and
APIs to enable monitoring of the cache usage characteristics. For example, memcached is
accessible through a telnet session. A large number of statistics are available, including the hit
and miss counts as shown in the snippet in Figure 52.
1.	STAT get_hits 98567
2.	STAT get_misses 11001
3.	STAT evictions 0
    Figure 52 Example of memcached monitoring output
   Application level caching is also known as the cache-aside pattern . The name references the
fact that the application code effectively bypasses the data storage systems if the required
request results are available. This contrasts with other caching patterns, in which the application
always reads from and writes to the cache. These are known as read-through, write-through and write-
behind caches as explained below:

	Read-through: The application satisfies all requests by accessing the cache. If the data
required is not available in the cache, a loader in invoked to access the data systems and
store the results in the cache for the application to utilize.
	Write-through: The application always writes updates to the cache. When the cache is
updated, a writer is invoked to write the new cache values to the database. When the
database is updated, the application can complete the request.
	Write-behind: Like write-through, except the application does not wait for the value to
written to the database from the cache. This increases request responsiveness at the
expense of possible lost updates if the cache server crashes before a database update is
completed. This is also known as a write-back cache, and internally is the strategy used
by most database engines.

   The beauty of these caching approaches is that they simplify application logic. Applications
always utilize the cache for reads and writes, and the cache provides the magic to ensure the
cache interacts appropriately with the backend storage systems. This contrasts with the cache-
side pattern, in which application logic must be cognizant of cache misses.
   The application still needs to make this magic happen of course. These strategies require a
cache technology which can be augmented with an application-specific handler that performs
database reads and writes when the application accesses the cache. For example, NCache
supports provider interfaces that the application implements. These are invoked automatically
on cache misses for read-through caches and on writes for write-through caches. Other such
caches are essentially dedicated database caches, and hence require cache access to be identical
to the underlying database model. An example of this is Amazons DynamoDB Accelerator
(DAX).  DAX sits between the application code and DynamoDB, and transparently acts as a
high-speed in memory cache to reduce database access times.
   One significant advantage of the cache-aside strategy is that it is resilient to cache failure. In
such circumstances, as the cache is unavailable, all requests are essentially handled as a cache
miss. Performance will suffer, but services will still be able to satisfy requests. In addition,
scaling cache-aside platforms such as Redis and Memcached is straightforward due their simple,
distributed key-value store model. For these reasons, the cache-aside pattern is the primary
approach seen in massively scalable systems.
Web Caching
   One of the reasons that Web sites are so highly responsive is that the Internet is littered with
Web caches. Web caches stores a copy of a given resource for a defined time period. The caches
intercept client requests and if they have a requested resource cached locally, it returns the copy
rather than forwarding the request to the target service. Hence many requests can be satisfied
without placing a burden on the service. Also, as the caches are closer to the client, the requests
will have lower latencies.
   Figure 53 gives an overview of the Web caching architecture. Multiple levels of caches exist,
starting from the clients local Web browser cache, local proxy caches within organizations and
Internet Service Providers, and reverse proxy caches that exist within the services execution
domain. Web browser caches are also known as private caches (for a single user) and proxy
caches are shared caches that support requests from multiple users.
   Caches typically store the results of GET requests only, and the cache key is the URI of the
associated GET. When a client sends a GET request, it may be intercepted by one or more
caches along the request path. Any cache with a fresh copy of the requested resource may
respond to the request. If no cached content is found, the request is served by the service
endpoint, which is also called the origin server.

   Figure 53 Web Caches in the Internet
   Services can control what results are cached and for how long they are stored by using
HTTP caching directives. Services set these directives in various HTTP response headers, as
shown in the simple example in Figure 54. We will describe these directives in the following
subsections.
1.	Response:
2.	HTTP/1.1 200 OK Content-Length: 9842
3.	Content-Type: application/json
4.	Cache-Control: public
5.	Date: Fri, 26 Mar 2019 09:33:49 GMT
6.	Expires: Fri, 26 Mar 2019 09:38:49 GMT
   Figure 54 Example HTTP Response with caching directives
  Cache-Control
   The Cache-Control HTTP header can be used by client requests and service responses to
specify how the caching should be utilized for the resources of interest.

*	no-store: Specifies that a resource from a request response should not be cached. This
is typically used for sensitive data that needs to be retrieved from the origin servers each
request.
*	no-cache: Specifies that a cached resource must be revalidated with an origin server
before use. We discuss revalidation in the Etag subsection below.
*	private: Specifies a resource can be cached only by a user-specific device such as a
Web browser
*	public: Specifies a resource can be cached by any proxy server
*	max-age: defines the length of time in seconds a cached copy of a resource should be
retained. After expiration, a cache must refresh the resource by sending a request to the
origin server.

  Expires and Last-Modified
   The Expires and Last-Modified HTTP headers interact with the max-age directive to
control how long cached data is retained.
   Caches have limited storage resources and hence must periodically evict items from memory
to create space. To influence cache eviction, services can specify how long resources in the
cache should remain valid, or fresh. Once this time period for a cached resource expires, it
becomes stale and may become a candidate for eviction. When a request arrives for a fresh
resource, the cache serves the locally stored results without contacting the origin server.
   Freshness is calculated using a combination of header values. The "Cache-Control: max-
age=N" header is the primary directive, and this value the specifies the freshness period in
seconds.
   If max-age is not specified, the Expires header is checked next. If this header exists, then it
is used to calculate the freshness period. As a last resort, the Last-Modified header can specify
the freshness lifetime based on a heuristic calculation that the cache can support. This is usually
calculated as (Date header value  Last-Modified header value)*0.1.

  Etag
   HTTP provides another directive that controls cache item freshness. This is known as an
Etag. An Etag is an opaque value that van be used by a cache to check if a cached resource is
still valid. Lets explain this using another winter sports example.
   A ski resort posts a weather report at 6am every day during the ski season. If the weather
changes during the day, the resort updates the report. Sometimes this happens two or three
times each day, and sometimes not at all if the weather is stable. When a request arrives for the
weather report, the service responds with a maximum age to define cache freshness, and also an
ETag that represents the version of the weather report that was last issued. This is shown in
Figure 55, which tells a cache to treat the weather report resource as fresh for at least 3600
seconds, or 60 minutes.
1.	Request:
2.	GET /skico.com/weather/Blackstone
3.
4.	Response:
5.	HTTP/1.1 200 OK Content-Length: ...
6.	Content-Type: application/json
7.	Date: Fri, 26 Mar 2019 09:33:49 GMT
8.	Cache-Control: public, max-age=3600
9.	ETag: 09:33:49"
<!-- Content omitted -->

   Figure 55 HTTP Etag Example
   For the next hour, the cache simply serves this cached weather report to all clients who issue
a GET request. This means the origin servers are freed from processing these requests  the
outcome that we want from effective caching. After an hour though, the resource become stale.
Now, when a request arrives for a stale resource, the cache forwards it to the origin server  with
a If-None-Match directive along with the Etag to enquire if the resource, in our case the
weather report, is still valid. This is known as revalidation.
   There are two possible responses to this request.

1.	If the Etag in the request matches the value associated with the resource in the services,
the cached value is still valid. The origin server can therefore return a 304 (Not
Modified) response, as shown in Figure 56. No response body is needed as the cached
value is still current, thus saving bandwidth, especially for large resources. The response
may also include new cache directives to update the freshness of the cached resource.
2.	The origin server may ignore the revalidation request and respond with a 200 response
code and a response body and Etag representing the latest version of the weather report.
1.	Request:
2.	GET /upic.com/weather/Blackstone
3.	If-None-Match: "09:33:49
4.	Response:
5.	HTTP/1.1 304 Not Modified
    Figure 56 Validating an Etag
   In the service implementation, a mechanism is needed to support revalidation. In our
weather report example, one strategy is as follows:

1.	Generate new daily report: The weather report is constructed and stored in a database.
The service also creates a new cache entry that identifies the weather report resource and
associates an Etag with this version of the resource, for example {#resortname-weather,
Etag value}.
2.	GET requests: When a GET request arrives, return the weather report and the Etag.
This will also populate Web caches along the network response path,
3.	Conditional GET requests: Lookup the Etag value in cache at {#resortname-
weather} and return 304 if the value has not changed. If the cached Etag has changed,
return 200 along with the latest weather report and a new Etag value.
4.	Update weather report: A new version of the weather report is stored in the database
and the cached Etag value is modified to represent this new version of the response.

   When used effectively, Web caching can significantly reduce latencies and save network
bandwidth. This is especially true for large items such as images and documents. Further, as
Web caches handle requests rather than application services, this reduces the request load on
origin servers, creating additional capacity.
   Proxy caches such as Squid  and Varnish  are extensively deployed in the Internet. Scalable
applications therefore exploit the powerful facilities provided by HTTP caching to exploit his
caching infrastructure.
Summary and Further Reading
   Caching is an essential component of any scalable distribution. Caching stores information
that is requested by many clients in memory and serves this information as the results to client
requests. While the information is still valid, it can be served potentially millions of times
without the cost of recreation.
   Application caching using a distributed cache is the most common approach to caching in
scalable systems. This approach requires the application logic to check for cached values when a
client request arrives and return these if available. If the cache hit rate is high, with most
requests being satisfied with cached results, load on backend services and database can be
considerably reduced.
   The Internet also has a built in, multilevel caching infrastructure. Applications can exploit
this through the use of cache directives that are part of HTTP headers. These directives enable a
service to specify what information can be cached, for how long it should be cached, and
protocol for checking to see if a stale cache entry is still valid.

?

CHAPTER 7
__________________________

Asynchronous Messaging Systems
Introduction to Messaging
  Messaging Primitives

  Message Persistence

  Publish-Subscribe

Example: RabbitMQ
  Producer-Consumer example

  Persistent Messages

  Message Distribution

  Message Filtering

  RabbitMQ RPC
Clustering and Mirroring

Messaging Patterns
  Competing Consumers

  At Least Once Processing

  Poison Messages
Summary and Further Reading

?
CHAPTER 8
__________________________

Serverless Processing Systems
   Scalable systems experience widely varying patterns of usage. For some applications, load
may be high during business hours and low or non-existent during the evening. Other
applications, for example an online concert ticket sales system, might have low background
traffic for 99% of the time. When tickets for a major series of shows are released, the demand
can spike by 100x of normal load for a number of hours before dropping down to background
levels. Elastic load balancing, as described in Chapter XXX, is one approach for handling these
spikes. Another is cloud-based serverless computing, which well examine in this chapter.
The Attractions of Serverless
   The transition of major organizational IT systems from on-premise to public cloud
platforms deployments seems inexorable. Organizations from startups to Government agencies
to multinationals see clouds as digital transformation platforms and a foundational technology
to improve business continuity.
   Two of the great attractions of cloud platforms is their pay-as-you-go billing and ability to
rapidly scale up (and down) virtual resources to meet fluctuating workloads and data volumes.
This ability to scale of course doesnt come for free. Your applications need to be architected to
leverage the scalable services provided by cloud platforms. And of course, as we discussed in
Chapter XXX, cost and scale are indelibly connected. The more resources a system utilizes for
extended periods, the larger the cloud bills will be at the end of the month.
   And monthly clouds bills can be big. Really big. Even worse, unexpectedly big! Cases of
sticker shock for significant cloud overspend are rife  in one survey 69% of respondents
regularly overspent on their cloud budget by more than 25%.  One well known case spent
$500K on an Azure task before it was noticed. Reasons attributed for overspending are many,
including lack of deployment of auto-scaling solutions, poor long-term capacity planning, and
inadequate exploitation of cloud architectures leading to sub-optimal system footprints.
   On a cloud platform, architects are confronted with a myriad of architectural decisions.
These are broad, in terms of the overall architectural pattern or style the systems adopts  e.g.
microservices, n-tier, event driven  and narrow, specific to individual components and the
cloud services that the system is built upon. In this sense, architecturally significant decisions
pervade all aspects of the system design and deployment on the cloud. And the consequences of
these decisions are highly apparent when you receive your monthly cloud spending bill.
   Traditionally, cloud applications have been deployed on an Infrastructure-as-a-Service (IaaS)
platform utilizing virtual machines (VMs). In this case, you pay for the resources you deploy
regardless of how highly utilized they are. If load increases, the application spins up new virtual
machines to increase capacity, typically using the cloud-provided load balancing service. Your
costs are essentially proportional to the type of VMs you choose, the duration they are deployed
for, and the amount of data the application stores and transmits.
   Since around 2016, major cloud providers have offered an alternative to explicitly
provisioning virtual processing resources. Known as serverless platforms, they do not require any
compute resources to be statically provisioned. Using technologies such as AWS Lambda or
Google App Engine (GAE), the application code is loaded and executed on demand, when
requests arrive. If there are no active requests, there are essentially no resources in use and no
charges to meet. Serverless platforms also manage autoscaling (up and down) for you. As
simultaneous requests arrive, additional processing capacity is created to handle requests and,
ideally, provide consistently low response times. When request loads drop, additional processing
capacity is decommissioned, and no charges are incurred.
   Every serverless platform varies in the details of its implementation. For example, a limited
number of mainstream programming languages and application server frameworks are typically
supported. Platforms also provide several configuration settings that can be used to balance
performance, scalability and costs. In general, costs are proportional to the type of server
instance chosen to execute a request, the number of requests and processing duration for each
request, and/or how long each application server instance remains resident on the serverless
infrastructure.
   Welcome to the world of cloud deployments. Every platform is proprietary and different is
subtle ways. The devil is, as usual, in the details. So lets explore some of those devilish details
for the Google App Engine and AWS Lambda platforms
Google App Engine
  The Basics
   Google App Engine (GAE) was the first offering from Google as part of what is now
known as the Google Cloud Platform. It has been in general release since 2011, and enables
developers to upload and execute HTTP-based application services on Googles managed cloud
infrastructure.
   GAE supports developing applications in Go, Java, Python,  Node.js,  PHP, .NET, and
Ruby. To build an application on GAE, developers can utilize common HTTP-based
application frameworks that are built with the GAE runtime libraries provided by Google. For
example, in Python, applications can utilize Flask, Django and web2py, and in Java the primary
supported platform is servlets built on the Jetty JEE web container.
   Application execution is managed dynamically by GAE, which launches compute resources
to meet request demand. Applications generally access a managed persistent storage platform
such as Googles Firestore  or Google Cloud SQL , or interact with a messaging service like
Googles Cloud PubSub .
   GAE comes in two flavors, known as the standard environment and the flexible
environment. The basic difference is that the standard environment is more closely managed by
GAE, with development restrictions in terms of language versions supported, but is able to
scale rapidly in response to increased loads. The flexible environment, as its name hints at, gives
more options in terms of development capabilities that can be used, but is not as suitable to
rapid scaling. The next two subsections expand on these two alternatives, and in the rest of
chapter, well focus on the highly scalable standard environment.
  GAE Standard Environment
   In the standard environment, developers upload their application code to a GAE project
that is associated with a base URL. This code must define HTTP endpoints that can be invoked
by clients making requests to the project URL. When a request is received, GAE will route it to
a processing instance to execute the application code. These are known as resident instances for
the application and are the major component of the cost incurred for utilizing GAE.
   Each project configuration can specify a collection of parameters that control when GAE
loads a new instance and invokes a resident instance. The two simplest settings control the
minimum and maximum instances that GAE will have resident. The minimum can be zero,
which is perfect for applications that have periods of inactivity, as this incurs no costs. When a
request arrives, GAE dynamically loads an application instance and invokes the processing for
the endpoint. Multiple simultaneous requests can be sent to the same instance, up to some
configured limit (more on this when we discuss auto-scaling). GAE will then load additional
instances on demand until the specified maximum instances value is reached. By setting the
maximum, an application can put a lid on costs, albeit with the potential for increased latencies
if load continues to grow.
   Standard environment applications can be built in Go, Java, Python,  Node.js,  PHP, and
Ruby.  As GAE itself is responsible for loading the runtime environment for an application, it
restricts the supported versions  to a small number per programming language. The language
used also affects the time to load a new instance on GAE. For example, a lightweight runtime
environment such as Go will start on a new instance in less than a second. In comparison, a
heavier weight Java Virtual Machine is of the order of 1-3 seconds on average. This load time is
also influenced by the number of external libraries that the application incorporates.
    Hence, while there is variability across languages, loading new instances is uniformly fast.
Much faster than booting a virtual machine. This makes the standard environment extremely
well suited for applications that experience rapid spikes in load. GAE is able to quickly add new
resident instances as request volumes increase.  Requests are dynamically routed to instances
based on load, and hence assume a purely stateless application model to support effective load
distribution. Subsequently, instances are released with little delay once the load drops, again
reducing costs.
   GAE is an extremely powerful platform for scalable applications, and one well explore in
much more detail in the case study later in this chapter.

  GAE Flexible Environment

  AutoScaling
   Autoscaling is an option that you specify in the app.yaml file that is passed to GAE when
you upload your server code. An autoscaled application is managed by GAE according to a
collection of default parameter values, which you can override in your app.yaml. The basic
scheme is shown in Figure 1.
   GAE basically manages the number of deployed server instances for an application
based on incoming traffic load. If there are no incoming requests, then GAE will not
schedule any instances and you will pay nothing. When a request arrives, GAE deploys an
instance to process the request.
   Deploying an instance can take anything from a few 100 milliseconds to a few seconds
depending on the programming language you are using. This means latency can be high if
there are no resident instances. To mitigate the instance loading latency effects, you can
specify a minimum number of instances to keep available for processing requests. This of
course costs money.
   As the request load grows, the GAE scheduler will dynamically load more instances to
handle requests. Three parameters control precisely how this operates, namely:
   Target CPU Utilization:	Sets the CPU utilization threshold above which more
instances will be started to handle traffic. The range is 0.5 (50%) to 0.95 (95%). The default
is 0.6 (60%).
   Maximum Concurrent Requests: Sets the maximum number of concurrent requests an
instance can accept before the scheduler spawns a new instance. The default value is 10,
and the maximum is 80. The documentation doesnt state the minimum allowed value, but
presumably 1 would define a single-threaded service.
   Target Throughput Utilization:	This is used in conjunction with the value specified
for maximum concurrent requests to specify when a new instance is started. The range is
0.5 (50%) to 0.95 (95%). The default is 0.6 (60%). It works like this - when the number of
concurrent requests for an instance reaches a value equal to maximum concurrent requests
value multiplied by the target throughput utilization, the scheduler tries to start a new
instance.
   Got that? ??
   So by default, an instance will handle 10 x 0.6 = 6 concurrent requests before a new
instance is created. And if these 6 (or less) requests cause the CPU utilization for an
instance to go over 60%, the scheduler will also try and create a new instance.
   I think.

   Figure 1 GAE Autoscaling
   But wait, theres more!
   You can also specify values to control when GAE scales instances up based on the time
requests spend in the request pending queue, waiting to be dispatched to an instance for
processing. This maximum pending latency parameter is the maximum amount of time that
App Engine should allow a request to wait in the pending queue before starting additional
instances to handle requests to reduce latency. The default value is 30ms. The lower the
value, the quicker an application will scale up. And the more it will probably cost you.
   Theres also a minimum pending latency parameter, with a default value of zero. If you
are brave, how the minimum and maximum values work together is explained here.
   These autoscaling parameter settings give us the ability to fine tune a services behavior
to balance performance and cost. Which is great, isnt it?

AWS Lambda
Case Study: Balancing Scalability and Costs
  The Basic Conundrum
  Dont Accept the Defaults
  Choosing Parameter Values
  Comparing Configurations
  Lessons Learned
Summary and Further reading

?
ABOUT THE AUTHOR

   Insert author bio text here. Insert author bio text here Insert author bio text here Insert
author bio text here Insert author bio text here Insert author bio text here Insert author bio text

?
?
   Random words section
   Database connections for a single database are also a limitation. Each open connection takes
database resources, and even if we restrict each server accessing the database to opening 50
connections maximum, the 4000 available on our GCP SQL server will be exhausted when we
deploy 80 application servers. Were hitting limits in scalability for a single database instance.

   Its usually recommended that clients follow something akin to an exponential backoff
algorithm as they see errors. The client blocks for a brief initial wait time on the first failure, but
as the operation continues to fail, it waits proportionally to 2n, where n is the number of failures
that have occurred. By backing off exponentially, we can ensure that clients arent hammering
on a downed server and contributing to the problem.

   Exponential backoff has a long and interesting history in computer networking.

   Furthermore, its also a good idea to mix in an element of randomness. If a problem with a
server causes a large number of clients to fail at close to the same time, then even with back off,
their retry schedules could be aligned closely enough that the retries will hammer the troubled
server. This is known as the thundering herd problem.

   We can address thundering herd by adding some amount of random jitter to each clients
wait time. This will space out requests across all clients, and give the server some breathing
room to recover.

   Another solution is to keep the per-session data in a database. Generally, this is bad for
performance because it increases the load on the database: the database is best used to store
information less transient than per-session data. To prevent a database from becoming a single
point of failure, and to improve scalability, the database is often replicated across multiple
machines, and load balancing is used to spread the query load across those replicas. Microsoft's
ASP.net State Server technology is an example of a session database. All servers in a web farm
store their session data on State Server and any server in the farm can retrieve the data.

       https://www.youtube.com/watch?v=eNliOm9NtCM
       https://engineering.fb.com/data-infrastructure/scribe/
       https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-
in-a-single-repository/fulltext
      https://www.pornhub.com/insights/2019-year-in-review
       https://kogo.iheart.com/content/2020-04-23-youtube-celebrates-15th-anniversary-by-
featuring-first-video-ever-posted/
       http://pcmuseum.tripod.com/comphis4.html
       https://www.internetsociety.org/internet/history-internet/brief-history-internet/
       https://en.wikipedia.org/wiki/Usenet
       https://en.wikipedia.org/wiki/History_of_the_World_Wide_Web
       https://www.nngroup.com/articles/100-million-websites/
       https://en.wikipedia.org/wiki/Dot-com_bubble
       https://www.internetworldstats.com/stats.htm
       https://blog.twitter.com/engineering/en_us/topics/infrastructure/2017/the-infrastructure-
behind-twitter-scale.html
       https://en.wikipedia.org/wiki/Sydney_Harbour_Bridge
       https://en.wikipedia.org/wiki/Sydney_Harbour_Tunnel
       https://en.wikipedia.org/wiki/Auckland_Harbour_Bridge
       https://en.wikipedia.org/wiki/Auckland_Harbour_Bridge#'Nippon_clip-ons'
       https://en.wikipedia.org/wiki/HipHop_for_PHP
       https://www.bloomberg.com/news/articles/2014-09-24/obamacare-website-costs-exceed-2-
billion-study-finds
       http://www.informationweek.com/healthcare/policy-and-regulation/oregon-dumps-failed-
health-insurance-exchange/d/d-id/1234875

       https://www.researchgate.net/publication/318049054_Chapter_2_Hyperscalability_-
_The_Changing_Face_of_Software_Architecture
       https://en.wikipedia.org/wiki/Flask_(web_framework)
       Mark Richards and Neal Ford, Fundamentals of Software Architecture: An Engineering
Approach 1st Edition, OReilly Media, 2020
       https://aws.amazon.com/ec2/instance-types/
       https://en.wikipedia.org/wiki/Reverse_proxy
       https://redis.io/
       https://memcached.org/
       https://www.allthingsdistributed.com/2019/08/modern-applications-at-aws.html
       https://samnewman.io/patterns/architectural/bff/
       https://en.wikipedia.org/wiki/Intel_80386
       https://en.wikipedia.org/wiki/Amdahl%27s_law
       https://en.wikipedia.org/wiki/Dining_philosophers_problem
       https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/package-summary.html
       Except Vector and HashTable, which are legacy classes, thread safe and slow!
       https://en.wikipedia.org/wiki/Wavelength-
division_multiplexing#:~:text=In%20fiber%2Doptic%20communications%2C%20wavelength,%2C%20
colors)%20of%20laser%20light.
       https://en.wikipedia.org/wiki/Core_router
       https://en.wikipedia.org/wiki/Tier_1_network#List_of_Tier_1_networks
       https://www.google.com/intl/en/ipv6/statistics.html
       http://www.opengroup.org/dce/
       http://www.corba.org
       https://docs.oracle.com/javase/9/docs/specs/rmi/index.html
       http://erlang.org/doc/man/rpc.html
       https://golang.org/pkg/net/rpc/
       Fielding, Roy Thomas (2000). "Architectural Styles and the Design of Network-based Software
Architectures". Dissertation. University of California, Irvine.
       https://en.wikipedia.org/wiki/Packet_loss
  https://medium.com/baseds/modes-of-failure-part-1-6687504bfed6#
       https://en.wikipedia.org/wiki/Two_Generals%27_Problem
       Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson. 1985. Impossibility of distributed
consensus with one faulty process. J. ACM 32, 2 (April 1985), 374382.
       https://medium.com/@chrshmmmr/consensus-in-blockchain-systems-in-short-691fc7d1fefe
       https://ieeexplore.ieee.org/abstract/document/8123011
       www.ntp.org

       https://chrony.tuxfamily.org/
       https://engineering.fb.com/production-engineering/ntp-service/
       https://en.wikipedia.org/wiki/Roy_Fielding
       https://app.swaggerhub.com/help/tutorials/openapi-3-tutorial
       https://app.swaggerhub.com
       Node.js is a notable exception here as it is single threaded. However, it employs an
asynchronous programming model for blocking I-Os that supports handling many simultaneous
requests.
       https://tools.ietf.org/html/rfc6265
       https://www.oracle.com/java/technologies/java-ee-glance.html
       https://expressjs.com/
       https://palletsprojects.com/p/flask/
       http://tomcat.apache.org/
       See https://tomcat.apache.org/tomcat-9.0-doc/config/executor.html for default Tomcat
Executor configuration settings
       https://en.wikipedia.org/wiki/Java_Management_Extensions
       https://en.wikipedia.org/wiki/JConsole
       https://github.com/javamelody/javamelody/wiki
       https://en.wikipedia.org/wiki/OSI_model
       http://cbonte.github.io/haproxy-dconv/2.3/intro.html#3.3.5
       http://cbonte.github.io/haproxy-dconv/2.3/intro.html#3.3.6
       https://www.nginx.com/blog/microservices-at-netflix-architectural-best-practices/
       https://blog.twitter.com/engineering/en_us/topics/infrastructure/2017/the-infrastructure-
behind-twitter-scale.html
       https://memcached.org/
       https://redis.io/
       https://www.ehcache.org/documentation/3.3/caching-patterns.html#cache-aside
       https://www.alachisoft.com/resources/docs/ncache/prog-guide/server-side-api-
programming.html
        https://aws.amazon.com/dynamodb/dax/
       http://www.squid-cache.org/
       https://varnish-cache.org/
       https://cloud.google.com/firestore, formerly known as Google Cloud DataStore.
       https://cloud.google.com/sql/docs
       https://cloud.google.com/pubsub
       https://cloud.google.com/appengine/docs/the-appengine-environments
   Building Scalable
Distributed Systems

  Ian Gorton

   Copyright  2021 Ian Gorton
   All rights reserved.
   ISBN:

DEDICATION

   Insert dedication text here. Insert dedication text here. Insert dedication text here. Insert
dedication text here. Insert dedication text here. Insert dedication text here. Insert dedication
text here. Insert dedication text here. Insert dedication text here. Insert dedication text here.

TABLE OF CONTENTS
   Introduction to Scalable Systems	10
   What is Scalability?	10
   System scale in early 2020s: Examples	12
   How did we get here? A short history of system growth	13
   The 1980s	13
   1990-1995	14
   1996-2000	14
   2000-2006	14
   2007-2020 (today)	15
   Scalability Basic Design Principles	15
   Scalability and Costs	17
   Summary	19
   References	19
   Distributed Systems Architectures: A Whirlwind Tour	20
   Basic System Architecture	20
   Scale Out	22
   Scaling the Database with Caching	23
   Distributing the Database	25
   Multiple Processing Tiers	26
   Increasing Responsiveness	28
   Summary and Further Reading	30
   An Overview of Concurrent Systems	31
   Why Concurrency?	31
   Threads in Java	33
   Order of Thread Execution	35
   Problems with Thread  Race Conditions	36
   Problems with Thread  Deadlock	39
   Thread States	43
   Thread Coordination	44
   Thread Pools	47
   Barrier Synchronization	49
   Thread-Safe Collections	51
   Summary and Further Reading	52
   Exercises	52
   Distributed Systems Fundamentals	54
   Communications Basics	54
   Communications Hardware	55
   Communications Software	57
   Remote Method Invocation	60
   Partial Failures	64
   Consensus in Distributed Systems	68
   Time in Distributed Systems	70
   Summary and Further Reading	72
   Application Services	73
   Service Design	73
   Application Programming Interface (API)	73
   Designing Services	75
   State Management	77
   Applications Servers	80
   Horizontal Scaling	82
   Load Balancing	83
   Load Distribution Policies	84
   Health Monitoring	84
   Elasticity	84
   Session Affinity	85
   Summary and Further Reading	87
   Caching	88
   Application Caching	88
   Web Caching	92
   Cache-Control	93
   Expires and Last-Modified	93
   Etag	93
   Summary and Further Reading	95

ACKNOWLEDGMENTS

   Insert acknowledgments text here. Insert acknowledgments text here. Insert
acknowledgments text here. Insert acknowledgments text here. Insert acknowledgments text
here. Insert acknowledgments text here. Insert acknowledgments text here. Insert
acknowledgments text here. Insert acknowledgments text here. Insert acknowledgments text
here.

CHAPTER 1
__________________________
Introduction to Scalable Systems
   The last 20 years have seen unprecedented growth in the size, complexity and capacity of
software systems. This rate of growth is hardly likely to slow down in the next 20 years  what
these future systems will look like is close to unimaginable right now. The one thing we can
guarantee is that more and more software systems will need to be built with constant growth -
more requests, more data, more analysis - as a primary design driver.
   Scalable is the term used in software engineering to describe software systems that can
accommodate growth. In this chapter we will explore what precisely is meant by the ability to
scale  known, not surprisingly, as scalability. Well also describe a few examples that put hard
numbers on the capabilities and characteristics of contemporary applications and give a brief
history of the origins of the massive systems we routinely build today. Finally, we will describe
two general principles for achieving scalability that will recur in various forms throughout the
rest of this book and examine the indelible link between scalability and cost.
What is Scalability?
   Intuitively, scalability is a pretty straightforward concept. If we ask Wikipedia for a
definition, it tells us scalability is the property of a system to handle a growing amount of work
by adding resources to the system. We all know how we scale a highway system  we add more
traffic lanes so it can handle a greater number of vehicles. Some of my favorite people know
how to scale beer production  they add more capacity in terms of the number and size of
brewing vessels, the number of staff to perform and manage the brewing process, and the
number of kegs they can fill with tasty fresh brews. Think of any physical system  a transit
system, an airport, elevators in a building  and how we increase capacity is pretty obvious.
   Unlike physical systems, software is somewhat amorphous. It is not something you can point
at, see, touch, feel, and get a sense of how it behaves internally from external observation. Its a
digital artifact. At its core, the stream of 1s and 0s that make up executable code and data are
hard for anyone to tell apart. So, what does scalability mean in terms of a software system?
   Put very simply, and without getting into definition wars, scalability defines a software
systems capability to handle growth in some dimension of its operations. Examples of
operational dimensions are:

*	the number of simultaneous user or external (e.g. sensor) requests a system can process
*	the amount of data a system can effectively process and manage
*	the value that can be derived from the data a system stores

   For example, imagine a major supermarket chain is rapidly opening new stores and
increasing the number of self-checkout kiosks in every store. This requires the core supermarket
software systems to:

*	Handle increased volume from item sale scanning without decreased response time.
Instantaneous responses to item scans are necessary to keep customers happy.
*	Process and store the greater data volumes generated from increased sales. This data is
needed for inventory management, accounting, planning and likely many other
functions.
*	Derive real-time (e.g. hourly) sales data summaries from each store, region and country
and compare to historical trends. This trend data can help highlight unusual events in
regions (e.g. unexpected weather conditions, large crowds at events, etc.) and help the
stores affected quickly respond.
*	Evolve the stock ordering prediction subsystem to be able to correctly anticipate sales
(and hence the need for stock reordering) as the number of stores and customers grow

   These dimensions are effectively the scalability requirements of a system. If, over a year, the
supermarket chain opens 100 new stores and grows sales by 400 times (some of the new stores
are big!), then the software system needs to scale to provide the necessary processing capacity to
enable the supermarket to operate efficiently. If the systems dont scale, we could lose sales as
customers are unhappy. We might hold stock that will not be sold quickly, increasing costs. We
might miss opportunities to increase sales by responding to local circumstances with special
offerings. All these reduce customer satisfaction and profits. None are good for business.
   Successfully scaling is therefore crucial for our imaginary supermarkets business growth, and
is in fact the lifeblood of many modern internet applications. But for most business and
Government systems, scalability is not a primary quality requirement in the early stages of
development and deployment. New features to enhance usability and utility become the drivers
of our development cycles. As long as performance is adequate under normal loads, we keep
adding user-facing features to enhance the systems business value.
   Still, its not uncommon for systems to evolve into a state where enhanced performance and
scalability become a matter of urgency, or even survival. Attractive features and high utility
breed success, which brings more requests to handle and more data to manage. This often
heralds a tipping point, where design decisions that made sense under light loads are now
suddenly technical debt. External trigger events often cause these tipping points  look in the
March/April 2020 media at the many reports of Government Unemployment and supermarket
online ordering sites crashing under demand caused by the coronavirus pandemic.
   Increasing a systems capacity in some dimension by increasing resources is commonly called
scaling up or scaling out  well explore the difference between these later. In addition, unlike
physical systems, it is often equally important to be able to scale down the capacity of a system to
reduce costs. The canonical example of this is Netflix, which has a predictable regional diurnal
load that it needs to process. Simply, a lot more people are watching Netflix in any geographical
region at 9pm than are at 5am. This enables Netflix to reduce its processing resources during
times of lower load. This saves the cost of running the processing nodes that are used in the
Amazon cloud, as well as societally worthy things such as reducing data center power
consumption. Compare this to a highway. At night when few cars are on the road, we dont
retract lanes (except for repairs). The full road capacity is available for the few drivers to go as
fast as they like.
   Theres a lot more to consider about scalability in software systems, but lets come back to
these issues after examining the scale of some contemporary software systems circa 2020.
System scale in early 2020s: Examples
   Looking ahead in this technology game is always fraught with danger. In 2008 I wrote [1]:

   While petabyte datasets and gigabit data streams are today's frontiers for data-intensive applications, no
doubt 10 years from now we'll fondly reminisce about problems of this scale and be worrying about the difficulties
that looming exascale applications are posing.

   Reasonable sentiments, it is true, but exascale? Thats almost commonplace in todays world.
Google reported multiple exabytes of Gmail in 2014 , and by now, do all Google services
manage a yottabyte or more? I dont know. Im not even sure I know what a yottabyte is!
Google wont tell us about their storage, but I wouldnt bet against it. Similarly, how much data
do Amazon store in the various AWS data stores for their clients. And how many requests does,
say, DynamoDB process per second collectively, for all client applications supported. Think
about these things for too long and your head will explode.
   A great source of information that sometimes gives insights into contemporary operational
scales are the major Internet companys technical blogs. There are also Web sites analyzing
Internet traffic that are highly illustrative of traffic volumes. Lets take a couple of point in time
examples to illustrate a few things we do know today. Bear in mind these will look almost quaint
in a year or four.

*	Facebooks engineering blog describes Scribe , their solution for collecting, aggregating,
and delivering petabytes of log data per hour, with low latency and high throughput.
Facebooks computing infrastructure comprises millions of machines, each of which
generates log files that capture important events relating to system and application
health. Processing these log files, for example from a Web server, can give development
teams insights into their applications behavior and performance, and support fault
finding. Scribe is a custom buffered queuing solution that can transport logs from
servers at a rate of several terabytes per second and deliver them to downstream analysis
and data warehousing systems. That, my friends, is a lot of data!
*	You can see live Internet traffic for numerous services at www.internetlivestats.com. Dig
around and youll find statistics like Google handles around 3.5 billion search requests a
day, Instagram uploads about 65 million photos per day, and there is something like 1.7
billion web sites. It is a fun site with lots of information to amaze you. Note the data is
not really live, just estimates based on statistical analyses of multiple data sources.
*	In 2016 Google published a paper describing the characteristics of their code base .
Amongst the many startling facts reported is: The repository contains 86TBs of data, including
approximately two billion lines of code in nine million unique source files. Remember, this was
2016.

   Still, real, concrete data on the scale of the services provided by major Internet sites remain
shrouded in commercial-in-confidence secrecy. Luckily, we can get some deep insights into the
request and data volumes handled at Internet scale through the annual usage report from one
tech company. You can browse their incredibly detailed usage statistics here from 2019 here .
Its a fascinating glimpse into the capabilities of massive scale systems. Beware though, this is
Pornhub.com. The report is not for the squeamish. Heres one PG-13 illustrative data point 
they had 42 billion visits in 2019! Ill let interested readers browse the data in the report to their
hearts content. Some of the statistics will definitely make your eyes bulge!
How did we get here? A short history of system
growth
   I am sure many readers will have trouble believing there was civilized life without Internet
search, YouTube and social media. By coincidence, the day I type this sentence is the 15 year
anniversary of the first video being uploaded to YouTube . Only 15 years. Yep, it is hard for
even me to believe. Theres been a lot of wine under the bridge since then. I cant remember
how we survived!
   So, lets take a brief look back in time at how we arrived at the scale of todays systems. This
is from a personal perspective  one which started at college in 1981 when my class of 60 had
access to a shared lab of 8 state-of-the-art so-called microcomputers . By todays standards, micro
they were not.
  The 1980s
   An age dominated by mainframe and minicomputers. These were basically timeshared
multiuser systems where users interacted with the machines via dumb terminals. Personal
computers emerged in the early 1980s and developed throughout the decade to become useful
business and (relatively) powerful development machines. They were rarely networked however,
especially early in the decade. The first limited incarnation of the Internet emerged during this
time . By the end of the 1980s, development labs, universities and increasingly businesses had
email and access to exotic internet-based resources such as Usenet discussion forums   think of
a relatively primitive and incredibly polite reddit.
  1990-1995
   Personal computers and networking technology, both LANs and WANS, continued to
improve dramatically through this period. This created an environment ripe for the creation of
the World Wide Web (WWW) as we know it today. The catalyst was the HTTP/HTML
technology that had been pioneered at CERN by Tim Berners-Lee  during the 1980s. In 1993
CERN made the WWW technology available on a royalty-free basis. And the rest is history  a
platform for information sharing and money-making had been created. By 1995, the number of
web sites was tiny, but the seeds of the future were planted with companies like Yahoo! in 1994
and Amazon and eBay in 1995
  1996-2000
   During this period, the number of web sites grew from around 10,000 to 10 million , a truly
explosive growth period. Networking bandwidth and access also grew rapidly, with initially dial-
up modems for home users (yep, dial-up) and then early broadband technologies becoming
available.
   This surge in users with Internet access heralded a profound change in how we had to think
about building systems. Take for example a retail bank. Before providing online services, it was
possible to accurately predict the loads the banks business systems would experience. You
knew how many people worked in the bank and used the internal systems, how many
terminals/PCs were connected to the banks networks, how many ATMs you had to support,
and the number and nature of connections to other financial institutions. Armed with this
knowledge, we could build systems that support say a maximum of 3000 concurrent users, safe
in the knowledge that this number could not be exceeded. Growth would also be relatively slow,
and probably most of the time (eg outside business hours) the load would be a lot less than the
peak. This made our software design decisions and hardware provisioning a lot easier.
   Now imagine our retail bank decides to let all customers have Internet banking access. And
the bank has 5 million customers. What is our maximum load now? How will load be dispersed
during a business day? When are the peak periods? What happens if we run a limited time
promotion to try and sign up new customers? Suddenly our relatively simple and constrained
business systems environment is disrupted by the higher average and peak loads and
unpredictability you see from Internet-based user populations.
   During this period, companies like Amazon, eBay, Google, Yahoo! and the like were
pioneering many of the design principles and early versions of advanced technologies for highly
scalable systems. They had to, as their request loads and data volumes were growing
exponentially.
  2000-2006
   The late 1990s and early 2000s saw massive investments in, and technological innovations
from so called dot com companies, all looking to provide innovative and valuable online
businesses. Spending was huge, and not all investments were well targeted. This led to a little
event called the dot com crash  during 2000/2001. By 2002 the technology landscape was
littered with failed investments  anyone remember Pets.Com? Nope. Me neither. About 50%
of dot coms disappeared during this period. Of those that survived, albeit with much lower
valuations, many have become the staples we all know and use today.
   The number of web sites grew from around 10 to 80 million during this period, and new
service and business models emerged. In 2005, YouTube was launched. 2006 saw Facebook
become available to the public. In the same year, Amazon Web Services, which had low key
beginnings in 2004, relaunched with its S3 and EC2 services. The modern era of Internet-scale
computing and cloud-hosted systems was born.
  2007-2020 (today)
   We now live in a world with nearly 2 billion web sites, of which about 20% are active. There
are something like 4 billion Internet users . Huge data centers operated by public cloud
operators like AWS, GCP and Azure, along with a myriad of private data centers, for example
Twitters operational infrastructure , are scattered around the planet. Clouds host millions of
applications, with engineers provisioning and operating their computational and data storage
systems using sophisticated cloud management portals. Powerful, feature-rich cloud services
make it possible for us to build, deploy and scale our systems literally with a few clicks of a
mouse. All you must do is pay your cloud provider bill at the end of the month.
   This is the world that this book targets. A world where our applications need to exploit the
key principles for building scalable systems and leverage highly scalable infrastructure platforms.
Bear in mind, in modern applications, most of the code executed is not written by your
organization. It is part of the containers, databases, messaging systems and other components
that you compose into your application through API calls and build directives. This makes the
selection and use of these components at least as important as the design and development of
your own business logic. They are architectural decisions that are not easy to change.
Scalability Basic Design Principles
   As we have already discussed, the basic aim of scaling a system is to increase its capacity in
some application-specific dimension. A common dimension is increasing the number of
requests that a system can process in a given time period. This is known as the systems
throughput. Lets use an analogy to explore two basic principles we have available to us for
scaling our systems and increasing throughput.
   In 1932, one of the worlds great icons, the Sydney Harbor Bridge , was opened. Now it is a
fairly safe assumption that traffic volumes in 2020 are somewhat higher than in 1932. If you
have driven over the bridge at peak hour in the last 30 years, then you know that its capacity is
exceeded considerably every day. So how do we increase throughput on physical infrastructures
such as bridges?
   This issue became very prominent in Sydney in the 1980s, when it was realized that the
capacity of the harbor crossing had to be increased. The solution was the rather less iconic
Sydney Harbor tunnel , which essentially follows the same route underneath the harbor. This
provides 4 more lanes of traffic, and hence added roughly 1/3rd more capacity to harbor
crossings. In not too far away Auckland, their harbor bridge  also had a capacity problem as it
was built in 1959 with only 4 lanes. In essence, they adopted the same solution as Sydney,
namely, to increase capacity. But rather than build a tunnel, they ingeniously doubled the
number of lanes by expanding the bridge with the hilariously named  Nippon Clipons , which
widened the bridge on each side. Ask a Kiwi to say Nippon Clipons and you will understand
why this is funny.
   These examples illustrate the first strategy we have in software systems to increase capacity.
We basically replicate the software processing resources to provide more capacity to handle
requests and thus increase throughput, as shown in Figure 1. These replicated processing
resources are analogous to the lane ways on bridges, providing a mostly independent processing
pathway for a stream of arriving requests. Luckily, in cloud-based software systems, replication
can be achieved at the click of a mouse, and we can effectively replicate our processing
resources thousands of times. We have it a lot easier than bridge builders in that respect.

   Figure 1 Increasing Capacity through Replication
   The second strategy for scalability can also be illustrated with our bridge example. In Sydney,
some observant person realized that in the mornings a lot more vehicles cross the bridge from
north to south, and in the afternoon we see the reverse pattern. A smart solution was therefore
devised  allocate more of the lanes to the high demand direction in the morning, and sometime
in the afternoon, switch this around. This effectively increased the capacity of the bridge
without allocating any new resources  we optimized the resources we already had available.
   We can follow this same approach in software to scale our systems. If we can somehow
optimize our processing, by maybe using more efficient algorithms, adding extra indexes in our
databases to speed up queries, or even rewriting our server in a faster programming language,
we can increase our capacity without increasing our resources. The canonical example of this is
Facebooks creation of (the now discontinued)  HipHop for PHP , which increased the speed
of Facebooks web page generation by up to 6 times by compiling PHP code to C++.
   Well revisit these two design principles  namely replication and optimization - many times
in the remainder of this book. You will see that there are many complex implications of
adopting these principles that arise from the fact that we are building distributed systems.
Distributed systems have properties that make building scalable systems interesting, where
interesting in this context has both positive and negative connotations.
Scalability and Costs
   Lets take a trivial hypothetical example to examine the relationship between scalability and
costs. Assume we have a Web-based (e.g. web server and database) system that can service a
load of 100 concurrent requests with a mean response time of 1 second. We get a business
requirement to scale up this system to handle 1000 concurrent requests with the same response
time. Without making any changes, a simple load test of this system reveals the performance
shown in Figure 2 (left). As the request load increases, we see the mean response time steadily
grow to 10 seconds with the projected load. Clearly this is not scalable and cannot satisfy our
requirements in its current deployment configuration.

   Figure 2 Scaling an application. (Left)  non-scalable performance. (Right)  scalable performance
   Clearly some engineering effort is needed in order to achieve the required performance.
Figure 2 (right) shows the systems performance after it has been modified. It now provides the
specified response time with 1000 concurrent requests. Hence, we have successfully scaled the
system. Party time!
   A major question looms however. Namely, how much effort and resources were required to
achieve this performance? Perhaps it was simply a case of scaling up by running the Web server
on a more powerful (virtual) machine. Performing such reprovisioning on a cloud might take 30
minutes at most. Slightly more complex would be reconfiguring the system to scale out and run
multiple instances of the Web server to increase capacity. Again, this should be a simple, low
cost configuration change for the application, with no code changes needed. These would be
excellent outcomes.
   However, scaling a system isnt always so easy. The reasons for this are many and varied, but
heres some possibilities:
*	the database becomes less responsive with 1000 requests per second, requiring an
upgrade to a new machine
*	the Web server generates a lot of content dynamically and this reduces response time
under load. A possible solution is to alter the code to more efficiently generate the
content, thus reducing processing time per request.
*	the request load creates hot spots in the database when many requests try to access and
update the same records simultaneously. This requires a schema redesign and
subsequent reloading of the database, as well as code changes to the data access layer.
*	the Web server framework that was selected emphasized ease of development over
scalability. The model it enforces means that the code simply cannot be scaled to meet
the request load requirements, and a complete rewrite is required. Another framework?
Another programming language even?

   Theres a myriad of other potential causes, but hopefully these illustrate the increasing effort
that might be required as we move from possibility (1) to possibility (4).
   Now lets assume option (1), upgrading the database server, requires 15 hours of effort and a
thousand dollars extra cloud costs per month for a more powerful server. This is not
prohibitively expensive. And lets assume option (4), a rewrite of the Web application layer,
requires 10,000 hours of development due to implementing in a new language (e.g. Java instead
of Ruby). Options (2) and (3) fall somewhere in between options (1) and (4). The cost of 10,000
hours of development is seriously significant. Even worse, while the development is underway,
the application may be losing market share and hence money due to its inability to satisfy client
requests loads. These kinds of situations can cause systems and businesses to fail.
   This simple scenario illustrates how the dimensions of resource and effort costs are
inextricably tied to scalability. If a system is not designed intrinsically to scale, then the
downstream costs and resources of increasing its capacity to meet requirements may be massive.
For some applications, such as Healthcare.gov , these (more than $2 billion) costs are borne
and the system is modified to eventually meet business needs. For others, such as Oregons
health care exchange , an inability to scale rapidly at low cost can be an expensive ($303million)
death knell.
   We would never expect someone would attempt to scale up the capacity of a suburban home
to become a 50 floor office building. The home doesnt have the architecture, materials and
foundations for this to be even a remote possibility without being completely demolished and
rebuilt. Similarly, we shouldnt expect software systems that do not employ scalable
architectures, mechanisms and technologies to be quickly evolved to meet greater capacity
needs. The foundations of scale need to be built in from the beginning, with the recognition
that the components will evolve over time. By employing design and development principles
that promote scalability, we can more rapidly and cheaply scale up systems to meet rapidly
growing demands.
   Software systems that can be scaled exponentially while costs grow linearly are known as
hyperscale systems, defined as:
   Hyper scalable systems exhibit exponential growth in computational and storage capabilities while
exhibiting linear growth rates in the costs of resources required to build, operate, support and evolve the required
software and hardware resources.
    You can read more about hyperscale systems in this article  [3].

Summary
   The ability to scale an application quickly and cost-effectively should be a defining quality of
the software architecture of contemporary Internet-facing applications. We have two basic ways
to achieve scalability, namely increasing system capacity, typically through replication, and
performance optimization of system components. The rest of this book will delve deeply into
how these two basic principles manifest themselves in constructing scalable distributed systems.
Get ready for a wild ride.
References

1.	Ian Gorton, Paul Greenfield, Alex Szalay, and Roy Williams. 2008. Data-Intensive
Computing in the 21st Century. Computer 41, 4 (April 2008), 3032.
2.	Rachel Potvin and Josh Levenberg. 2016. Why Google stores billions of lines of code
in a single repository. Commun. ACM 59, 7 (July 2016), 7887.
3.	Ian Gorton (2017). Chapter 2. Hyperscalability  The Changing Face of Software
Architecture. 10.1016/B978-0-12-805467-3.00002-8.
?

CHAPTER 2
__________________________

Distributed Systems Architectures:
A Whirlwind Tour
   In this chapter well introduce some of the fundamental approaches to scaling a software
system. The type of systems this book is oriented towards are the internet-facing systems we all
utilize every day. Ill let you name your favorite. These systems accept requests from users
through Web and mobile interfaces, store and retrieve data based on user requests or events
(e.g. a GPS-based system), and have some intelligent features such as providing recommendations
or providing notifications based on previous user interactions.
   Well start with a simple system design and show how it can be scaled. In the process, several
concepts will be introduced that well cover in much more detail later in this book. Hence this
chapter just gives a broad overview of these concepts and how they aid in scalability  truly a
whirlwind tour!
Basic System Architecture
   Virtually all massive scale systems start off small and grow due to their success. Its common,
and sensible, to start with a development framework such as Ruby on Rails or Django or
equivalent, which promotes rapid development to get a system quickly up and running.  A
typical, very simple software architecture for starter systems which closely resembles what you
get with rapid development frameworks is shown in Figure 3. This comprises a client tier,
application service tier, and a database tier. If you use Rails or equivalent, you also get a
framework which hardwires a Model-View-Controller (MVC) pattern for Web application
processing and an Object-Relational Mapper (ORM) that generates SQL queries.
   With this architecture, users submit requests to the application from their mobile app or
Web browser. The magic of Internet networking (see Chapter 4) delivers these requests to the
application service which is running on a machine hosted in some corporate or commercial
cloud data center. Communications uses a standard network protocol, typically HTTP.
   The application service runs code that supports an application programming interface (API)
that clients use to format data and send HTTP requests to. Upon receipt of a request, the
service executes the code associated with the requested API. In the process, it may read from or
write to a database, depending on the semantics of the API. When the request is complete, the
service sends the results to the client to display in their app or browser.

   Figure 3 Basic Multi-Tier Distributed Systems Architecture
   Many systems conceptually look exactly like this. The application service code exploits a
server execution environment that enables multiple requests from multiple users to be
processed simultaneously. Theres a myriad of these application server technologies  JEE and
Spring for Java, Flask for Python  that are widely used in this scenario. This approach leads to
what is generally known as a monolithic architecture . Monoliths grow in complexity as the
application becomes more feature rich. This eventually makes it hard to modify and test rapidly,
and the execution footprint can become extremely heavyweight as all the API implementations
run in the same application service.
   Still, if request loads stay relatively low, this application architecture can suffice. The service
has the capacity to process requests with consistently low latency. But if request loads keep
growing, this means latencies will grow as the service has insufficient CPU/memory capacity for
the concurrent request volume and hence requests will take longer to process. In these
circumstances, our single server is overloaded and has become a bottleneck.
   In this case, the first strategy for scaling is usually to scale up the application service
hardware. For example, if your application  is running on AWS, you might upgrade your server
from a modest t3.xlarge instance with 4 (virtual) CPUs and 16GBs of memory to a t3.2xlarge
instance which doubles the number of CPUs and memory available for the application .
   Scale up is simple. It gets many real-world applications a long way to supporting larger
workloads. It obviously just costs more money for hardware, but thats scaling for you.
   Its inevitable however for many applications the load will grow to a level which will swamp
a single server node, no matter how many CPUs and how much memory you have. Thats when
you need a new strategy  namely scaling out, or horizontal scaling, that we touched on in
Chapter 1.
Scale Out
   Scaling out relies on the ability to replicate a service in the architecture and run multiple
copies on multiple server nodes. Requests from clients are distributed across the replicas so that
in theory, if we have N replicas, each server node processes {#requests/N}. This simple
strategy increases an applications capacity and hence scalability.
   To successfully scale out an application, we need two fundamental elements in our design.
As illustrated in Figure 4, these are:

1)	Load balancer: All user requests are sent to a load balancer, which chooses a service
replica to process the request. Various strategies exist for choosing a target service, all
with the core aim of keeping each resource equally busy. The load balancer also relays
the responses from the service back to the client. Most load balancers belong to a class
of Internet components known as reverse proxies , which control access to server
resources for client requests. As an intermediary, reverse proxies add an extra network
hop for a request, and hence need to be extremely low latency to minimize the
overheads they introduce. There are many off-the-shelf load balancing solutions as well
as cloud-provider specific ones, and well cover the general characteristics of these in
much more detail in Chapter XXX.
2)	Stateless services: For load balancing to be effective and share requests evenly, the
load balancer must be free to send consecutive requests from the same client to different
service instances for processing. This means the API implementations in the services
must retain no knowledge, or state, associated with an individual clients session. When a
user accesses an application, a user session is created by the service and a unique session
identified is managed internally to identify the sequence of user interactions and track
session state. A classic example of session state is a shopping cart. To use a load balancer
effectively, the data representing the current contents of a users cart must be stored
somewhere  typically a data store  such that any service replica can access this state
when it receives a request as part of a user session. In Figure 4 this is labeled as a Session
Store.

   Figure 4 Scale out Architecture
   Scale out is attractive as, in theory, you can keep adding new (virtual) hardware and services
to handle increased request loads and keep request latencies consistent and low. As soon as you
see latencies rising, you deploy another server instance. This requires no code changes and
hence is relatively cheap  you just pay for the hardware you deploy.
   Scale out has another highly attractive feature. If one of the services fails, the requests it is
processing will be lost. But as the failed service manages no session state, these requests can be
simply reissued by the client and sent to another service instance for processing. This means the
application is resilient to failures in the service software and hardware, thus enhancing the
applications availability. Availability is a key feature of distributed systems, and one we will
discuss in depth in Chapter XXX.
   Unfortunately, as with any engineering solution, simple scaling out has limits. As you add
new service instances, the request processing capacity grows, potentially infinitely. At some
stage however, reality will bite and the capability of your single database to provide low latency
query responses will diminish. Slow queries will mean longer response times for clients. If
requests keep arriving faster than they are being processed, some system component will fail
due to resource exhaustion and clients will see exceptions and request timeouts. Essentially your
database has become a bottleneck that you must engineer away in order to scale your
application further.
Scaling the Database with Caching
   Scaling up by increasing the number of CPUs, memory and disks in a database server can go
a long way to scaling a system. For example, at the time of writing Google Cloud Platform can
provision a SQL database on a db-n1-highmem-96 node, which has 96 vCPUs, 624GB of memory,
30TBs of disk and can support 4000 connections. This will cost somewhere between $6K and
$16K per year, which sounds a good deal to me! Scaling up is a very common database
scalability strategy.
   Large databases need constant care and attention from highly skilled database administrators
to keep them tuned and running fast. Theres a lot of wizardry in this job  e.g. query tuning,
disk partitioning, indexing, on-node caching  and hence database administrators are valuable
people that you want to be very nice to. They can make you application services highly
responsive indeed.
   In conjunction with scale up, a highly effective approach is querying the database as
infrequently as possible in your services. This can be achieved by employing distributed caching
in the service tier. Caching stores recently retrieved and commonly accessed database results in
memory so they can be quickly retrieved without placing a burden on the database. For data that
is frequently read and changes rarely, your processing logic can be modified to first check a
distributed cache, such as a Redis  or memcached  store. These cache technologies are
essentially distributed Key-Value stores with very simple APIs. This scheme is illustrated in
Figure 5. Note that the Session Store from Figure 4 has disappeared. This is because we can use a
general-purpose distributed cache to store session identifiers along with application data.
   Accessing the cache requires a remote call from your service, but if the data you need is in
the cache, on a fast network this is far less expensive than querying the database instance.
Introducing a caching layer also requires your processing logic to be modified to check for
cached data. If what you want is not in the cache, your code must still query the database and
load the results into the cache as well as return it to the caller. You also need to decide when to
remove or invalidate cached results  this depends on your applications tolerance to serving
stale results to clients and the volume of data you cache.

   Figure 5 Introducing Distributed Caching
   A well-designed caching scheme can be absolutely invaluable in scaling a system. Caching
works great for data that rarely changes and is accessed frequently, such as inventory, event and
contact data. If you can handle a large percentage, like 80% or more, of read requests from your
cache, then you effectively buy extra capacity at your databases as they are not involved in
handling requests.
   Still, many systems need to rapidly access terabyte and larger data stores that make a single
database effectively prohibitive. In these systems, a distributed database is needed.
Distributing the Database
   There are more distributed database technologies around in 2020 than you probably want to
imagine. Its a complex area, and one well cover extensively later in Chapter XXX. In very
general terms, there are two major categories:

1)	Distributed SQL stores from major vendors such as Oracle and IBM. These enable
organizations to scale out their SQL database relatively seamlessly by storing the data
across multiple disks that are queried by multiple database engine replicas. These
multiple engines logically appear to the application as a single database, hence
minimizing code changes.
2)	Distributed so-called NoSQL stores from a whole array of vendors. These products use
a variety of data models and query languages to distribute data across multiple nodes
running the database engine, each with their own locally attached storage. Again, the
location of the data is transparent to the application, and typically controlled by the
design of the data model using hashing functions on database keys. Leading products in
this category are Cassandra, MongoDB and Neo4j.

   Figure 6 Scaling the Data Tier using a Distributed Database
   Figure 6 shows how our architecture incorporates a distributed database. As the data
volumes grow, a distributed database has features to enable the number of storage nodes to be
increased.  As nodes are added (or removed), the data managed across all nodes is rebalanced to
attempt to ensure the processing and storage capacity of each node is equally utilized.
   Distributed databases also promote availability. They support replicating each data storage
node so if one fails or cannot be accessed due to network problems, another copy of the data is
available. The models utilized for replication and the trade-offs these require (spoiler 
consistency) are covered in later chapters.
   If you are utilizing a major cloud provider, there are also two deployment choices for your
data tier. You can deploy your own virtual resources and build, configure, and administer your
own distributed database servers. Alternatively, you can utilize cloud-hosted databases. The
latter simplifies the administrative effort associated with managing, monitoring and scaling the
database, as many of these tasks essentially become the responsibility of the cloud provider you
choose. As usual, the no free lunch principle applies.
Multiple Processing Tiers
   Any realistic system that we need to scale will have many different services that interact to
process a request. For example, accessing a Web page on the Amazon.com web site can require
in excess of 100 different services being called before a response is returned to the user .
   The beauty of the stateless, load balanced, cached architecture we are elaborating in this
chapter is that we can extend the core design principles and build a multi-tiered application. In
fulfilling a request, a service can call one or more dependent services, which in turn are
replicated and load-balanced. A simple example is shown in Figure 7. There are many nuances
in how the services interact, and how applications ensure rapid responses from dependent
services. Again, well cover these in detail in later chapters.

   Figure 7 Scaling Processing Capacity with Multiple Tiers
   This design also promotes having different, load balanced services at each tier in the
architecture. For example, Figure 8 illustrates two replicated Internet-facing services that both
utilized a core service that provides database access. Each service is load balanced and employs
caching to provide high performance and reliability. This design is often used to provide a
service for Web clients and a service for mobile clients, each of which can be scaled
independently based on the load they experience. Its commonly called the Backend For
Frontend (BFF) pattern .

   Figure 8 Scalable Architecture with Multiple Services
   In addition, by breaking the application into multiple independent services, we can scale each
based on the service demand. If for example we see an increasing volume of requests from
mobile users and decreasing volumes from Web users, we can provision different numbers of
instances for each service to satisfy demand. This is a major advantage of refactoring monolithic
applications into multiple independent services, which can be separately built, tested, deployed
and scaled.
Increasing Responsiveness
   Most client application requests expect a response. A user might want to see all auction
items for a given product category or see the real estate that is available for sale in a given
location. In these examples, the client sends a request and waits until a response is received.
This time interval between sending the request and receiving the result is the latency of the
request. We can decrease latencies by using caching and precalculated responses, but many
requests will still result in a database access.
   A similar scenario exists for requests that update data in an application. If a user updates
their delivery address immediately prior to placing an order, the new delivery address must be
persisted so that the user can confirm the address before they hit the purchase button. The
latency in this case includes the time for the database write, which is confirmed by the response
the user receives.
   Some update requests however can be successfully responded to without fully persisting the
data in a database. For example, the skiers and snowboarders amongst you will be familiar with
lift ticket scanning systems that check you have a valid pass to ride the lifts that day. They also
record which lifts you take, the time you get on, and so on. Nerdy skier/snowboarders can then
use the resorts mobile app to see how many lifts they ride in a day.
   As a person waits to get on a lift, a scanner device validates the pass using an RFID chip
reader. The information about the rider, lift, and time are then sent over the Internet to a data
capture service operated by the ski resort. The lift rider doesnt have to wait for this to occur, as
the latency could slow down their loading process. Theres also no expectation from the lift
rider that they can instantly use their app to ensure this data has been captured. They just get on
the lift, talk smack with their friends, and plan their next run.
   Service implementations can exploit this type of scenario to improve responsiveness. The
data about the event is sent to the service, which acknowledges receipt and concurrently stores
the data in a remote queue for subsequent writing to the database. Writing a message to a queue
is much faster than writing to a database, and this enables the request to be successfully
acknowledged much more quickly. Another service is deployed to read messages from the
queue and write the data to the database. When the user checks their lift rides  maybe 3 hours
or 3 days later  the data has been persisted successfully. The basic architecture to implement
this approach is illustrated in Figure 9.

   Figure 9 Increasing Responsiveness with Queueing
   Whenever the results of a write operation are not immediately needed, an application can use
this approach to improve responsiveness and hence scalability. Many queueing technologies
exist that applications can utilize, and well discuss how these operate in later chapters. These
queueing platforms all provide asynchronous communications. A producer service writes to the
queue, which acts as temporary storage, while another consumer service removes messages from
the queue and makes the necessary updates to, in our example, a database that stores skier lift
ride details.
   The key is that the data eventually gets persisted. Eventually typically means a few seconds at
most but use cases that employ this design should be resilient to longer delays without
impacting the user experience.
Summary and Further Reading
   This chapter has provided a whirlwind tour of the major approaches we can utilize to scale
out a system as a collection of communicating services and distributed databases. Much detail
has been brushed over, and as you no doubt know, in software systems the devil is in the detail.
Subsequent chapters will therefore progressively start to explore these details, starting with
concurrent and distributed systems fundamentals. Next there are sections on the application
service and data management tiers as portrayed in the basic distributed systems architecture
blueprint described in this chapter.
   Another area this chapter has skirted around is the subject of software architecture. Weve
used the term services for distributed components in an architecture that implement application
business logic and database access. These services are independently deployed processes that
communicate using remote communications mechanisms such as HTTP. In architectural terms,
these services are most closely mirrored by those in the Service Oriented Architecture (SOA)
pattern, an established architectural approach for building distributed systems. A more modern
evolution of this approach revolves around microservices. These tend to be more cohesive,
encapsulated services that promote continuous development and deployment.
   Well touch on these differences in a later chapter. If youd like a much more in depth
discussion of these, and software architecture issues in general, then Mark Richards and Neal
Fords book is an excellent place to start.

Mark Richards and Neal Ford, Fundamentals of Software Architecture: An Engineering
Approach 1st Edition, OReilly Media, 2020

   Finally, theres a class of big data software architectures that address some of the issues that
come to the fore with very large data collections. One of the most prominent is data
reprocessing. This occurs when data that has already been stored and analyzed needs to be re-
analyzed due to code changes. This reprocessing may occurs due to software fixes, or the
introduction of new algorithms that can derive more insights from the original raw data. Theres
a good discussion of the Lambda and Kappa architectures, both of which are prominent in this
space, in this article.

   Jay Krepps, Questioning the Lambda Architecture,
https://www.oreilly.com/radar/questioning-the-lambda-architecture/
?
CHAPTER 3
__________________________

An Overview of Concurrent
Systems

   Scaling a system naturally involves adding multiple independently moving parts. We run our
servers on multiple machines, our databases across multiple storage nodes, all in the quest of
adding more capacity. Consequently, our solutions are distributed across multiple locations, with
each processing events concurrently.
   Any distributed system is hence by definition a concurrent system, even if each node is
processing events one at a time. The behavior of the various nodes has to be coordinated in
order to make the application behave as desired. As well see later, coordinating nodes in a
distributed system is fraught with dangers. Luckily, our industry has matured sufficiently to
provide complex, powerful software frameworks that hide many of these distributed system
nasties (most of the time!) from our applications.
   This chapter is concerned with concurrent behavior in our systems on a single node. By
explicitly writing our software to perform multiple actions concurrently, we can optimize the
processing on a single node, and hence increase our processing capacity both locally and system
wide. Well use the Java 7.0 concurrency capabilities for examples, as these are at a lower level of
abstraction than those introduced in Java 8.0. Knowing how concurrent systems operate closer
to the machine is useful foundational knowledge when building concurrent and distributed
systems.
   A final point. This chapter is a concurrency primer. It wont teach you everything you need
to know to build complex, high performance concurrent systems. It will also be useful if your
experience writing concurrent programs is rusty, or you have written concurrent code in another
programming language. The further reading section points you to more comprehensive
coverage of this topic for those who wish to delve deeper.
Why Concurrency?
   Think of a busy coffee shop. If everyone orders a simple coffee, then the barista can quickly
and consistently deliver each drink. Suddenly, the person in front of you orders a soy, vanilla, no
sugar, quadruple shot iced brew. Everyone in line sighs and starts reading their social media, In
two minutes the line is out of the door.
    Processing requests in Web applications is analogous to our coffee example. In a coffee
shop, we enlist the help of a new barista to simultaneously make coffees on a different machine
to keep the line length in control and serve customers quickly. In software, to make applications
responsive, we need to somehow process requests in our server an overlapping manner.
   In the good old days of computing, each CPU was only able to execute a single machine
instruction at any instant. If our server application runs on such a CPU, why do we need to
structure our software systems to execute multiple instructions concurrently? It all seems slightly
pointless.
   There is actually a very good reason. Virtually every program does more than just execute
machine instructions. For example, when a program attempts to read from a file or send a
message on the network, it must interact with the hardware subsystem (disk, network card) that
is peripheral to the CPU. Reading data from a modern hard disk takes around 10 milliseconds
(ms). During this time, the program must wait for the data to be available for processing.
   Now, even an ancient  CPU such as a circa 1988 Intel 80386  can execute more than 10
million instructions per second (mips). 10ms is 1/100th of a second. How many instructions
could our 80386 execute in 1/100th second. Do the math. Its a lot! A lot of wasted processing
capacity, in fact.
   This is how operating systems such as Linux can run multiple programs on a single CPU.
While one program is waiting for an input-output (I-O) event, the operating system schedules
another program to execute. By explicitly structuring our software to have multiple activities
that can be executed in parallel, the operating system can schedule tasks that have work to do
while others wait for I-O. Well see in more detail how this works with Java later in this chapter.
   In 2001, IBM introduced the worlds first multicore processor, a chip with two CPUs  see
Figure 10. Today, even my laptop has 16 CPUs, or cores as they are commonly known. With a
multicore chip, a software system that is structured to have multiple parallel activities can be
executed in parallel. Well, up the number of available cores, anyway. In this way, we can fully
utilize the processing resources on a multicore chip, and hence increase our applications
capacity.

   Figure 10 Simplified view of a multicore processor
   The primary way to structure a software system as concurrent activities is to use threads.
Every programming language has its own threading mechanism. The underlying semantics of all
these mechanisms are similar  there are only a few primary threading models in mainstream use
 but obviously the syntax varies by language. In the following sections, well see how threads
are supported in Java.
Threads in Java
   Every software process has a single thread of execution by default. This is the thread that the
operating system manages when it schedules the process for execution. In Java, the main()
function you specify as the entry point to your code defines the behavior of this thread. This
single thread has access to the programs environment and resources such as open file handles
and network connections. As the program calls methods in objects instantiated in the code, the
runtime stack is used to pass parameters and manage variable scopes. Standard programming
language run time stuff, basically. This is a sequential process.
   In your systems, you can use programming language features to create and execute additional
threads. Each thread is an independent sequence of execution and has its own runtime stack to
manage local object creation and method calls. Each thread also has access to the process
global data and environment. A simple depiction of this scheme is shown in Figure 11.

   Figure 11 Comparing a single and multithreaded process
   In Java, we can define a thread using a class that implements the Runnable interface and
defines a run() method. A simple example is depicted in Figure 12.
1.	class NamingThread implements Runnable {
2.
3.	  private String name;
4.
5.	 	public NamingThread(String threadName) {
6.		  name = threadName ;
7.	    System.out.println("Constructor called: " + threadName) ;
8.	  }
9.
10.	  public void run() {
11.		  //Display info about this  thread
12.	    System.out.println("Run called : " + name);
13.	    System.out.println(name + " : " + Thread.currentThread());
14.	    // and now terminate  ....
15.		}
16.	}
17.
   Figure 12 A Simple Java Thread class
   To execute the thread, we need to construct a Thread object using an instance of our
Runnable and call the start() method to invoke the code in its own execution context. This is
shown in Figure 13, along with the output of running the code. Note this example has two
threads  the main() thread and our own NamingThread. The main thread starts the
NamingThread, which executes asynchronously, and then waits for 1 second to give our
run() method in NamingThread ample time to complete. This order of execution can be seen
from examining the outputs in Figure 13.
   For illustration, we also call the static currentThread() method, which returns a string
containing:

*	The system generated thread identifier
*	The thread priority, which by default is 5 for all threads. Well cover priorities later.
*	The identifier of the parent thread  in this example both parent threads are main

   Note to instantiate a thread, we call the start() method, not the run() method we define
in the Runnable. The start() method contains the internal system magic to create the
execution context for a separate thread to execute (e.g. see Figure 11). If we call run() directly,
the code will execute, but no new thread will be created. The run() method will execute as part
of the main thread, just like any other Java method invocation that you know and love. You will
still have a single threaded code.
1.	public static void main(String[] args) {
2.
3.	  NamingThread name0 = new NamingThread("My first thread");
4.
5.	  //Create the thread
6.	  Thread t0 = new Thread (name0);
7.
8.	  // start the threads
9.	  t0.start();
10.
11.	  //delay the main thread for a second (1000 miliiseconds)
12.	  try {
13.	    Thread.currentThread().sleep(1000);
14.	  } catch (InterruptedException e) {
15.	      }
16.
17.	      //Display info about the main thread and terminate
18.	      System.out.println(Thread.currentThread());
19.	    }
20.
21.	===EXECUTION OUTPUT===
22.	Constructor called: My first thread
23.	Run called : My first thread
24.	My first thread : Thread[Thread-0,5,main]
25.	Thread[main,5,main]

Figure 13 Creating and executing a thread
   In the example, we use sleep() to make sure the NamimgThread will terminate before the
main thread. Coordinating two threads by delaying for an absolute time period (e.g. 1 second in
our example) is not a very robust mechanism. What if for some reason - a slower CPU, a long
delay reading disk, additional complex logic in the method  our thread doesnt terminate in the
expected time frame? In this case, main will terminate first  this is not we intend. In general, if
you are using absolute times for thread coordination, you are doing it wrong. Almost always.
Like 99.99999% of the time.
   A simple and robust mechanism for one thread to wait until another has completed its work
is to use the join() method. In Figure 13, we could replace the try-catch block with:

   t0.join();

   This method causes the calling thread (in our case, main) to block until the thread referenced
by t0 terminates. If the referenced thread has terminated before the call to join(), then the
method call returns immediately. In this way we can coordinate, or synchronize, the behavior of
multiple threads. Synchronization of multiple threads is in fact the major focus of rest of this
chapter.
Order of Thread Execution
   TL;DR The system scheduler (in Java, this lives in the JVM) controls the order of thread
execution. From the programmers perspective, the order of execution is non-deterministic. Get
used to that term, well use it a lot. The concept of non-determinism is fundamental to
understanding multithreaded code.
   Lets illustrate this by building on our earlier example. Instead of creating a single
NamingThread, lets create and start up a few. 3 in fact, as shown in Figure 14:
1.	NamingThread name0 = new NamingThread("thread0");
2.	      NamingThread name1 = new NamingThread("thread1");
3.	      NamingThread name2 = new NamingThread("thread2");
4.
5.	      //Create the threads
6.	      Thread t0 = new Thread (name0);
7.	      Thread t1 = new Thread (name1);
8.	      Thread t2 = new Thread (name2);
9.
10.	      // start the threads
11.	      t0.start();
12.	      t1.start();
13.	      t2.start();
14.
15.	===EXECUTION OUTPUT===
16.	Run called : thread0
17.	thread0 : Thread[Thread-0,5,main]
18.	Run called : thread2
19.	Run called : thread1
20.	thread1 : Thread[Thread-1,5,main]
21.	thread2 : Thread[Thread-2,5,main]
22.	Thread[main,5,main]
   Figure 14 Multiple threads exhibiting non-deterministic behavior
   The output shown in Figure 14 is a sample from just one execution. You can see the code
starts three threads sequentially, namely t0, t1 and t2 (lines 11-13). Looking at the execution
trace, we see thread t0 completes (line 17) before the others start. Next t2s run() method is
called (line 18) followed by t1s run() method, even though t1 was started before t2. Thread t1
then runs to completion (line 20) before t1, and eventually the main thread and the program
terminate.
   This is just one possible order of execution. If we run this program again, we will almost
certainly see a different execution trace. This is because the JVM scheduler is deciding which
thread to execute, and for how long. Simply, once the scheduler has given a thread an execution
time slot on the CPU, it interrupts the thread and schedules another one to run. This ensures
each threads is given an opportunity to make progress. Hence the threads run independently
and asynchronously until completion, and the scheduler decides which thread runs when based
on a scheduling algorithm.
   We will examine the basic scheduling algorithm used later in this chapter. But for now, there
is a major implication for programmers, namely, regardless of the order of thread execution,
your code should produce correct results. Sounds easy?
   Read on.
Problems with Thread  Race Conditions
   Non-deterministic execution of threads implies that the code statements that comprise the
threads:

*	Will execute sequentially as defined for each thread
*	Can be overlapped in any order across threads, as how many statements are executed for
each thread execution slot is the scheduler.

   Hence, when many threads are executed on a single processor, their execution is interleaved.
The executes some steps from one thread, then performs some steps from another, and so on.
If we are executing on a multicore CPU, then we can execute one thread per core. The
statements of each thread execution are still however interleaved in a non-deterministic manner.
   Now, if every thread simply does its own thing, and is completely independent, this is not a
problem. Each thread executes until it terminates, as in our trivial example in Figure 14. Piece of
cake! Why are these thread things meant to be complex?
   Unfortunately, totally independent threads are not how most multithreaded systems behave.
If you refer back to Figure 11, you will see that multiple threads share the global data within a
process. In Java this is both global and static data.
   Threads can use shared data structures to coordinate their work and communicate status
across threads. For example, we may have threads handling requests from a Web clients, one
thread per request. We also want to keep a running total of how many requests we process each
day. When a thread completes a request, it increments a global RequestCounter object that all
threads share and update after each request. At the end of the day, we know how many requests
were processed. A lovely solution indeed.
   Figure 15 shows a very simple implementation that mimics our example scenario by creating
50k threads to update a shared counter. Note we use a lambda function for brevity to create the
threads, and a dont do this at home 5 second delay in main to allow the threads to finish.
   What you can do at home is run this code a few times and see what results you get. In 10
executions my mean was 49995. I didnt once get the correct answer, namely 50000. Weird.
   Why?
1.	public class RequestCounter {
2.	  final static private int NUMTHREADS = 50000;
3.	  private int count = 0;
4.
5.	  public  void inc() {
6.	    count++;
7.	  }
8.
9.	  public int getVal() {
10.	    return this.count;
11.	  }
12.
13.	  public static void main(String[] args) throws InterruptedException
{
14.	    final RequestCounter counter = new RequestCounter();
15.
16.	    for (int i = 0; i < NUMTHREADS; i++) {
17.	      // lambda runnable creation
18.	      Runnable thread = () -> {counter.inc(); };
19.		    new Thread(thread).start();
20.	    }
21.
22.	    Thread.sleep(5000);
23.	    System.out.println("Value should be " + NUMTHREADS + "It is: " +
counter.getVal());
24.	  }
25.	}
   Figure 15 Example of a race condition
   The answer lies in how abstract, high-level programming language statements, in Java in this
case, are executed on a machine. To perform an increment of a counter, the CPU must (1) load
the current value into a register, (2) increment the register value, and (3) write the results back to
the original memory location.
   As Figure 16 shows, at the machine level these three operations are not indistinguishable, or
as more commonly known, atomic. One thread can load the value into the register, but before it
writes the incremented value back, the scheduler interrupts it and allows another thread to start.
This thread loads the old value of the counter and writes back the incremented value.
Eventually the original thread executes again, and writes back its incremented value, which just
happens to be the same as what is already in memory.
   Weve lost an update. From our 10 tests of the code in Figure 15, we see this is happening
on average 5 times in 50000 increments.  Hence such events are rare, but even if it happens 1
time in 10 million, you still have an incorrect result.

   Figure 16 Increments are not atomic at the machine level
   This is called a race condition. Race conditions can occur whenever multiple threads make
changes to some shared state, in this case a simple counter. Essentially, different interleaving sof
the threads can produce different results. Race conditions are insidious, evil errors, because their
occurrence is typically rare, and they can be hard to detect as most of the time the answer will
be correct. Try running the code in Figure 15 with 1000 threads instead of 50000, and you will
see this in action. I got the correct answer four times out of 5.
   Same code. Occasionally different results. Like I said  race conditions evil! Luckily,
eradicating them is straightforward if you take a few precautions.
   The key is to identify and protect critical sections. A critical section is a section of code that
updates shared data structures, and hence must be executed atomically if accessed by multiple
threads. Our example of incrementing a shared counter is an example of a critical section. What
about removing an item from a list? We need to delete the head node of the list, and move the
reference to the head of the list from the removed node to the next node in the list. Both
operations must be performed atomically to maintain the integrity of the list. Its a critical
section.
   In Java, the synchronized keyword defines a critical section. If use to decorate a method,
then when multiple threads attempt to call that method on the same shared object, only one is
permitted to enter the critical section. All others block until the thread exits the synchronized
method, at which point the scheduler chooses the next thread to execute the critical section. We
say the execution of the critical section is serialized, as only one thread at a time can be
executing the code inside it.
   To fix the example in Figure 15, we therefore just need to identify the inc() method as a
critical section, ie:

   synchronized public void inc() {
       count++;
     }

   Test it out as many times as you like. Youll always get the correct answer. Slightly more
formally, this means any interleaving of the threads thar the scheduler throws at us will always
produce the correct results.
   The synchronized keyword can also be applied to blocks of statements within a method. For
example, we could rewrite the above example as:

   public void inc() {
        synchronized(this){
              count++;
           }
   }

   Underneath the covers, every Java object has a monitor lock, sometimes known as an
intrinsic lock, as part of its runtime representation. To enter a synchronized method or block of
statements as shown above, a thread must acquire the monitor lock. Only one thread can own
the lock at any time, and hence execution is serialized. This, very basically, is how Java
implements critical sections.
   Aa a rule of thumb, we should keep critical sections as small as possible so that the serialized
code is minimized. This can have positive impacts on performance and hence scalability. Well
return to this topic later, but if you are interested, have a read about Amdahls Law  for an
explanation on why this is desirable.
Problems with Thread  Deadlock
   To ensure correct results in multithreaded code, we have seen we have to restrict non-
determinism to serialize access to critical sections. This avoids race conditions. However, if we
are not careful, we can write code that restricts non-determinism so much that our program
stops. This is formally known as a deadlock.
   A deadlock occurs when two threads acquire exclusive access to a resource that the other
thread also needs to make progress. The scenario below shows how this can occur:

   Two threads sharing access to two shared variables via synchronized blocks
1.	thread 1: enters critical section A
2.	thread 2: enters critical section B
3.	thread 1: blocks on entry to critical section B
4.	thread 2: blocks on entry to critical section A
5.	Both threads wait forever ?

   A deadlock, also known as a deadly embrace, causes a program to stop. It doesnt take a
vivid imagination to realize that this can cause all sorts of undesirable outcomes. Im happily
texting away while my autonomous vehicle drives me to the bar. Suddenly, the code deadlocks.
It wont end well.
   Deadlocks occur in more subtle circumstances than the simple example above. The classic
example is the Dining Philosophers  problem. The story goes like this.
   Five philosophers sit around a shared table. Being philosophers, they spend a lot of time
thinking deeply. In between bouts of deep thinking, they replenish their brain functions by
eating from a plate of food that sits in front of them. Hence a philosopher is either eating or
thinking, or transitioning between these two states.
   In addition, the philosophers must all be very close, highly dexterous and all COVID19
vaccinated friends, as they share chop sticks to eat with. Only five chopsticks are on the table,
placed between each philosopher. When one wishes to eat, they follow a protocol of picking up
their left chopstick first, then their right chopstick. Once they are ready to think again, they first
return the right chopstick, then the left.

   Figure 17 The Dining Philosophers Problem
   Figure 17 depicts our all-female philosophers, each identified by a unique number. As each is
either concurrently eating or thinking, we can model each philosopher as a thread. The code is
shown in Figure 18. The shared chopsticks are represented by instances of the Java Object
class. As only one object can hold the monitor lock on an object, they are used as entry
conditions to the critical sections in which the philosophers acquire the chopsticks they need to
eat. After eating, the chopsticks are returned to the table and the lock is released on each so that
neighboring philosophers can eat whenever they are ready.
1.	public class Philosopher implements Runnable {
2.
3.	  private final Object leftChopStick;
4.	  private final Object rightChopStick;
5.
6.	  Philosopher(Object leftChopStick, Object rightChopStick) {
7.	    this.leftChopStick = leftChopStick;
8.	    this.rightChopStick = rightChopStick;
9.	  }
10.	  private void LogEvent(String event) throws InterruptedException {
11.	    System.out.println(Thread.currentThread()
12.	                                  .getName() + " " + event);
13.	    Thread.sleep(1000);
14.	  }
15.
16.	  public void run() {
17.	    try {
18.	      while (true) {
19.	        LogEvent(": Thinking deeply");
20.	        synchronized (leftChopStick) {
21.	          LogEvent( ": Picked up left chop stick");
22.	          synchronized (rightChopStick) {
23.	            LogEvent(": Picked up right chopstick  eating");
24.	            LogEvent(": Put down right chopstick");
25.	          }
26.	          LogEvent(": Put down left chopstick. Ate too much");
27.	        }
28.	      } // end while
29.	    } catch (InterruptedException e) {
30.	       Thread.currentThread().interrupt();
31.	  }
32.	 }
33.	}
   Figure 18 A Philosopher thread
   To bring our philosophers to life, we must instantiate a thread for each and give each
philosopher access to its neighboring chopsticks. This is done through the thread constructor
call on line 16 in Figure 19. In the for loop we create five philosophers and start these as
independent threads, where each chopstick is accessible to two threads, one as a left chopstick,
and one as a right.
1.	private final static int NUMCHOPSTICKS = 5 ;
2.	private final static int NUMPHILOSOPHERS = 5;
3.	public static void main(String[] args) throws Exception {
4.
5.	  final Philosopher[] ph = new Philosopher[NUMPHILOSOPHERS];
6.	  Object[] chopSticks = new Object[NUMCHOPSTICKS];
7.
8.	  for (int i = 0; i < NUMCHOPSTICKS; i++) {
9.	    chopSticks[i] = new Object();
10.	  }
11.
12.	  for (int i = 0; i < NUMPHILOSOPHERS; i++) {
13.	    Object leftChopStick = chopSticks[i];
14.	    Object rightChopStick = chopSticks[(i + 1) % chopSticks.length];
15.
16.	    ph[i] = new Philosopher(leftChopStick, rightChopStick);
17.	            }
18.
19.	    Thread th = new Thread(ph[i], "Philosopher " + (i + 1));
20.	    th.start();
21.	  }
22.	}
   Figure 19 Dining Philosophers - deadlocked version
   Running this code produces the following output on my first attempt. This was lucky. If you
run the code you will almost certainly see different outputs, but the final outcome will be the
same.

   Philosopher 4 : Thinking deeply
   Philosopher 5 : Thinking deeply
   Philosopher 1 : Thinking deeply
   Philosopher 2 : Thinking deeply
   Philosopher 3 : Thinking deeply
   Philosopher 4 : Picked up left chop stick
   Philosopher 1 : Picked up left chop stick
   Philosopher 3 : Picked up left chop stick
   Philosopher 5 : Picked up left chop stick
   Philosopher 2 : Picked up left chop stick

   10 lines of output, then  nothing! We have a deadlock. This is a classic circular waiting
deadlock. Imagine the following scenario:

1.	Each philosopher indulges in a long thinking session
2.	Simultaneously, they all decide they are hungry and reach for their left chop stick.
3.	No philosopher can eat (proceed) as none can pick up their right chop stick

   The philosophers in this situation would figure out some way to proceed by putting down a
chop stick or two until one or more can eat. We can sometimes do this is our software by using
timeouts on blocking operations. When the timeout expires a thread releases the critical section
and retries, allowing other blocked threads a chance to proceed. This is not optimal though, as
blocked threads hurt performance, and setting timeout values in an inexact science.
   Its much better however to design a solution to be deadlock free. This means that one or
more threads will always be able to make progress. With circular wait deadlocks, this can be
achieved by imposing a resource allocation protocol on the shared resources, so that threads will
not always request resources in the same order.
   In the Dining Philosophers problem, we can do this by making sure one of our philosophers
picks up their right chop stick first. Lets assume we instruct Philosopher 4 to do this.  This
leads to a possible sequence of operations such as below:

   Philosopher 0 picks up left chopstick (chopStick[0]) then right (chopStick[1])
   Philosopher 1 picks up left chopstick (chopStick[1]) then right (chopStick[2])
   Philosopher 2 picks up left chopstick (chopStick[2]) then right (chopStick[3])
   Philosopher 3 picks up left chopstick (chopStick[3]) then right (chopStick[4])
   Philosopher 4 picks up left chopstick (chopStick[0]) then right (chopStick[4])

   In this example, Philosopher 4 must block, as Philosopher 0 already has acquired access to
chopstick[0]. With Philosopher 4 blocked, Philosopher 3 is assured access to chopstick[4] and
can then proceed to satisfy their appetite.
   The fix for the Dining Philosophers solution is shown in Figure 20.
1.	if (i == NUMPHILOSOPHERS - 1) {
2.	  // The last philosopher picks up the right fork first
3.	  ph[i] = new Philosopher(rightChopStick, leftChopStick);
4.	} else {
5.	  // all others pick up the left chop stick first
6.	  ph[i] = new Philosopher(leftChopStick, rightChopStick);
7.	}
8.	            }
    Figure 20 Solving the Dining Philosophers deadlock
   More formally we are imposing an ordering on the acquisition of shared resources, such that:

   chopStick[0]< chopStick[1]< chopStick[2]< chopStick[3]< chopStick[4]

   This means each thread will always attempt to acquire chopstick[0] before chopstick[1], and
chopstick[1] before chopstick[2], and so on. For philosopher 4, this means it will attempt to
acquire chopstick[0] before chopstick[4], thus breaking the potential for a circular wait deadlock.
   Deadlocks are a complicated topic and this section has just scratched the surface. Youll see
deadlocks in many deployed systems, and well revisit them later when discussing concurrent
database accessed and locking.

Thread States
   Lets briefly describe the various states that a thread can have during its lifecycle.
Multithreaded systems have a system scheduler that decides which threads to run when. In Java,
the scheduler is known as a preemptive, priority-based scheduler. In short this means it chooses
to execute the highest priority thread which wishes to run.
   Every thread has a priority (by default 5, range 0 to 10). A thread inherits its priority from its
parent thread. Higher priority threads get scheduled more frequently than lower priority threads,
but in most applications having all threads as the default priority suffices.
   Scheduling is a JVM-specific implementation detail based on the Java specification. But in
general schedulers behave as follows.
   The scheduler cycles threads through four states, based on their behavior. These are:
   Created: A thread object has been created but its start() method has not been invoked.
Once start() in invoked, the thread enters the runnable state.
   Runnable: A thread is able to run. The scheduler will choose which thread(s) to execute in a
FIFO manner. Threads then execute until they block (e.g. on a synchronized statement),
execute a yield(), suspend() or sleep() statement,  or the run() method terminates, or
are preempted by the scheduler. Preemption occurs when a higher priority thread becomes
runnable, or when a system-specific timeslice value expires. Timeslicing allows the scheduler to
ensure that all threads eventually get chance to execute  no execution hungry threads can hog
the CPU.
   Blocked: A thread is blocked if it is waiting for a lock, a notification event to occur (e.g.
sleep timer to expire, resume() method executed), or is waiting for a network or disk request to
complete. When the event a blocked thread is waiting for occurs, it moves back to the runnable
state.
   Terminated: A threads run() method has completed or it has called the stop() method. The
thread will no longer be scheduled.
   An illustration of this scheme is in Figure 21. The scheduler effectively maintains FIFO
queue in the Runnable state for each thread priority. High priority threads are used typically to
respond to events (eg an emergency timer), and execute for a short period of time. Low priority
threads are used for background, ongoing tasks like checking for corruption of files on disk
through recalculating checksums.

   Figure 21 Threads states and transitions

Thread Coordination
   There are many problems that require threads with different roles to coordinate their
activities. Imagine a collection of threads that accept a document, do some processing on that
document (e.g. generate a pdf), and then send the processed document to a shared printer pool.
Each printer can only print one document at a time, so it reads from a shared print queue,
printing documents in the order they arrive.
   This printing problem is an illustration of the classic producer-consumer problem. Producers
generate and send messages via a shared FIFO buffer to consumers. Consumers retrieve these
messages, process them, and then go to try get more work from the buffer. A simple illustration
of this problem is in Figure 22. Its a bit like a 24 hour, 365 day restaurant in New York, the
kitchen keeps producing and the wait staff collect the food and deliver to hungry diners.
Forever.
   Like virtually all real things, the buffer has a limited capacity. Producers generate new items,
but if the buffer is full, they must wait until some item(s) have been consumer before they can
add the new item to the buffer.  Similarly, if the consumers are consuming faster then the
producers are producing, them they must wait if there are no items in the buffer, and somehow
get alerted when new items arrive.

   Figure 22 The Producer Consumer Problem
   One way for a producer to wait for space in the buffer, or a consumer to wait for an item, is
to keep retrying an operation. A producer could sleep for a second, and then retry the put
operation until it succeeds. A consumer could do likewise.
   This solution is called polling, or busy waiting. It works fine, but as the second name implies,
each producer and consumer are using resources (CPU, memory, maybe network?) each time it
retries and fails. If this is not a concern, then cool, but in scalable systems were always aiming
to optimize resource usage, and polling can be wasteful.
   A better solution is for producers and consumers to block until their desired operation, put
or get respectively, can succeed. Blocked threads consume no resources and hence provide an
efficient solution. To facilitate this, thread programming models provide blocking operations
that enable threads to signal to other threads when an event occurs. With the producer-
consumer problem, the basic scheme is as follows:

*	When a producer adds an item to the buffer, it sends a signal to any blocked consumers
to notify them that there is an item in the buffer
*	When a consumer retrieves an item from the buffer, it sends a signal to any blocked
producers to notify them there is capacity in the buffer for new items.

   In Java, the two basic primitives are wait() and notify(). Briefly, they work like this:
*	A thread may call wait() within a synchronized block if some condition it requires to
hold is not true. For example, a thread may attempt to retrieve a message from a buffer,
but if the buffer has no messages to retrieve, it calls wait() and blocks until another
thread adds a message,  sets the condition to true, and calls notify() on the same object.
*	notify() wakes up a thread that has called wait() on the object.

   These Java primitives are used to implement guarded blocks. Guarded blocks use a condition
as a guard that must hold before a thread resumes the execution. The code snippet below shows
how the guard condition, empty, is used to block a thread that is attempting to retrieve a message
from an empty buffer.
1.	while (empty) {
2.	  try {
3.	    System.out.println("Waiting for a message");
4.	    wait();
5.	  } catch (InterruptedException e) {}
6.	}
    When another thread adds a message to the buffer, it executes notify() as in the code
fragment below.
1.	// Store message.
2.	this.message = message;
3.	empty = false;
4.	// Notify consumer that message is available
5.	notify();
    The full implementation of this example is given in the code examples. There are a number
of variations of the wait() and notify() methods, but these go beyond the scope of what we can
cover in this overview. And luckily, Java provides us with abstractions that hide this complexity
from out code.
   An example that is pertinent to the producer-consumer problem is the BlockingQueue
interface in java.util.concurrent.BlockingQueue. A BlockingQueue implementation
provides a thread-safe object that can be used as the buffer in a producer-consumer scenario.
There are 5 different implementations of the BlockingQueue interface. Lets use one of these,
the LinkedBlockingQueue, to implement the producer-consumer. This is shown in Figure 23.
1.	class ProducerConsumer {
2.	   public static void main(String[] args)
3.	     BlockingQueue buffer = new LinkedBlockingQueue();
4.	     Producer p = new Producer(buffer);
5.	     Consumer c = new Consumer(buffer);
6.	     new Thread(p).start();
7.	     new Thread(c).start();
8.	   }
9.	 }
10.
11.	class Producer implements Runnable {
12.	   private boolean active = true;
13.	   private final BlockingQueue buffer;
14.	   public Producer(BlockingQueue q) { buffer = q; }
15.	   public void run() {
16.
17.	     try {
18.	       while (active) { buffer.put(produce()); }
19.	     } catch (InterruptedException ex) { // handle exception}
20.	   }
21.	   Object produce() { // details omitted, sets active=false }
22.	 }
23.
24.	 class Consumer implements Runnable {
25.	   private boolean active = true;
26.	   private final BlockingQueue buffer;
27.	   public Consumer(BlockingQueue q) { buffer = q; }
28.	   public void run() {
29.
30.	     try {
31.	       while (active) { consume(buffer.take()); }
32.	     } catch (InterruptedException ex) { // handle exception }
33.	   }
34.	   void consume(Object x) {  // details omitted, sets active=false }
35.	 }
36.
   Figure 23 Producer-Consumer with a LinkedBlockingQueue
   This solution absolves the programmer from having to be concerned with the
implementation of coordinating access to the shared buffer, and greatly simplifies the code.
   The java.util.concurrent  package is a treasure trove for building multithreaded Java
solutions. The following sections will briefly highlight a few more powerful and extremely useful
capabilities.
Thread Pools
   Many multithreaded systems need to create and manage a collection of threads that perform
similar tasks. For example, in the producer-consumer problem, we can have a collection of
producer threads and a collection of consumer threads, all simultaneously adding and removing
items, with coordinated access to the shared buffer.
   These collections are known as thread pools. Thread pools comprise several worker threads,
which typically perform a similar purpose and are managed as a collection. We could create a
pool of producer threads which wait for an item to process, write the final product to the
buffer, and then wait to accept another item to process. When we stop producing items, the
pool can be shutdown in a safe manner, so no partially processed items are lost through an
unanticipated exception.
   In java.util.concurrent, thread pools are supported by the ExecutorService interface.
This extends the base Executor interface with a set of methods to manage and terminate
threads in pool. A simple producer-consumer example using a fixed size thread pool  is shown
in Figure 24 and Figure 25. The Producer class in Figure 24 is a Runnable that sends a single
message to the buffer and then terminates. The Consumer simply takes messages from the
buffer until an empty string is received, upon which it terminates.

1.	class Producer implements Runnable {
2.
3.	  private final BlockingQueue buffer;
4.
5.	  public Producer(BlockingQueue q) { buffer = q; }
6.
7.	  @Override
8.	  public void run() {
9.
10.	  try {
11.	    sleep(1000);
12.	    buffer.put("hello world");
13.
14.	  } catch (InterruptedException ex) {
15.	    // handle exception
16.	  }
17.	 }
18.	}
19.
20.	class Consumer implements Runnable {
21.	  private final BlockingQueue buffer;
22.
23.	  public Consumer(BlockingQueue q) { buffer = q; }
24.
25.	  @Override
26.	   public void run() {
27.	      boolean active = true;
28.	      while (active) {
29.	          try {
30.	             String  s = (String) buffer.take();
31.	             System.out.println(s);
32.	             if (s.equals("")) active = false;
33.	          } catch (InterruptedException ex) {
34.	              / handle exception
35.	          }
36.	      } /
37.	      System.out.println("Consumer terminating");
38.	    }
39.	 }
40.
   Figure 24 Producer and Consumer for thread pool implementation
   In Figure 25, we create a single consumer to take messages from the buffer. We then create
fixed size thread pool of size 5 to manage our producers. This causes the JVM to pre-allocate
five threads that can be used to execute any Runnable objects that are executed by the pool.
   In the for() loop, we then use the ExecutorService to run 20 producers. As there are
only 5 threads available in the thread pool, only a maximum of 5 producers will be executed
simultaneously. All others are placed in wait queue which is managed by the thread pool. When
a producer terminates, the next Runnable in the wait queue is executed using any an available
thread in the pool.
   Once we have requested all the producers to be executed by the thread pool, we call the
shutdown() method on the pool. This tells the ExecutorService not to accept any more tasks
to run. We next call  the awaitTermination() method, which blocks the calling thread until
all the threads managed by the thread pool are idle and no more work is waiting in the wait
queue. Once awaitTermination() returns, we know all messages have been sent to the
buffer, and hence send an empty string to the buffer which will act as a termination value for
the consumer.
1.	public static void main(String[] args) throws
InterruptedException
2.	  {
3.	    BlockingQueue buffer = new LinkedBlockingQueue();
4.
5.	    //start a single consumer
6.	    (new Thread(new Consumer(buffer))).start();
7.
8.	    ExecutorService producerPool = Executors.newFixedThreadPool(5);
9.	    for (int i = 0; i < 20; i++)
10.	      {
11.	        Producer producer = new Producer(buffer) ;
12.	        System.out.println("Producer created" );
13.	        producerPool.execute(producer);
14.	      }
15.
16.	      producerPool.shutdown();
17.	      producerPool.awaitTermination(10, TimeUnit.SECONDS);
18.
19.	      //send termination message to consumer
20.	      buffer.put("");
21.	    }
   Figure 25 Thread pool-based Producer Consumer solution
   Like most topics in this chapter, theres many more sophisticated features in the Executor
framework that can be used to create multithreaded programs. This description has just covered
the basics. Thread pools are important as they enable our systems to rationalize the use of
resources for threads. Every thread consumes memory, for example the stack size for a thread is
typically around 1MB. Also, when we switch execution context to run a new thread, this
consumes CPU cycles. If our systems create threads in an undisciplined manner, we will
eventually run out of memory and the system will crash. Thread pools allow us to control the
number of threads we create, and utilize them efficiently.
   Well revisit thread pools throughout the remainder of this book, as they are a key concept
for efficient and scalable management of ever the increasing request loads that servers must
satisfy.

Barrier Synchronization
   I had a high school friend whose family, at dinner times, would not allow anyone to start
eating until the whole family was seated at the table. I thought this was weird, but many years
later it serves as a good analogy for the concept known as barrier synchronization. Eating
commenced only after all family members arrived at the table.
   Multithreaded systems often need to follow such a pattern of behavior. Imagine a
multithreaded image processing system. An image arrives and a non-overlapping segment of the
image is passed to each thread to perform some transformation upon  think Instagram filters
on sterioids. The image is only fully processed when all threads have completed. In software
systems, we use a mechanism called barrier synchronization to achieve this style of thread
coordination.
   The general scheme is shown in Figure 26. In this example, the main() thread creates four
new threads and all proceed independently until they reach the point of execution defined by
the barrier. As each thread arrives, it blocks. When all threads have arrived at this point, the
barrier is released, and each thread can continue with its processing.

   Figure 26 Barrier Synchronization
   Java providers three primitives for barrier synchronization. Lets see how one of the three,
namely the CountDownLatch, works.
   When you create a CountDownLatch, you pass a value to its constructor that represents the
number of threads that must bock at the barrier before they are all allowed to continue. This is
called in the thread which is managing the barrier points for the system  in Figure 26this would
be main().

   CountDownLatch  nextPhaseSignal = new CountDownLatch(numThreads);

   Next we create the worker threads that will perform some actions and then block at the
barrier until they all complete. To do this, we need to pass each thread a reference to
CountDownLatch.

   for (int i = 0; i < numThreads; i++) {
               Thread worker = new Thread(new
WorkerThread(nextPhaseSignal));
               worker.start();
           }

   After launching the worker threads, the main() thread will call the .await() method to
block until the latch is triggered by the worker threads.

   nextPhaseSignal.await();

   Each worker thread will complete its task and before exiting call the .countDown() method
on the latch. This decrements the latch value. When the last thread calls .countDown() and the
latch value becomes zero, all threads that have called .await() on the latch transition from the
blocked to the runnable state. In our example this would be the main()  thread. At this stage we
are assured that all workers have completed their assigned task.

   nextPhaseSignal.countDown();

   Any subsequent calls to .countDown() will return immediately as the latch has been
effectively triggered. Note .countDown() is non-blocking, which is a useful property for
applications in which threads have more work to do after reaching the barrier.
   Our example illustrates using a CountDownLatch to block a single thread until a collection
of threads have completed their work. We can invert this use case with a latch however if we
initialize its value to one. Multiple threads could call .await() and block until another thread
calls .countDown()  to release all waiting threads. This example is analogous to a simple gate,
which one thread opens to allow a collection of others to continue.
   CountDownLatch is a simple barrier synchronizer. Its a single use tool, as the initializer
value cannot be reset. More sophisticated features are provided by the CyclicBarrier and
Phaser classes in Java and hopefully armed with the knowledge of how barrier synchronization
works from this section, these will be straightforward to understand.
Thread-Safe Collections
   Many programmers, once they delve into the wonders of multithreaded programs, are
surprised to discover that the collections in the java.util package  are not thread safe. Why, I
hear you ask? The answer, luckily, is simple. Its to do with performance. Calling synchronized
methods incurs overheads. Hence to attain faster execution for single threaded programs, the
collections are not thread-safe.
   If you want to share an ArrayList, Map or your favorite data structure from java.util
across multiple threads, you must ensure modifications to the structure are placed in critical
sections. This approach places the burden on the client of the collection to safely make updates,
and hence is error prone  a programmer might forget to make modifications in a
synchronized block.
   Its always safer to use inherently thread-safe collections in out multithreaded code. For this
reason, the Java collections framework provides a factory method that creates a thread-safe
version of java.util collections. Heres an example of creating a thread-safe list.

   List<String> list = Collections.synchronizedList(new ArrayList<>());

   Whats really happening here is that we are creating a wrapper around the base collection
class, which has synchronized methods. These delegate the actual work to the original class, in
a thread-safe manner of course. You can use this approach for any collection in the java.util
package, and the general form is:

   Collections.synchronized.(collection)

   where . is List, Map, Set, and so on.
   Of course, when using the synchronized wrappers, you pay the performance penalty for
acquiring the monitor lock and serializing access from multiple threads. This means the whole
collection is locked while a single thread makes a modification, greatly limiting concurrent
performance (remember Amdahls Law?). For this reason, Java 5.0 included the concurrent
collections package, namely java.util.concurrent. It contains a rich collection of classes
specifically designed for efficient multithreaded access.
   In fact weve already seen one of these classes  the LinkedBlockingQueue. This uses a
locking mechanism that enables items to be added to and removed from the queue in parallel.
This finer grain locking mechanism utilizes the java.util.concurrent.lock.Lock class
rather than the monitor lock approach. This allows multiple locks to be utilized on the same
collection, hence enabling safe concurrent access.
   Another extremely useful collection that provides this finer-grain locking is the
ConcurrentHashMap. This provides the same methods as the non-thread safe Hashtable, but
allows non-blocking reads and concurrent writes based on a concurrencyLevel value you can
pass to the constructor (the default value is 16).

   ConcurrentHashMap (int initialCapacity, float loadFactor,
                        int concurrencyLevel)

   Internally, the hash table is divided into individually lockable segment, often known as
shards. This means updates can be made concurrently to hash table entries in different shards of
the collection, increasing performance.
   There are a number of subtle issues concerning iterators and collections, but these are
beyond the scope of this chapter. Well however investigate some of these in the exercises at the
end of the chapter.
Summary and Further Reading
   This chapter has only brushed the surface of concurrency in general and its support in Java.
The best book to continue learning more about the basic concepts of concurrency in is the
classic Java Concurrency in Practice by Brian Goetz et al. If you understand everything in this
book, youll be writing pretty great concurrent code.
   Java concurrency support has moved on considerably however since Java 5.  In the world of
Java 12 (or whatever version is current when you read this), there are new features such as
CompleteableFutures, lambda expressions and parallel streams. The functional programming
style introduced in Java 8.0 makes it easy to create concurrent solutions without directly creating
and managing threads. A good source of knowledge for Java 8.0 features is Mastering
Concurrency Programming with Java 8 by Javier Fernndez Gonzlez.

   Other excellent sources include:
*	Doug Lea, Concurrent Programming in Java: Design Principles and Patterns, 2nd
Edition
*	Raoul-Gabriel Urma, Mario Fusco, and Alan Mycroft, Java 8 in Action: Lambdas,
Streams, and functional-style programming, manning Publications, 1st Edition, 2014.

Exercises

   volatile
   thread safe v non thread safe collection  performance, including CHP sharding
   copyonwrite example  immutable, slow

   collections, iterators

CHAPTER 4
__________________________

Distributed Systems Fundamentals

   Scaling a system naturally involves adding multiple independently moving parts. We run our
software components on multiple machines and our databases across multiple storage nodes, all
in the quest of adding more capacity. Consequently, our solutions are distributed across multiple
machines in multiple locations, with each machine processing events concurrently, and
exchanging messages over a network.
   This fundamental nature of distributed systems has some profound implications on the way
we design, build and operate our solutions. This chapter provides the basic nature of the beast
information you need to know to appreciate the issues and complexities of distributed software
systems. We briefly cover communications networks hardware and software, how to deal with
the implications of communications failures, distributed coordination, and the complex issue of
time in distributed systems.
Communications Basics
   Every distributed system has software components that communicate over a network. If a
mobile banking app requests the users current bank account balance, a (simplified) sequence of
communications occurs along the lines of:

1.	The cell phone app sends a request over the cellular network addressed to the bank to
retrieve the users bank balance
2.	The request is routed across the internet to where the banks web servers are located.
3.	The banks web server authenticates the request (checks its really the supposed user)
and sends a request to a database server for the account balance.
4.	The database server reads the account balance from disk and returns it to the web server
5.	The web server sends the balance in a reply message addressed to the app, which is
routed over the internet and the cellular network until the balance magically appears on
the screen of the mobile device

   It almost sounds simple when you read the above, but in reality, theres a huge amount of
complexity hidden beneath this sequence of communications. Lets examine some of these in
the following sections.
  Communications Hardware
   The bank balance request will inevitably traverse multiple different networking technologies
and devices. The global internet is a heterogeneous machine, comprising different types of
network communications channels and devices that shuttle many millions of messages a second
across networks to their intended destinations.
   Different types of communications channels exist. The most obvious categorization is wired
versus wireless. For each category there are multiple network transmission hardware
technologies that can ship bits from one machine to another. Each technology has different
characteristics, and the ones we typically care about are speed and range.
   For physically wired networks, the two most common types are local area networks (LANs)
and wide area networks (WANs). LANs are networks that can connect devices at building
scale, being able to transmit data over a small number (e.g. 1-2) of kilometers. Contemporary
LANs can transport between 100 megabits per second (Mbps) to 1 gigabits per second (Gbps).
This is known as the networks bandwidth, or capacity. The time taken to transmit a message
across a LAN  the networks latency  is sub-millisecond with modern LAN technologies.
   WANs are networks that traverse the globe and make up what we collectively call the
internet. These long-distance connections are the high speed data pipelines connecting cites
and countries and continents with fiber optic cables. These cables support a networking
technology known as wavelength division multiplexing  which makes it possible to transmit up
171 Gbps over 400 different channels, giving more than 70 Terabits per second (Tbps) of total
bandwidth for a single fiber link. The fiber cables that span the world normally comprise four or
more strands of fiber, giving bandwidth capacity of hundreds of Tbps for each cable.
   Latency is more complicated with WANs however. WANs transmit data over 100s to 1000s
of kilometers, and the maximum speed that the data can travel is the theoretical speed of light.
In reality, fiber optic cables cant reach the speed of light, but do get pretty close to it as we can
see in Table 1.

Path
Distance
Time -
Speed of
Light
Time - Fiber
Optic Cable
New York to
San Francisco
4,148 km
14 ms
21 ms
New York to
London
5,585 km
19 ms
28 ms
New York to
Sydney
15,993 km
53 ms
80 ms
   Table 1 WAN Speeds
   Actual times will be slower than this as the data needs to pass through networking
equipment known as routers . Routers are responsible for transmitting data on the physical
network connections to ensure data is transmitted across the internet from source to
destination. Routers are specialized, high speed devices that can handle several hundred Gbps of
network traffic, pulling data off incoming connections and sending the data out to different
outgoing network connections based on their destination. Routers at the core of the internet
comprise racks of these devices and hence can process 10s to hundreds of Tbps. This is how
you and 1000s of your friends get to watch a steady video stream on Netflix.
   Wireless technologies have different range and bandwidth characteristics. Wi-Fi routers that
we are all familiar with in our homes and offices are wireless ethernet networks and use 802.11
protocols to send and receive data. The most widely used Wi-Fi protocol, 802.11ac, allows for
maximum (theoretical) data rates of up to 5,400Mbps. The most recent 802.11ax protocol, also
known as Wi-Fi 6, is an evolution of 802.11ac technology that promises increased throughput
speeds of up to 9.6Gbps. The range of Wi-Fi routers is of the order of 10s of meters, and of
course is affected by physical impediments like walls and floors.
   Cellular wireless technology uses radio waves to send data from our phones to routers
mounted on cell towers, which are generally connected by wires to the core internet for message
routing. Each cellular technology introduces improved bandwidth and other dimensions of
performance. The most common technology at the time of writing is 4G LTE wireless
broadband. 4G LTE is around 10 times faster than the older 3G, able to handle sustained
download speeds around 10 Mbps (peak download speeds are nearer 50 Mbps) and upload
speeds between 2 and 5 Mbps.
   Emerging 5G cellular networks promise 10x bandwidth improvements over existing 4G,
with 1-2 millisecond latencies between devices and cell towers. This is a great improvement over
4G latencies which are in the 20-40 millisecond range. The trade-off is range. 5G base station
range operates at about 500m maximum, whereas 4G provides reliable reception at distances of
10-15kms.
   This whole collection of different hardware types for networking comes together in the
global internet. The internet is heterogeneous network, with many different operators globally
and every type of hardware imaginable. Figure 27 (from Wikipedia ) shows a simplified view of
the major components that comprise the internet. Tier 1 networks are the global high-speed
internet backbone. There are around 20 Tier 1 Internet Service Providers (ISPs) who manage
and control global traffic. Tier 2 ISPs are typically regional (e.g. one country), have lower
bandwidth than Tier 1 ISPs, and deliver content to customers through Tier 3 ISPs. Tier 3 ISPs
are the ones that charge your exorbitant fees for your home internet every month.

   Figure 27 Simplified view of the Internet (from Wikipedia)
   Theres a lot more complexity to how the internet works than described here. That
complexity is beyond the scope of this chapter. From a distributed systems software
perspective, we need to understand more about the magic that enables all this hardware to
route messages from say my cell phone, to my bank and back. This is where the IP protocol
comes in.

  Communications Software
   Software systems on the internet communicate using the Internet Protocol suite. The
Internet Protocol suite specifies host addressing, data transmission formats, message routing
and delivery characteristics. There are four abstract layers, which contain related protocols that
support the functionality required at that layer. These are, from lowest to highest:

1)	the data link layer, specifying communication methods for data across a single network
segment. This is implemented by the device drivers and network cards that live inside
your devices.
2)	the internet layer specifies addressing and routing protocols that make it possible for
traffic to traverse the independently managed and controlled networks that comprise the
internet. This is the IP protocol in the internet protocol suite.
3)	the transport layer, specifying protocols for reliable and best-effort host-to-host
communications. This is where the well-known TCP and UDP protocols live.
4)	the application layer, which comprises several application level protocols such as HTTP
and SCP.

   Each of the higher layer protocols builds on the features of the lower layers. The enables the
IP suite to support end to end data communications across the internet. In the following, well
briefly cover the IP protocol for host discovery and message routing, and the TCP and UDP
transport protocols that can be utilized by distributed applications.
   Internet Protocol (IP)
   IP defines how hosts are assigned addresses on the internet and how messages are
transmitted between two hosts who know each others addresses.
   Every device on the internet has its own address. These are known as Internet Protocol (IP)
addresses. The location of an IP address can be found using an internet wide directory service
known as Domain Naming Service (DNS). DNS is a widely distributed, hierarchical database
that acts as the address book of the internet.
   The technology currently used to assign IP addresses, known as Internet Protocol version 4
(IPv4), will eventually be replaced by its successor, IPv6. IPv4 is a 32-bit addressing scheme that
before long will run out of addresses due to the number of devices connecting to the internet.
IPv6 is a 128-bit scheme that will offer an (almost) infinite number of IP addresses. As an
indicator, in July 2020 about 33% of the traffic processed by Google.com  is IPv6.
   DNS servers are organized hierarchically. A small number of root DNS servers, which are
highly replicated, are the starting point for resolving and an IP address. When an internet
browser tries to find a web site, a network host known as the local DNS server that is managed
by your employer or ISP, will contact a root server with the requested host name. The root
server replies with a referral to a so-called authoritative DNS server that manages name resolution
for, in our banking example, .com addresses. There is an authoritative name server for each top-
level internet domain (e.g. .com, .org, .net, etc).
   Next the local DNS server will query the .com DNS server, which will reply with the address
of the DNS server which knows about all the IP addresses managed by mybank.com. This DNS
is queried, and it returns the actual IP address we need to communicate with the application.
The overall scheme is illustrated in Figure 28.

   Figure 28 Example DNS Lookup for mybank.com
   The whole DNS database is highly geographically replicated so there are no single points of
failure, and requests are spread across multiple physical servers. Local DNS servers also
remember the IP addresses of recently contacted hosts, which is possible as IP addresses dont
change very often. This means the complete name resolution process doesnt occur for every
site we contact.
   Armed with a destination IP address, a host can start sending data across the network as a
series of IP packets. IP has the task of delivering data from the source to the destination host
based on the IP addresses in the packet headers. IP defines a packet structure that contains the
data to be delivered, along with header data including source and destination IP addresses. Data
sent by an application is broken up into a series of packets which are independently transmitted
across the Internet.
   IP is known as a best-effort delivery protocol. This means it does not attempt to compensate
for the various error conditions that can occur during packet transmission. Possible
transmission errors include data corruption, packet loss and duplication. In addition, as every
packet is routed from source to destination independently, different packets may be delivered to
the same destination via different network paths, resulting in out-of-order delivery to the
receiver. Treating every packet independently is known as packet-switching. This allows the
network to dynamically respond to conditions such as link failure and congestion, and hence is a
defining characteristic of the internet.
   Because of this design, the IP is unreliable. If two hosts require reliable data transmission,
they need to add additional features to make this occur. This is where the next layer in the IP
protocol suite, the transport layer, enters the scene.

   Transmission Control Protocol (TCP)
   Once an application or browser has discovered the IP address of the server it wishes to
communicate with, it can send messages using the transport protocol API. This is achieved
using Transmission Control Protocol (TCP) or User Datagram Protocol (UDP), which are the
established standard transport protocols for the IP network stack.
   Distributed applications can choose which of these protocols to use. APIs for both TCP and
UDP are widely available in mainstream programming languages such as Java, Python and C++.
In reality, use of these APIs is not common as higher-level programming abstractions hide the
details from most applications. In fact, the IP protocol suite application layer contains several of
these application level APIs, including HTTP, which is very widely used in mainstream
distributed systems. Still, its important to understand TCP, UDP and their differences.
   Most requests on the internet are sent using TCP. TCP is:

*	connection-oriented
*	stream-oriented
*	reliable

   TCP is a connection-oriented protocol. Before any messages are exchanged between
applications, TCP uses a 3-step handshake to establish a two-way connection between the client
and server applications. The connection stays open until the TCP client calls close to terminate
the connection with the TCP server. The server responds by acknowledging the close request
before the connection is dropped.
   Once a connection is established, a client sends a sequence of requests to the server as a data
stream. When a data stream is sent over TCP, it is broken up into individual network packets,
with a maximum packet size of 65535 bytes. Each packet contains a source and destination
address, which is used by the underlying IP protocol to route the messages across the network.
   The internet is a packet-switched network, which means every packet is individually routed
across the network. The route each packet traverses can vary dynamically based on the
conditions in the network, such as link congestion or failure. This means the packets may not
arrive at the server in the same order they are sent from the client. To solve this problem, a TCP
sender includes a sequence number in each packet so the receiver can reassemble packets into a
stream that is identical to the order they were sent.
   Reliability is needed as network packets can be lost or delayed during transmission between
sender and receiver. To achieve reliable packet delivery, TCP uses a cumulative
acknowledgement mechanism. This means a receiver will periodically send an acknowledgement
packet that contains the highest sequence number of the received packets. This implicitly
acknowledges all packets sent with a lower sequence number, meaning all have been successfully
received. If a sender doesnt receive an acknowledgement within a timeout period, the packet is
resent.
   TCP has many other features, such as checksums to check packet integrity, and dynamic
flow control to ensure a sender doesnt overwhelm a slow receiver by sending data too quickly.
Along with connection establishment and acknowledgments, this makes TCP a relatively
heavyweight protocol, which trades off reliable over efficiency.
   This is where UDP comes into the picture. UDP is a simple connectionless protocol, which
exposes the user's program to any unreliability of the underlying network. There is no guarantee
of in order delivery, or even delivery for that matter. It can be thought of as a thin veneer (layer)
on top of the underlying IP protocol, and deliberately trades off raw performance over
reliability. This however is highly appropriate for many modern applications where the odd lost
packet has very little effect. Think streaming movies, video conferencing and gaming, where one
lost packet is unlikely to be perceptible by a user.

   Figure 29 Comparing TCP and UDP
   Figure 29 depicts some of the major differences between TCP and UDP. TCP incorporates
a connection establishment 3-packet handshake, and piggybacks acknowledgements (ACK) of
packets so that any packet loss can be handled by the protocol. Theres also a TCP connection
close phase involving a 4-way handshake that is not shown in the diagram. UDP dispenses with
connection establishment, tear down, acknowledgements and retries. Hence applications using
UDP need to be tolerant of packet loss and client or server failures and behave accordingly.
Remote Method Invocation
   Its perfectly feasible to write our distributed applications using APIs that interact directly
with transport layer protocols like TCP. If we use for example the TCP sockets library, we can
create a connection, known as a socket, between a client and a server and exchange data over
that connection.
   A socket is basically a pipe between the client and server. Once the socket is created, the
client sends data to the server in a stream. In our bank example, the client might request a
balance for the users checking account. Ignoring specific language issues (and security!!), the
client might send a message payload as follows over a socket to the server:

   {balance, 000169990}

   In this message, balance represents the operation we want the server to execute, and
000169990 is the bank account number.
   In the server, we need to know that the first string in the message is the operation identifier,
and based on this value being balance, the second is the bank account number. The server
then uses these values to presumably query a database, retrieve the balance and send back the
results, perhaps as a message formatted with the account number and current balance, as below:

   {000169990, 220.77}

   In any complex system, the server will support many operations. In mybank.com, we might
have for example login, transfer, address, statement, transactions, and so on. Each
will be followed by different message payloads that the server needs to interpret correctly to
fulfill the clients request.
   What we are defining here is an application specific protocol. As long as we send the necessary
values in the correct order for each operation, the server will be able to respond correctly. If we
have an erroneous client that doesnt adhere to our application protocol, well, our server needs
to do thorough error checking.
   Stepping back, if we were defining the mybank.com server in an object-oriented language
such as Java, we would have each operation it can process as a method, which is passed an
appropriate parameter list for that operation, as shown in Figure 30.
1.	// Simple mybank.com server interface
2.	public interface MyBank {
3.	    public float balance  (String accNo);
4.	    public boolean  statement(String month) ;
5.	    // other operations
6.	 }
   Figure 30 Simple mybank.com server interface
   There are several advantages of having such an interface, namely:
*	Calls from the client to the server can be statically checked by the compiler to ensure
they are of the correct type
*	Changes in the server interface (e.g. add a new parameter) force changes in the client
code to adhere to the new method signature
*	The interface is clearly defined by the class definition, and hence straightforward for a
client programmer to utilize

   These benefits of an explicit interface are of course well known in sequential programming.
The whole discipline of object-oriented design is pretty much based upon these foundations,
where an interface defines a contract between the caller and callee. Compared to the implicit,
application protocol we need to program to with sockets, the advantages are significant.
   This fact was recognized reasonably early in the evolution of distributed systems. Since the
early 1990s, we have seen an evolution of technologies that enable us to define explicit server
interfaces and call these across the network using essentially the same syntax as we would in a
sequential program. A summary of the major approaches is given in Table 2. Collectively they
are known as Remote Procedure Call (RPC) or Remote Method Invocation (RMI) technologies.

Technology
Dates
Main features
Distributed Computing
Environment (DCE)
Early
1990s
DCE RPC provides a standardized approach for
client-server systems. Primary languages were
C/C++.
Common Object Request
Broker Architecture
(CORBA)
Early
1990s
Facilitates language-neutral client-server
communications based on an object-oriented
Interface Definition Language (IDL). Primary
language support in C/C++, Java, Python, Ada.
Java Remote Method
Invocation (RMI)
Late
1990s
A pure Java-based remote method invocation
that facilitates distributed client-server systems
with the same semantics as Java objects.
XML Web Services
2000
Supports client-server communications based on
HTTP and XML. Servers define their remote
interface in the Web Services Description
Language (WSDL)
   Table 2 Summary of major RPC/RMI Technologies
   While the syntax and semantics of these RPC/RMI technologies vary, the essence of how
each operates is the same. Lets continue with our Java example of mybank.com to use this as
an example of the whole classification.
   Using Java RMI, we can trivially make our MyBank interface in Figure 30 a remote interface,
as shown in Figure 31.
1.	import java.rmi.*;
2.	// Simple mybank.com server interface
3.	public interface MyBank extends Remote{
4.	    public float balance  (String accNo)
5.	         throws RemoteException;
6.	    public boolean  statement(String month)
7.	         throws RemoteException ;
8.	    // other operations
9.	 }
   Figure 31 Java RMI Example
   The empty java.rmi.Remote interface serves as a marker to inform the Java compiler we
are creating an RMI server. In addition, each method must throw
java.rmi.RemoteException. These exceptions represent errors that can occur when a
distributed call between two objects is invoked over a network. The most common reasons for
such an exception would be a communications failure or the server object having crashed.
   We then must provide a class that implements this remote interface. Figure 32 shows an
extract of the server implementation - the complete code for this example is in the Chapter 4
code repository. Points to note are:
*	The server extends the UnicastRemoteObject class. This essentially provides the
functionality to instantiate remotely callable object.
*	Once the server object is constructed, its availability must be advertised to remote
clients. This is achieved by storing a reference to the object in the RMI Registry, and
associating a logical name with it  in this example, MyBankServer. The registry is a
simple directory service that enables clients to look up the location (network address and
object reference) of and obtain a reference to an RMI server.
1.	public class MyBankServer extends UnicastRemoteObject
2.	                          implements MyBank  {
3.	            // constructor/method implementations omitted
4.	   public static void main(String args[]){
5.	        try{
6.	          MyBankServer server=new MyBankServer();
7.	          // create a registry in local JVM on default port
8.	          Registry registry = LocateRegistry.createRegistry(1099);
9.	          registry.bind("MyBankServer", server);
10.	          System.out.println("server ready");
11.	        }catch(Exception e){
12.	                 // code omitted for brevity}
13.	        }
14.	   }
    Figure 32 Implementing an RMI Server
   An extract from the client code to connect to the server is shown in Figure 33. It obtains a
reference to the remote object by performing a lookup operation (line 3) in the RMI Registry
and specifying the logical name that identifies the server. The reference returned by the lookup
operation can then be used to call the server object in the same manner a local object. However
there is a difference  the client must be ready to catch a RemoteException that will be thrown
by the Java runtime when the server object cannot be reached.
1.	 // obtain a remote reference to the server
2.	 MyBank bankServer=
3.	        (MyBank)Naming.lookup("rmi://localhost:1099/MyBankServer");
4.	 //now we can call the server
5.	 System.out.println(bankServer.balance("00169990"));
    Figure 33 Implementing an RMI Client
   Figure 34 depicts the call sequence amongst the components that comprise a RMI system.
The Stub and Skeleton are compiler-generated objects that facilitate the remote communications.
The skeleton is a TCP network endpoint (host, port) that listens for calls to the associated
server. The sequence of operations is as follows:

1.	When the server reference is stored in the RMI Registry, the entry contains the
client stub that can be used to make remote calls to the server.
2.	The client queries the registry, and the stub for the server is returned.
3.	The client stub accepts a method call to the server interface from the client
4.	The stub transforms the request into a network request to the server host. This
transformation process is known as marshalling.
5.	The skeleton accepts network requests from the client, and unmarshalls the network
packet data into a valid call to the server  object implementation
6.	The skeleton waits for the method to return a result
7.	The skeleton marshalls the method results into a network reply packet that is sent
the client
8.	The stub unmarshalls the data passes the result to the client call site

   Figure 34 Schematic depicting the call sequence for establishing a connection and making a call to a RMI
server object
   This Java RMI example illustrates the basics that are used for implementing any RPC/RMI
mechanism, even in modern languages like Erlang  and Go . Regardless of implementation,
the basic attraction of these approaches to is to provide an abstract calling mechanism that
provides  location transparency for clients making remote server calls.
   RPC/RMI is not without its flaws. Marshalling and unmarshalling can become inefficient for
complex object parameters. Cross language marshalling  client in one language, server in
another  can cause problems due to types being represented differently in different languages,
causing subtle incompatibilities. And if a remote method signature changes, all clients need to
obtain a new compatible stub which can be cumbersome in large deployments.
   For these reasons, most modern systems are built around simpler protocols based on HTTP
and using JSON for parameter representation. Instead of operation names, HTTP verbs (PUT,
GET, POST, etc) have associated semantics that are mapped to a specific URL. This approach
originated in the work by Roy Fielding on the REST approach . REST has a set of semantics
that comprise a RESTful architecture style, and in reality, most systems do not adhere to these.
Well discuss REST and HTTP API mechanisms in the Chapter 5.
Partial Failures
   The components of distributed systems communicate over a network. In communications
technology terminology, the shared local and wide area networks that our systems communicate
over are known as asynchronous networks. With asynchronous networks:

*	nodes can choose to send data to other nodes at any time
*	The network is half-duplex, meaning that one node sends a request and must wait for a
response from the other. These are two separate communications.
*	The time for data to be communicated between nodes is variable, due to reasons like
network congestion, dynamic packet routing and transient network connection failures
*	The receiving node may not be available due to a software or machine crash
*	Data can be lost. In wireless networks, packets can be corrupted and hence dropped due
to weak signals or interference. Internet routers can drop packets during congestion .
*	Nodes do not have identical internal clocks, hence they are not synchronized

   What does this mean for our applications? Well, put simply, when a client sends a request to
server, how long does it wait until it receives a reply? Is the server node just being slow? Is the
network congested and the packet has been dropped? If the client doesnt get a reply, what
should it do?
   Lets explore these scenarios in detail. The core problem is known as handling partial
failures, and the general situation is depicted in Figure 35.

   Figure 35 Handling Partial Failures
   When a client node sends a network request to a server node and expects a response, the
following outcomes may occur:

1.	The request succeeds and a rapid response is received. All is good.
2.	The destination IP address lookup may fail. In this case the client rapidly receives a
error message.
3.	The IP address is valid but the destination node or target server process has failed.
Again the sender will receive an error message.
4.	The request is received by the target server, which fails while processing the request and
no response is ever sent.
5.	The request is received by the target server, which is heavily loaded. It processes the
request but takes a long time (e.g 24 seconds!) to respond
6.	The request is received by the target server and a response is sent. However, the
response is not received by the client due to a network failure.

   Numbers (1) to (3) are easy for the client to handle, as a response is received rapidly. A result
from the server or an error message  either allows the client to proceed. Failures that can be
detected quickly are easy to deal with.
   Numbers (4) to (6) pose a problem for the client. It has no insight into the reason why a
response has not been received. From the clients perspective, these three outcomes look the
same. It cannot know, without waiting forever, whether the response will arrive eventually, or
never arrive. And waiting forever doesnt get much work done. More insidiously, the client
cannot know if the operation succeeded and server or network failure caused the result to never
arrive, or if the request is on its way - delayed simply due to congestion in the network/server.
These faults are collectively known as crash faults .
   The typical solution that clients adopt to handle crash faults is to resend the request after a
configured timeout period. This however is fraught with danger, as Figure 36 illustrates.  The
client sends a request to the server to deposit money in a bank account. When it receives no
response after a timeout period, it resends the request. What is the resulting balance? The server
may have applied the deposit, and it may not, depending on the partial failure scenario.

   Figure 36 Client retries a request after timeout
   The chance that the deposit may occur twice is a fine outcome for the customer. The bank
though is unlikely to be amused by this possibility. Hence we need a way to ensure in our server
operations that retried, duplicate requests from clients only result in the request being applied
once. This is necessary to maintain correct application semantics. This property is known as
idempotence. Idempotent operations can be applied multiple times without changing the result
beyond the initial application. This means that for the example in Figure 36, the client can retry
the request as many times as it likes, and the account will only be increased by $100.
   Requests that make no persistent state changes are naturally idempotent. This means all read
requests are inherently safe and no extra work is needed in the server. Updates are a different
matter. The system needs to devise a mechanism such that duplicate client requests can be
detected by the server, and they do not cause any state changes. In API terms, these endpoints
cause mutation of the server state and must be idempotent.
   The general approach to building idempotent operations is as follows:
*	Clients include a unique idempotence-key in all requests that mutate state. The key identifies
a single operation from the specific client or event source. It is usually a composite of a
user identifier, such as the session key, and a unique value such as a local timestamp,
UUID or a sequence number.
*	When the server receives a request, it checks to see if it has previously seen the
idempotence key value by reading from a database that is uniquely designed for
implementing idempotence. If the key is not in the database, this is a new request. The
server therefore performs the business logic to update the application state. It also stores
the idempotence key in a database to indicate that the operation has been successfully
applied.
*	If the idempotence key is in the database, this indicates that this request is a retry from
the client and hence should not be processed. In this case the server returns a valid
response for the operation so that (hopefully) the client wont retry again.

   The database used to store idempotence keys can be implemented in, for example:
*	a separate database table or collection in the transactional database used for the
application data
*	a dedicated database that provides very low latency lookups, such as a simple key-value
store

   Unlike application data, idempotence keys dont have to be retained forever. Once a client
receives an acknowledgement of a success for an individual operation, the idempotence key can
be discarded. The simplest way to achieve this is to automatically remove idempotence keys
from the store after a specific time period, such as 60 minutes or 24 hours, depending on
application needs and request volumes.
   In addition, an idempotent API implementation must ensure that the application state is
modified, and the idempotence key is stored. Both must occur for success. If the application
state is modified and, due to some failure, the idempotent key is not stored, then a retry will
cause the operation to be applied twice. If the idempotence key is stored but for some reason
the application state is not modified, then the operation has not been applied. If a retry arrives,
it will be filtered out as duplicate as the idempotence key already exists, and the update will be
lost.
   The implication here is that the updates to the application state and idempotence key store
must both occur, or neither must occur. If you know your databases, youll recognize this as a
requirement for transactional semantics. (Well discuss how distributed transaction are achieved
in Chapter XXX) This will ensure exactly-once semantics, which guarantees that all messages will
always be processed exactly once  what we need for idempotence.
   Exactly once does not mean that there are no message transmission failures, retries and no
application crashes. These are all inevitable. The important thing is that the retries eventually
succeed and the result is the always the same.
   Well return to the issue of communications delivery guarantees in later chapters. As Figure
37 illustrates, theres a spectrum of semantics, each with different guarantees and performance
characteristics. At most once delivery is fast and unreliable  this is what the UDP protocol
provides. At least once delivery is the guarantee provided by TCP/IP, meaning duplicates are
inevitable. Exactly-once delivery, as weve discussed here, requires guarding against duplicates and
hence trades off reliability against slower performance.
   As well see, some advanced communications mechanisms can provide our applications with
exactly once semantics. However, these dont operate at Internet scale because of the
performance implications. That is why, as our applications are built on the at least once
semantics of TCP/IP, we must implement exactly once semantics in our APIs that cause state
mutation.

   Figure 37 Communications Delivery Guarantees
Consensus in Distributed Systems
   Crash faults have another implication for the way we build distributed systems. This is best
illustrated by the Two Generals Problem , which is illustrated in Figure 38.
   Imagine a city under siege by two armies. The armies lie on opposite sides of the city, and
the terrain surrounding the city is difficult to travel through and visible to snipers in the city. In
order to overwhelm the city, its crucial that both armies attack at the same time. This will
stretch the citys defenses and make victory more likely for the attackers. If only one army
attacks, then they will likely be repelled. How can the two generals reach agreement on the exact
time to attack, such that both generals know for certain that agreement has been reached? They
both need certainty that the other army will attack at the agreed time.
   To coordinate an attack, the first general sends a messenger to the other, with instructions to
attack at a specific time. As the messenger may be captured or killed by snipers, the sending
general cannot be certain the message has arrived unless they get an acknowledgement
messenger from the second general. Of course, the acknowledgement messenger may be
captured or killed, so even if the original messenger does get through, the first general may
never know. And even if the acknowledgement message arrives, how does the second general
know this, unless they get an acknowledgement from the first general?
   Hopefully the problem is apparent. With messengers being randomly captured or
extinguished, there is no guarantee the two generals will ever reach consensus on the attack
time. In fact, it can be proven that it is not possible to guarantee agreement will be reached. There
are solutions that increase the likelihood of reaching consensus. For example, Game of Thrones
style, each general may send 100 different messengers every time, thus increasing the probability
that at least one will make the perilous journey to the other friendly army and successfully
deliver the message.

   Figure 38 The Two Generals Problem
   The Two Generals problem is analogous to two nodes in a distributed system wishing to
reach agreement on some state, such as the value of a data item that can be updated at either.
Partial failures are analogous to losing messages and acknowledgements. Messages may be lost
or delayed for an indeterminate period of time.
   In fact it can be demonstrated that consensus on an asynchronous network in the presence
of crash faults, where messages can be delayed but not lost, is impossible to achieve within
bounded time. This is known as the FLP Impossibility Theorem .
   Luckily, this is only theoretical limitation, demonstrating its not possible to guarantee
consensus will be reached with unbounded message delays on an asynchronous network. In
reality, distributed systems reach consensus all the time. This is possible because while our
networks are asynchronous, we can establish sensible practical bounds on message delays and
retry after a timeout period. FLP is therefore a worst-case scenario, and as well discuss
algorithms for establishing consensus when we discuss distributed databases.
   Finally, we should note the issue of Byzantine failures. Imagine extending the Two Generals
problem to N Generals, who need to agree on a time to attack. However, in this scenario,
traitorous messengers may change the value of the time of the attack, or a traitorous general
may sense false information to other generals. This class of malicious failures are known as
Byzantine faults and are particularly sinister in distributed systems. Luckily, the systems we
discuss in this book typically live behind well-protected, secure enterprise networks and
administrative environments. This means we can practically exclude handling Byzantine faults.
Algorithms that do address malicious behavior exist, and if you are interested in a practical
example, take a look at Blockchain technologies  and BitCoin .
Time in Distributed Systems
   Every node in a distributed system has its own internal clock. If all the clocks on every
machine were perfectly synchronized, we could always simply compare the timestamps on
events across nodes to determine the precise order they occurred in. If this were reality, many of
the problems well discuss with distributed systems would pretty much go away.
   Unfortunately, this is not the case. Clocks on individual nodes drift due to environmental
conditions like changes in temperature or voltage. The amount of drift varies on every machine,
but values like 10-20 seconds a day are not uncommon. Or with my current coffee machine,
about 15 minutes a day!
   If left unchecked, clock drift would render the time on a node meaningless  like my coffee
machine if I dont correct it every few days. To address this problem, a number of time services
exist. A time service represents an accurate time source, such as a GPS or atomic clock, which
can be used to reset the clock on a node to correct for drift on packet-switched, variable-latency
data networks.
   The most widely used time service is NTP , which provides a hierarchically organized
collection of time servers spanning the globe. The root servers, of which there are around 300
worldwide, are the most accurate. Time servers in the next level of the hierarchy (approximately
20,000) synchronize to within a few milliseconds of the root server periodically, and so on
throughout the hierarchy, a with a maximum of 15 levels. Globally there are more than 175,000
NTP servers.
   Using the NTP protocol, a node in an application running the NTP client can synchronize
to a NTP server. The time on a node is set by a UDP message exchange with one or more NTP
servers. Messages are timestamped and through the message exchange the time taken for transit
is estimated. This becomes a factor in the algorithm used by NTP to establish what the time on
the client should be reset to.  A simple NTP configuration is shown in Figure 39. On a LAN,
machines can synchronize to an NTP server within a small number of milliseconds accuracy.

   Figure 39 Illustrating using the NTP Service
   One interesting effect of NTP synchronization for our applications is that the resetting of
the clock can move the local node time forwards or backwards. This means that if our
application is measuring the time taken for events to occur (e.g. to calculate event latencies), it is
possible that the end time of the event may be earlier than the start time if the ntp protocol has
set the local time backwards.
   In fact, a node has two clocks. These are:
	Time of Day Clock: This represents the number of milliseconds since midnight
January 1970. In Java, you can get the current time using
System.currentTimeMillis(). This is the clock that can be reset by NTP, and
hence may jump forwards or backwards if it is a long way ahead of NTP time.
	Monotonic Clock: This represents the amount of time (in seconds and nanoseconds)
since an unspecified point in the past, such as the last time the system was restarted. It
will only ever move forward, however it again may not be a totally accurate measure of
elapsed time because it stalls during a virtual machine suspension. In Java, you can get
the current monotonic clock time using System.nanoTime().
.
   Applications can use an NTP service to ensure the clocks on every node in the system are
closely synchronized. Its typical for an application to resynchronize clocks on anything from a
one hour to one day time interval. This ensures the clocks remain close in value. Still, if an
application really needs to precisely know the order of events that occur on different nodes,
clock drift is going to make this fraught with danger.
   There are other time services that provide higher accuracy than NTP. Chrony  supports the
NTP protocol but provides much higher accuracy and greater scalability than NTP  the reason
it has recently been adopted by Facebook . Amazon has built the Amazon Time Sync Service
by installing GPS and atomic clocks in its data centers. This service is available for free to all
Amazon cloud customers.
   The take away from this discussion is that our applications cannot rely on timestamps of
events on different node s to represent the actual order of these events. Clock drift even by a
second or two makes cross-node timestamps meaningless to compare. The implications of this
will become clear when we start to discuss distributed databases in detail.

Summary and Further Reading
   This chapter has covered a lot of ground to explain some of the fundamental issues
communications and time in distributed systems. The key issues that should resonate with the
you, dear reader, are as follows:
1.	Communications in distributed systems can transparently traverse many different types
of underlying physical networks  e.g. Wi-Fi, wireless, WANs and LANs.
2.	Communication latencies are highly variable, and influenced by the physical distance
between nodes, and transient network congestion.
3.	Communications can fail due to network communications fabric and router failures
that make nodes unavailable, and individual node failure.
4.	The sockets library has two variants, one that supports TCP/IP for reliable connection-
based communications (SOCK_STREAM), and one for unreliable UDP datagram-based
communications (SOCK_DGRAM).
5.	RMI/RPC technologies build on TCP/IP sockets to provide abstractions for client-
server communications that mirror making local method/procedure calls.
6.	Achieving agreement, or consensus on state across multiple nodes in the presence of
crash faults is not possible in bounded time on asynchronous networks. Luckily, real
networks, especially LANs, are fast and mostly reliable, meaning we can devise
algorithms that achieve consensus in practice.
7.	There is no reliable global time source that nodes in an application can rely upon to
synchronize their behavior. Clocks on individual nodes vary and cannot be used for
meaningful comparisons.

   These issues will pervade the discussions in the rest of this book. Many of the unique
problems and solutions that are adopted in distributed systems stem from these fundamentals.
Theres no escaping them!
   An excellent source for more detailed, more theoretical coverage of all aspects of distributed
systems is George Colouris et at, Distributed Systems: Concepts and Design, 5th Edition, Pearson, 2011.
   Likewise for computer networking, youll find out all you wanted to know and no doubt
more in James Kurose, Keith Ross, Computer Networking: A Top-Down Approach, 7th Edition, Pearson
2017.

CHAPTER 5
__________________________

Application Services
   In this chapter, were going to focus on the pertinent issues in achieving scalability for the
services tier in an application. Well explore API and service design and describe the salient
features of application servers that provide the execution environment for services. Well also
elaborate on topics such as horizontal scaling, load balancing and state management that we
introduced in Chapter 2.
Service Design
   In the simplest case, an application comprises one Internet facing service that persists data to
a local data store, as shown in Figure 40. Clients interact with the service through its published
API, which is accessible across the Internet.
   Lets look at the API and service implementation in more detail.

   Figure 40 A Simple Service
  Application Programming Interface (API)

   An API defines a contract between the client and server. The API specifies the types of
requests that are possible, the data that is needed to accompany the requests, and the results that
will be obtained. APIs have many different variations, as we explored in RPC/RMI discussions
in Chapter 4. While there remains some API diversity in modern applications, the predominant
style relies on HTTP APIs. These are typically, although not particularly accurately, classified as
RESTful.
   REST is actually an architectural style that was defined by Roy Fielding in his PhD thesis .
A great source of knowledge on RESTful APIs and the various degrees to which Web
technologies can be exploited is REST in Practice by Webber, Parastatidis, and Robinson. Here well
just briefly touch on the HTTP CRUD API pattern. This pattern does not fully implement the
principles of REST, but it is widely adopted in Internet systems today.
   CRUD stands for Create, Read, Update, Delete. A CRUD API specifies how clients perform
these operations in a specific business context. For example a user might create a profile, read
catalog items, update their shopping cart and delete items from their order. A HTTP CRUD API
makes these operations possible using four core HTTP verbs, as shown in Table 3.

Verb
Uniform Resource Identifier
Example
Purpose
POST
/skico.com/skiers/{skierID}/{date}
Create a new ski day record for a skier
GET
/skico.com/skiers/(skierID)
Get the profile information for a skier,
returned in a JSON response payload
PUT
/skico.com/skiers/{skierID}
Update skier profile
DELETE
/skico.com/skiers/{skierID}
Delete a skiers profile as they didnt
renew their pass!

   Table 3 HTTP CRUD Verbs
   A HTTP CRUD API applies HTTP verbs on resources identified by Uniform Resource
Identifiers (URIs). In Table 3 for example, a URI that identifies skier 768934 would be:

   /skico.com/skiers/768934

   A HTTP GET request to this resource would return the complete profile information for a
skier in the response payload, such as name, address, number of days visited, and so on. If a
client subsequently sends a HTTP PUT request to this URI, we are expressing the intent to
update the resource for skier 768934  in this example it would be the skiers profile. The PUT
request would provide the complete representation for the skiers profile as returned by the
GET request. Again, this would be as a payload with the request. Payloads are typically
formatted as JSON, although XML and other formats are also possible. If a client sends a
DELETE request to the same URI, then the skiers profile will be deleted.
   Hence the combination of the HTTP verb and URI define the semantics of the API
operation. Resources, represented by URIs, are conceptually like objects in Object Oriented
Design (OOD) or entities in Entity-Relationship (ER) model. Resource identification and
modeling hence follows similar methods to OOD and ER modeling. The focus however is on
resources that need to be exposed to clients in the API. The Further Reading section at the end
of this chapter points to useful sources of information for resource design.
   HTTP APIs can be specified using a notation called OpenAPI . At the time of writing the
latest version is 3.0. A tool called SwaggerHub  is the de facto standard to specify APIs in
OpenAPI. The specification is defined in YAML, and an example is show in Figure 41. It
defines the GET operation on the URI /resorts. If the operation is successful, a 200 response
code is returned along with a list of resorts in a format defined by a JSON schema that appears
later in the specification. If for some weird reason, the query to get a list of resorts operated by
skico.com returns no entries, a 404 response code is returned along with an error message that
is also defined by a JSON schema.
1.	paths:
2.	  /resorts:
3.	    get:
4.	      tags:
5.	        - resorts
6.	      summary: get a list of ski resorts in the database
7.	      operationId: getResorts
8.	      responses:
9.	        '200':
10.	          description: successful operation
11.	          content:
12.	            application/json:
13.	              schema:
14.	                $ref: '#/components/schemas/ResortsList'
15.	        '404':
16.	          description: Resorts not found. Unlikely unless we go broke
17.	          content:
18.	            application/json:
19.	              schema:
20.	                $ref: '#/components/schemas/responseMsg'
21.	         500:
22.	            $ref: "#/responses/Standard500ErrorResponse"
    Figure 41 OpenAPI Example
   It is also possible for the client to see a 500 Internal Error response code. This indicates
that the server encountered an unexpected condition that prevented it from fulfilling the
request. This error code is a generic "catch-all" response. It may be thrown if the server has
crashed, or if the server is temporarily unavailable due to a transient network error.

  Designing Services
   An application server container receives requests and routes them to the appropriate handler
function to process the request. The handler is defined by the application service and
implements the business logic required to generate results from the request. As multiple
simultaneous requests arrive at a service instance, each is typically  allocated a thread context to
execute the request.
   The sophistication of the routing functionality varies widely by technology platform and
language. For example, in Express.js, the container calls a specified function for requests that
match an API signature  known as a route path - and HTTP method. Figure 42 illustrates this
with a method that will be called when the client sends a GET request for a specific skiers
profile, as identified by the value of :skierID.
1.	app.get('/skiers/:skierID', function (req, res) {
2.	  // process the GET request
3.	  ProcessRequest(req.params)
4.	}
    Figure 42 Express.js request routing example
In Java, the Spring framework provides an equally sophisticated method routing technique. It
leverages a set of annotations that define dependencies and implement dependency injection to
simplify the service code. Figure 43 shows an example of annotations usage, namely:

	@RestController  identifies the class as a controller that implements an API and
automatically serializes the return object into the HttpResponse returned from the API.
	@GetMapping  maps the API signature to the specific method, and defines the format of
the response body
	@PathVariable  identifies the parameter as value that originates in the path for URI that
maps to this method
1.	@RestController
2.	public class SkierController {
3.
4.	    @GetMapping("/skiers/{skierID}",
5.	                produces = application/json)
6.	    public Profile GetSkierProfile(
7.	                        @PathVariable String skierID,
8.	                        ) {
9.	          // DB query method omitted for brevity
10.	        return GetProfileFromDB(skierID);
11.	    }
12.	}
    Figure 43 Spring method routing example
   Another Java technology, JEE servlets, also provide annotations, as shown in Figure 44, but
these are simplistic compared to Spring and other higher-level frameworks. The @WebServlet
annotation identifies the base pattern for the URI which should cause a particular servlet to be
invoked. This is /skiers in our example. The class that implements the API method must
extend the HttpServlet abstract class from the javax.servlet.http package and override
at least one method that implements a HTTP request.  The HTTP verbs map to methods as
follows:

	doGet: HTTP GET requests
	doPost: HTTP POST requests
	doPut: HTTP PUT requests
	doDelete: for HTTP DELETE requests

   Each method is passed as parameters a HttpServletRequest and HttpServletResponse
object. The servlet container creates the HttpServletRequest object, which contains members
that represent the components of the incoming HTTP request. This object contains the
complete URI path for the call, and it is the servlets responsibility to explicitly parse and
validate this, and extract path and query parameters if valid. Likewise, the servlet must explicitly
set the properties of the response using the HttpServletResponse object.
   Servlets therefore require more code from the application service programmer to implement.
However, they are likely to provide a more efficient implementation as there is less plumbing
involved compared to the more powerful annotation approaches of Spring et al. A good rule of
thumb is that if a framework is easier to use, it is likely to be less efficient. This is a classic
performance versus ease-of-use trade-off. Well see lots of these in this book
1.	import javax.servlet.http.*;
2.	@WebServlet(
3.	    name = SkiersServlet,
4.	    urlPatterns = /skiers
5.	)
6.	public class SkierServlet extends HttpServlet (
7.
8.	protected void doGet(HttpServletRequest request,
9.	                     HttpServletResponse response) {
10.	  // handles requests to /skiers/{skierID}
11.	  try {
12.	     // extract skierID from the request URI (not shown for brevity)
13.	     String skierID  = getSkierIDFromRequest(request);
14.	     if(skierID == null) {
15.	        // request was poorly formatted, return error code
16.	        response.setStatus(HttpServletResponse.SC_BAD_REQUEST);    }
17.	     else {
18.	        // read the skier profile from the database
19.	        Profile profile = GetSkierProfile (skierID);
20.	        // add skier profile as JSON to HTTP response and return 200
21.	        response.setContentType("application/json");
22.	        response.getWriter().write(gson.toJson(Profile);
23.	        response.setStatus(HttpServletResponse.SC_OK);
24.	     } catch(Exception ex) {
25.	         response.setStatus
26.	           (HttpServletResponse.SC_INTERNAL_SERVER_ERROR);    }
27.
28.	       }
29.	} }
    Figure 44 JEE Servlet Example
  State Management

   State management is a tricky, nuanced topic. The bottom line is that service implementations
that need to scale should avoid storing conversational state. What on earth does that mean?
Lets start by examining the topic of state management with HTTP.
   HTTP is known as stateless protocol. This means each request is executed independently,
without any knowledge of the requests that were executed before it from the same client.
Statelessness implies that every request needs to be self-contained, with sufficient information
provided by the client for the Web server to satisfy the request regardless of previous activity
from that client.
   The picture is a little more complicated that this simple description portrays, however. For
example:
	The underlying socket connection between a client and server is kept open so that the
overheads of connection creation are amortized across multiple requests from a client.
This is the default behavior for versions HTTP/1 and above.
	HTTP supports cookies, which are known as the HTTP State Management
Mechanism . Gives it away really!
	HTTP/2 supports streams, compression, and encryption, all of which require state
management

   So, originally HTTP was stateless, but perhaps not anymore? Armed with this confusion (!),
lets look at application services APIs built on top of HTTP.
   When a user or application connects to a service, it will typically send a series of requests to
retrieve and update information. Conversational state represents any information that is retained
between requests such that the subsequent request can assume the service has retained
knowledge about the previous interactions. Lets explore a simple example.
   In our skier service API, a user may request their profile by submitting a GET request to the
following URI:

   /skico.com/skiers/768934

   They may then use their app to modify their phone number and send a PUT request to a
URI designed for updating this field:

   /skico.com/skiers/phoneno/4123131169

   As this URI does not identify the skier, the service must know the unique identifier of the
skier, namely 768934. Hence for this PUT operation to succeed, the service must have retained
conversational state from the previous GET request.
   Implementing this approach is relatively straightforward. When the service receives the initial
GET request, it creates a session state object that uniquely identifies the client connection. In
reality, this is often performed when a user first connects to or logs in to a service. The service
can then read the skier profile from the database and utilize the session state object to store
conversational state  in our example this would be skierID. When the subsequent PUT
request arrives from the client it uses the session state object to look up the skierID associated
with this session and uses that to update the skiers phone number.
   Services that maintain conversational state are known as stateful services. Stateful services
are attractive from a design perspective as they can minimize the number of times a service
retrieves data (state) from the database and reduce the amount of data that is passed between
clients and the services. For services with light request loads they can make eminent sense and
are promoted by many frameworks to make services easy to build and deploy. For example, JEE
servlets support session management using the HttpSession object, and similar capabilities are
offered by the Session object in ASP.NET.
   As we scale our service implementations however, the stateful approach becomes
problematic. For a single service instance, we have two problems to consider:

1.	If we have multiple client sessions all maintaining session state, this will utilize available
service memory. The amount of memory utilized will be proportional the number of
clients we are maintaining state for. If a sudden spike of requests arrive, how can we be
certain we will not exhaust available memory and cause the service to fail?
2.	We also must be mindful about how long to keep session state available. A client may
stop sending requests but not cleanly close their connection to allow the state to be
reclaimed. All session management approaches support a default session time out. If we
set this to a short time interval, clients may see their state disappear unexpectedly. If we
set the session time out period to be too long, we may degrade service perform as it
runs low on resources.

   In contrast, stateless services do not assume that any conversational state from previous calls
has been preserved. The service should not maintain any knowledge from earlier requests, so
that each request can be processed individually. This requires the client to provide all the
necessary information for the service to process the request and provide a response.
   If we consider our example above, we could transform the PUT request to be stateless by
incorporating the skierID in the URI:

   /skico.com/skiers/768934/phoneno/4123131169

   In reality, this is a pretty dumb API design. As we discussed in the service API section, the
API should in fact transfer the complete resource  the skier profile  with the GET response
and PUT request. This means the GET request for the skier should retrieve and return the
complete skier profile so that the client app can display and modify any data items in the profile
as the user wishes. The complete updated profile is then sent as the payload in the subsequent
PUT operation, and used by the service to update the resource, that is persisted in a database.
This is shown in Figure 45.

   Figure 45 Stateless API Example
   Any scalable service will need stateless APIs. If a service needs to retain state pertaining to
client sessions  the classic shopping cart example - it must be stored externally to the service.
This invariably means an external data store.
   Well revisit this topic later in this chapter during the discussion of horizontal scaling. Thats
when stateless services really come to the fore.
Applications Servers
   Application servers are the heart of a scalable application, hosting the business services that
comprise an application. Their basic role is to accept requests from clients, apply application
logic to the requests, and reply to the client with the request results. Clients may be external or
internal, as in other services in the application that require to use the functionality of a specific
service.
   The technological landscape of application servers is broad and complex, depending on the
language you want to use and the specific capabilities that each offer. In Java, the Java
Enterprise Edition (JEE)  defines a comprehensive, feature rich standards-based platform for
application servers, with multiple different vendor and open source implementations.
   In other languages, the Express.js  server supports Node, Flask supports Python , and in
GoLang a service can be created by incorporating the net/http package. These
implementations are much more minimal and lightweight than JEE and are typically classified
are Web application frameworks. In Java, the Apache Tomcat server  is a somewhat equivalent
technology. Tomcat is open source implementation of a subset of the JEE platform, namely the
Java Servlet, JavaServer Pages, Java Expression Language and Java WebSocket technologies.
   Figure 46 depicts a simplified view of the anatomy of Tomcat. Tomcat implements a servlet
container, which is an execution environment for application-defined servlets. Application
defined Servlets are loaded into this container, which provides lifecycle management and a
multithreaded runtime environment.
   Request arrive at the IP address of the server, which is listening for traffic on specific ports.
For example, by default Tomcat listens on port 8080 for HTTP requests and 8443 for HTTPS
requests. Incoming requests are processed by one or more listener threads. These create a
TCP/IP socket connection between the client and server. If network requests arrive at a
frequency that cannot be processed by the TCP listener, pending requests are queued up in the
Sockets Backlog. The size of the backlog is operating system dependent. In most Linux versions
the default is 100.
   Once a connection is established, the TCP requests are marshalled by, in this example, a
HTTP Connector which generates the HTTP request that the application can process. The HTTP
request is then dispatched to an application service thread to process. Application container
threads are managed in a thread pool, essentially a Java Executor, which by default in Tomcat
is a minimum size of 25 threads and a maximum of 200. If there are no available threads to
handle a request, the container maintains them in a queue of runnable tasks and dispatches these
as soon as a thread becomes available. This queue by default is size Integer.MAX_VALUE  that
is, essentially unbounded . If a thread remains idle for by default, 60 seconds, it is killed to free
up resources in the JVM.

   Figure 46 Anatomy of a Web application server
   For each request, the method that corresponds with the HTTP request is invoked in a
thread. The servlet method processes the HTTP request headers, executes the business logic,
and constructs a response that is marshalled by the container back to a TCP/IP packet and sent
over the network to the client.
   In processing the business logic, servlets often need to query an external database. This
requires each thread executing the servlet methods to obtain a database connection and execute
database queries. As database connections are limited resources and consume resources in both
the client and database server, a fixed size connection pool is typically utilized. The pool hands
out open connection to requesting threads on demand.
   When a servlet wishes to submit a query to the database, it therefore requests an open
connection from the pool. If one is available, access to the connection is granted to the servlet
until it indicates it has completed its work. At that stage the connection is returned to the pool
and made available for another servlet to utilize. As the thread pool is typically larger than the
connection pool, a servlet may request a connection when none are available. The connection
pool maintains a request queue and hands out open connections on a FIFO basis, and threads
in the queue are blocked until there is availability.
   An application server framework such as Tomcat is hence highly configurable to different
handle different workloads. For example, the size of the thread and database connection pools
can be specified in configuration files that are read at startup.
   The complete Tomcat container environment runs within a single JVM, and hence
processing capacity is limited by the number of vCPUs available and the amount of memory
allocated as heap size. Each allocated thread consumes memory, and the various queues in the
request processing pipeline consume resources while requests are waiting. This means that
request latency will be governed by both the request processing time in the servlet and the time
spent waiting in queues for threads and connections to become available.
   In a heavily loaded server with many threads, context switching may start to degrade
perform, and available memory may be become limited. If perform degrades, queues grow as
requests wait for resources. This consumes more memory. If more requests are received than
can be queued up and processed by the server, then new TCP/IP connections will be refused,
and clients will see errors. Eventually, an overloaded server will run out of resources and start
throwing exceptions and crash.
   Time spent tuning configuration parameters to efficiently handle anticipated loads is rarely
wasted. A rule-of-thumb is that CPU utilization that consistently exceeds the 70-80% range is a
signal of overload. Similar insights exist for memory usage. Once any resource gets close to full
utilization, systems tend to exhibit less predictable performance, as more time is spent, for
example thread context switching and garbage collecting. This inevitably effects latencies and
throughput.
   Monitoring tools available with Web application frameworks enable engineers to gather a
range of important metrics, including latencies, active requests, queue sizes and so on. These are
invaluable for carrying out data-driven experiments that lead to performance optimization.
   Java-based application frameworks such as Tomcat will invariably support the JMX  (Java
Management Extensions) framework, which is a standard part of the Java Standard Edition
platform. JMX enables frameworks to expose monitoring information based on the capabilities
of MBeans (Managed Beans), which represent a resource of interest (e.g. thread, database
connections usage). This enables an eco-system of tools to offer capabilities for monitoring
JMX-supported. These range from JConsole  which is available in the JDK by default, to
powerful open source technologies such as JavaMelody  and many expensive commercial
offerings.
Horizontal Scaling
   A core principle of scaling a system is being able to easily add new processing capacity to
handle increased load. For most systems, a simple and effective approach is deploying multiple
instances of stateless server resources and using a load balancer to distribute the requests across
these instances. This is known as horizontal scaling and illustrated in Figure 47.

   Figure 47 Simple Load Balancing Example
   These two ingredients, namely stateless service replicas and a load balancer, are both
necessary. Lets explain why.
   Service replicas are deployed on their own (virtual) hardware. Hence if we have two replicas,
we double our processing capacity. If we have ten replicas, we have 10x capacity. This enables
our system to handle increased loads. The aim of horizontal scaling is to create a system
processing capacity that is the sum of the total resources available
   The servers need to be stateless, so that any request can be sent to any service replica to
handle. This decision is made by the load balancer, which can use various policies to distribute
requests. If the load balancer can keep each service replica equally busy, then we are effectively
using the processing capacity provided by the service replicas.
   Horizontal scaling also increases availability. With one service instance, if it fails, the service
is unavailable. This is known as a single point of failure (SPoF)  a bad thing, and one to avoid
in any scalable distributed system. Multiple replicas increase availability. If one replica fails,
requests can be directed to any  they are stateless, remember  replica. The system will have
reduced capacity until the failed server is replaced, but it will still be available. Which is
important. The ability to scale is crucial, but if a system is unavailable, then the most scalable
system ever built is still somewhat ineffective!
Load Balancing
   Load balancing aims to effectively utilize the capacity of a collection of services to optimize
the response time for each request. This is achieved by distributing requests across the available
services as evenly as possible and avoiding overloading some services while underutilizing
others. Clients send requests to the IP address of the load balancer, which redirects requests to
target services, and relays the results back to the client. This means clients never contact the
services directly, which is also beneficial for security as the services can live behind a security
perimeter and not be exposed to the Internet.
   Load balancers may act at the network level or the application level. These are often called Layer 4
and Layer 7 load balancers respectively. These names refer to network transport layer at Layer 4
in the Open Systems Interconnection (OSI) Reference Model , and the application layer at
Layer 7. The OSI model defines network functions in seven abstract layers. Each layer defines
standards for how data is packaged and transported. Lets explore the differences between the
two techniques.
   Network level load balancers distribute requests at the network connection level, operating
on individual TCP or UDP packets.  Routing decisions are made on the basis of client IP
addresses. Once a target service is chosen, the load balancer uses a technique called Network
Address Translation (NAT). This changes the destination IP address in the client request packet
from that of the load balancer to that of the chosen target. When a response is received from
the target, the load balancer changes the source address recorded in the packet header from the
targets IP address to its own. Network load balancers are relatively simple as they operate on
the individual packet level. This means they are extremely fast, as they provide few features
beyond choosing a target service and performing NAT functionality.
   In contrast, application level load balancers reassemble the complete HTTP request and base
their routing decisions on the values of the HTTP headers and on the actual contents of the
message. For example, a load balancer can be configured to send all POST requests to a subset
of available services, or distribute requests based on a query string in the URI. Application load
balancers are sophisticated reverse proxies. The richer capabilities they offer means they are
slightly slower than network load balancers, but the powerful features they offer can be utilized
to more than make up for the overheads incurred.
   In general, a load balancer has the following features which are explained in the following
sections:

1.	Load distribution policies
2.	Health monitoring
3.	Elasticity
4.	Session affinity

  Load Distribution Policies
   Load distribution policies dictate how the load balancer chooses a target service to process a
request. Any load balancer worth its salt will offer several load distribution policies  HAProxy
offers 10 in fact . The following are probably the most commonly supported across all load
balancers:
	round-robin: the load balancer distributes requests to available servers in a round-robin
fashion
	least connections:  the load balancer distributes new requests to the server with the least
open connections
	HTTP header field: the load balancer directs requests based on the contents of a specific
HTTP header field. For example all requests with the header field X-Client-
Location:US,Seattle could be routed to a specific set of servers.
	HTTP operation: the load balancer directs requests based on the HTTP verb in the
request

   Load balancers will also allow services to be allocated weights. For example, standard service
instances in the load balancing pool may have 4 vCPUs and each is allocated a weight of 1. If a
service with 8 vCPUs is added, it can be assigned a weight of 2 so the load balancer will send
twice as many requests its way.

  Health Monitoring
   A load balancer will periodically sends pings and attempts connections to test the health of
each service in the load balancing pool. These tests are called health checks. If a service
becomes unresponsive or fails connection attempts, it will be removed from the load balancing
pool and  no requests will be sent to that host. If the connection to the service has experienced
a transient failure, the load balancer will reincorporate the service once it becomes available and
healthy. If, however it has failed, the service will be removed from the load balancer target pool.
  Elasticity
   Spikes in request loads can cause the service capacity available to a load balancer to become
saturated, leading to longer response times and eventually request and connection failures.
Elasticity is the capability of a load balancer to dynamically provision new service capacity to
handle an increase in requests. As load increases, the load balancer starts up new resources and
directs requests to these, and as load decreases the load balancer stops services that are no
longer needed.
   An example of elastic load balancing is the Amazon Web Services (AWS) Auto-Scaling
groups. An Auto Scaling group is a collection of services available to a load balancer that is
defined with a minimum and maximum size. The load balancer will ensure the group always has
the minimum numbers of services available, and the group will never exceed the maximum
number. The actual number available at any time will depends on the client request load the
load balancer is handling for the services in the group. This scheme is illustrated in Figure 48.

   Figure 48 Elastic Load Balancing
   Typically, there are two ways to control the number of services in a group. The first is based
on a schedule, when the request load increases and decreases are predictable. For example, you
may have an online entertainment guide and publish the weekend events for a set of major cities
at 6pm on Thursday. This generates a higher load until Sunday at noon. An Auto Scaling group
could easily be configured to provision new services at 6pm Thursday and reduce the group size
to the minimum at noon Sunday.
   If increased load spikes are not predictable, elasticity can be controlled dynamically with a set
of configuration parameters. The most commonly used parameter is average CPU utilization
across the active services. For example, an Auto Scaling group can be defined to maintain
average CPU utilization at 70%. If this value is exceeded, the load balancer will start one or
more new services instances until the 70% threshold is reached. Instances need time to start 
often a minute or more - and hence a warmup period can be defined until the new instance is
considered to be contributing to the groups capacity. When the group average CPU utilization
drops below 70%, scale in or scale down will start and instances will be automatically stopped and
removed from the pool.
   Elasticity is a key feature that allows services to scale dynamically as demand grows. For
highly scalable systems, it is a mandatory capability. Like all advanced capabilities however, it
brings new issues for us to consider in terms of downstream capacity and costs. Well discuss
these in Chapter XXXX.

  Session Affinity
   Session affinity, or sticky sessions, are a load balancer feature for stateful services. With
sticky sessions, the load balancer sends all requests from the same client to the same service
instance. This enables the service to maintain in-memory state about each specific client session.
   There are various ways to implement sticky sessions. For example, HAProxy provides a
comprehensive set of capabilities to maintain client requests on the same service in the face of
service additions, removals and failures . AWS Elastic Load Balancing generates an HTTP
cookie that identifies the service a clients session is associated with. This cookie is returned to
the client, which must send it in subsequent request to ensure session affinity is maintained.
   Sticky sessions can be problematic for highly scalable systems. They lead to a load imbalance
problem, in which, over time, clients are not evenly distributed across services. This is illustrated
in Figure 49, where two clients are connected to one service while another service remains idle.

   Figure 49 Load Imbalance with Sticky Sessions
   Load imbalance occurs because client sessions last for varying amounts of time. Even if
sessions are evenly distributed initially, some will terminate quickly while others will persist. In a
lightly loaded system, this tends to not be an issue. However, in a system with millions of
sessions being created and destroyed constantly, load imbalance is inevitable. This will lead to
some services being underutilized, while others are overwhelmed and may potentially fail due to
resource exhaustion.
   Stateful services have other downsides. When a service inevitably fails, how do the clients
connected to that server recover the state that was being managed? If a service instance
becomes slow due to high load, how do clients respond? In general stateful servers create
problems that in large scale systems are difficult to design around and manage.
   Stateless services have none of these downsides. If one fails, clients get an exception and
retry, with their request routed to another live service. If a service is slow due to a transient
network outage, the load balancer takes it out of the service group until it passes health checks
of fails. All state is either externalized or provided by the client in each request, so service
failures can be handled easily by the load balancer.
   Stateless services enhance scalability, simplify failure scenarios and ease the burden of service
management. For scalable applications, these advantages far outweigh the disadvantages, and
hence their adoption in most major Internet sites such as Netflix.
Summary and Further Reading
   Services are the heart of a scalable software system. They define the contract defined as an
API that specifies their capabilities to clients. Services are defined and execute in an application
server container environment that hosts the service code and routes incoming API requests to
the appropriate processing logic. Application servers are highly programming language
dependent, but in general provide a multithreaded programming model that allows services to
process many requests simultaneously. If the threads in the container thread pool are all utilized,
the application server queues up requests until a thread becomes available.
   As request loads grow on a service, we can scale it out horizontally using a load balancer to
distribute requests across multiple instances. This architecture also provides high availability as
the multiple service configuration means the application can tolerate failures of individual
instances. The service instances are managed as pool by the load balancer, which utilizes a load
distribution policy to choose a target service for each request. Stateless services scale easily and
simplify failure scenarios by allowing the load balancer to simply resend requests to responsive
targets. Although most load balancers will support stateful services using a feature called sticky
sessions, stateful services make load balancing and handling failures more complex. Hence, they
are not recommended for highly scalable services.
   API design is a topic of great complexity and debate. An excellent overview of basic API
design and resource modeling is https://www.thoughtworks.com/insights/blog/rest-api-
design-resource-modeling.
   The Java Enterprise Edition (JEE) is an established and widely deployed server-side
technology. It has a wide range of abstractions for building rich and powerful services. The
Oracle tutorial is an excellent starting place for appreciating this platform -
https://docs.oracle.com/javaee/7/tutorial/.
   Much of the knowledge and information about load balancers is buried in the
documentation provided by the technology suppliers. You choose your load balancer and then
dive into the manuals. For an excellent, broad perspective on the complete field of load
balancing, this is a good resource.
   Tony Bourke, Server Load Balancing, OReilly Publishing?
CHAPTER 6
__________________________

Caching
   Caching is an essential ingredient of a scalable system. Caching makes the results of
expensive queries and computations available for reuse by subsequent requests at low cost. By
not having to reconstruct the cached results for every single request, the capacity of the system
is increased, and it is hence able to scale to handle greater workloads.
   Caches exist in many places in an application. The CPUs that run applications have multi-
level, fast hardware caches to reduce relatively slow main memory accesses. Database engines
can make use of main memory to cache the contents of the data store in memory so that in
many cases queries do not have to touch relatively slow disks.
   This chapter covers:
	application based caching, in which service business logic incorporates the caching and
access of precomputed results
	Web based caching, which exploits mechanisms built into the HTTP protocol to enable
caching of results within the infrastructure provided by the Internet.
Application Caching
   Application caching is designed to improve request responsiveness by storing the results of
queries and computations in memory so they can be subsequently served by later requests. For
example, think of an online newspaper site. Once posted, articles change infrequently and hence
can be cached and on first access and reused on all subsequent requests until the article is
updated.
   In general, caching relieves databases of heavy read traffic, as many queries can be served
directly from the cache. It also reduces computation costs for objects that are expensive to
construct, for example those needing queries that span several different databases. The net
effect is to reduce the computational load on our services and databases and create head room
for more requests.
   Caching requires additional resources, and hence cost, to store cached results. However, well
designed caching schemes are low cost compared to upgrading database and service nodes to
cope with higher request loads. As an indication of the value of caches, approximately 3% of
infrastructure at Twitter is dedicated to application level caches.  At Twitter scale, that is a lot
of infrastructure!
   Application level caching exploits dedicated distributed cache engines. The two predominant
technologies in this area are memcached  and Redis . Both are essentially distributed in-
memory key-value stores designed for arbitrary data (strings, objects) from the results of
database queries or downstream service API calls. The cache appears to services as a single
store, and objects are allocated to individual cache servers using hashing on the object key.
   The basic scheme is shown in Figure 50. The service first checks the cache to see if the data
it requires is available. If so, it returns the cached contents as the results  we have what is
known as a cache hit. If the data is not in the cache  a cache miss - the service retrieves the
requested data from the database and writes the query results to the cache so it is available for
subsequent client requests without querying the database.

   Figure 50 Application Level Caching
   Lets return to our mythical winter resort business for a use case. At a busy resort, skiers and
boarders can use their mobile app to get an estimate of the lift wait times across the resort. This
enables them to plan and avoid congested areas where they will have to wait to ride a lift for say
15 minutes (or sometimes more!).
   Every time a skier loads a lift, a message is sent to the companys service that collects data
about skier traffic patterns. Using this data, the system can estimate lift wait times from the
number of skiers who ride a lift and the rate they are arriving. This is an expensive calculation as
it requires aggregating potentially 10s of thousands of lift ride records and performing the wait
time calculation. For this reason, once the results are calculated, they are deemed valid for five
minutes. Only after this time has elapsed is a new calculation performed and results produced.
   Figure 51 shows an example of how a stateless LiftWaitService might work. When a
request arrives, the service first checks the cache to see if the latest wait times are available. If
they are, the results are immediately returned to the client. If the results are not in the cache, the
service calls a downstream service which can perform the lift wait calculations and returns them
as a List. These results are then stored in the cache and then returned to the client.
   Cache access requires a key with which to associate the results with. In this example we
construct the key with the string liftwaittimes: concatenated with the resort identifier that
is passed by the client to the service. This key is hashed by the cache to identify the server where
the cached value resides. When we write a new value to the cache (line 8), we pass a value of
300 seconds as a parameter to the put operation. This is known as a time to live, or TTL value. It
tells the cache that after 300 seconds this key-value pair should be evicted from the cache as the
value is no longer relevant.
   When the cache value is evicted, the next request will calculate the new values and store
them in the cache. But while the cache value is valid, all requests will utilize it, meaning there is
no need to perform the expensive lift wait time calculation for every call. Hence if we get N
requests in a 5 minute period, N-1 are served from the cache. Imagine if N is 20,000. This is a
lot of expensive calculations saved.
1.	public class LiftWaitService {
2.
3.	  public List getLiftWaits(String resort) {
4.	    List liftWaitTimes = cache.get(liftwaittimes: + resort);
5.	      if (liftWaitTimes == null) {
6.	         liftWaitTimes = skiCo.getLiftWaitTimes(resort);
7.	         // add result to cache, expire in 300 seconds
8.	         cache.put("liftwaittimes:" + resort, liftWaitTimes, 300);
9.	      }
10.	    return liftWaitTimes;
11.	  }
12.	}
    Figure 51 Caching Example
   Using an expiry time like the TTL is a common way to invalidate cache contents. It ensures a
service doesnt deliver stale, out of date results to a client. It also enables the system to have
some control over cache contents, which are typically limited. If cached items are not flushed
periodically, the cache may fill up. In this case, a cache will adopt a policy such as least recently
used or least accessed to choose cached values to evict and create space for more current, timely
results.
   Application caching can provide significant throughput boosts, reduced latencies, and
increased client application responsiveness. The key to achieving these desirable qualities is to
satisfy as many requests as possible from the cache. This is known as the cache hit rate. The
general design principle is to maximize the cache hit rate and minimize the cache miss rate 
when a request cannot be satisfied by cached items. When a cache miss occurs, the request must
be satisfied through querying databases or downstream services. The results of the request can
then be written to the cache and hence be available for further accesses.
   Theres no hard and fast rule on what the cache hit rate should be, as it depends on the cost
of constructing the cache contents and the update rate of cached items. Ideal cache designs have
many more reads than updates. This is because when an item must be updated, the application
needs to invalidate cache entries that are now stale because of the update. This means the next
request will result in a cache miss.
   When items are updated regularly, the cost of cache misses can negate the benefits of the
cache. Service designers therefore need to carefully consider query and update patterns an
application experiences, and construct caching mechanisms that yield the most benefit. It is also
crucial to monitor the cache usage once a service is in production to ensure the hit and miss
rates are in line with design expectations. Caches will provide both management utilities and
APIs to enable monitoring of the cache usage characteristics. For example, memcached is
accessible through a telnet session. A large number of statistics are available, including the hit
and miss counts as shown in the snippet in Figure 52.
1.	STAT get_hits 98567
2.	STAT get_misses 11001
3.	STAT evictions 0
    Figure 52 Example of memcached monitoring output
   Application level caching is also known as the cache-aside pattern . The name references the
fact that the application code effectively bypasses the data storage systems if the required
request results are available. This contrasts with other caching patterns, in which the application
always reads from and writes to the cache. These are known as read-through, write-through and write-
behind caches as explained below:

	Read-through: The application satisfies all requests by accessing the cache. If the data
required is not available in the cache, a loader in invoked to access the data systems and
store the results in the cache for the application to utilize.
	Write-through: The application always writes updates to the cache. When the cache is
updated, a writer is invoked to write the new cache values to the database. When the
database is updated, the application can complete the request.
	Write-behind: Like write-through, except the application does not wait for the value to
written to the database from the cache. This increases request responsiveness at the
expense of possible lost updates if the cache server crashes before a database update is
completed. This is also known as a write-back cache, and internally is the strategy used
by most database engines.

   The beauty of these caching approaches is that they simplify application logic. Applications
always utilize the cache for reads and writes, and the cache provides the magic to ensure the
cache interacts appropriately with the backend storage systems. This contrasts with the cache-
side pattern, in which application logic must be cognizant of cache misses.
   The application still needs to make this magic happen of course. These strategies require a
cache technology which can be augmented with an application-specific handler that performs
database reads and writes when the application accesses the cache. For example, NCache
supports provider interfaces that the application implements. These are invoked automatically
on cache misses for read-through caches and on writes for write-through caches. Other such
caches are essentially dedicated database caches, and hence require cache access to be identical
to the underlying database model. An example of this is Amazons DynamoDB Accelerator
(DAX).  DAX sits between the application code and DynamoDB, and transparently acts as a
high-speed in memory cache to reduce database access times.
   One significant advantage of the cache-aside strategy is that it is resilient to cache failure. In
such circumstances, as the cache is unavailable, all requests are essentially handled as a cache
miss. Performance will suffer, but services will still be able to satisfy requests. In addition,
scaling cache-aside platforms such as Redis and Memcached is straightforward due their simple,
distributed key-value store model. For these reasons, the cache-aside pattern is the primary
approach seen in massively scalable systems.
Web Caching
   One of the reasons that Web sites are so highly responsive is that the Internet is littered with
Web caches. Web caches stores a copy of a given resource for a defined time period. The caches
intercept client requests and if they have a requested resource cached locally, it returns the copy
rather than forwarding the request to the target service. Hence many requests can be satisfied
without placing a burden on the service. Also, as the caches are closer to the client, the requests
will have lower latencies.
   Figure 53 gives an overview of the Web caching architecture. Multiple levels of caches exist,
starting from the clients local Web browser cache, local proxy caches within organizations and
Internet Service Providers, and reverse proxy caches that exist within the services execution
domain. Web browser caches are also known as private caches (for a single user) and proxy
caches are shared caches that support requests from multiple users.
   Caches typically store the results of GET requests only, and the cache key is the URI of the
associated GET. When a client sends a GET request, it may be intercepted by one or more
caches along the request path. Any cache with a fresh copy of the requested resource may
respond to the request. If no cached content is found, the request is served by the service
endpoint, which is also called the origin server.

   Figure 53 Web Caches in the Internet
   Services can control what results are cached and for how long they are stored by using
HTTP caching directives. Services set these directives in various HTTP response headers, as
shown in the simple example in Figure 54. We will describe these directives in the following
subsections.
1.	Response:
2.	HTTP/1.1 200 OK Content-Length: 9842
3.	Content-Type: application/json
4.	Cache-Control: public
5.	Date: Fri, 26 Mar 2019 09:33:49 GMT
6.	Expires: Fri, 26 Mar 2019 09:38:49 GMT
   Figure 54 Example HTTP Response with caching directives
  Cache-Control
   The Cache-Control HTTP header can be used by client requests and service responses to
specify how the caching should be utilized for the resources of interest.

*	no-store: Specifies that a resource from a request response should not be cached. This
is typically used for sensitive data that needs to be retrieved from the origin servers each
request.
*	no-cache: Specifies that a cached resource must be revalidated with an origin server
before use. We discuss revalidation in the Etag subsection below.
*	private: Specifies a resource can be cached only by a user-specific device such as a
Web browser
*	public: Specifies a resource can be cached by any proxy server
*	max-age: defines the length of time in seconds a cached copy of a resource should be
retained. After expiration, a cache must refresh the resource by sending a request to the
origin server.

  Expires and Last-Modified
   The Expires and Last-Modified HTTP headers interact with the max-age directive to
control how long cached data is retained.
   Caches have limited storage resources and hence must periodically evict items from memory
to create space. To influence cache eviction, services can specify how long resources in the
cache should remain valid, or fresh. Once this time period for a cached resource expires, it
becomes stale and may become a candidate for eviction. When a request arrives for a fresh
resource, the cache serves the locally stored results without contacting the origin server.
   Freshness is calculated using a combination of header values. The "Cache-Control: max-
age=N" header is the primary directive, and this value the specifies the freshness period in
seconds.
   If max-age is not specified, the Expires header is checked next. If this header exists, then it
is used to calculate the freshness period. As a last resort, the Last-Modified header can specify
the freshness lifetime based on a heuristic calculation that the cache can support. This is usually
calculated as (Date header value  Last-Modified header value)*0.1.

  Etag
   HTTP provides another directive that controls cache item freshness. This is known as an
Etag. An Etag is an opaque value that van be used by a cache to check if a cached resource is
still valid. Lets explain this using another winter sports example.
   A ski resort posts a weather report at 6am every day during the ski season. If the weather
changes during the day, the resort updates the report. Sometimes this happens two or three
times each day, and sometimes not at all if the weather is stable. When a request arrives for the
weather report, the service responds with a maximum age to define cache freshness, and also an
ETag that represents the version of the weather report that was last issued. This is shown in
Figure 55, which tells a cache to treat the weather report resource as fresh for at least 3600
seconds, or 60 minutes.
1.	Request:
2.	GET /skico.com/weather/Blackstone
3.
4.	Response:
5.	HTTP/1.1 200 OK Content-Length: ...
6.	Content-Type: application/json
7.	Date: Fri, 26 Mar 2019 09:33:49 GMT
8.	Cache-Control: public, max-age=3600
9.	ETag: 09:33:49"
<!-- Content omitted -->

   Figure 55 HTTP Etag Example
   For the next hour, the cache simply serves this cached weather report to all clients who issue
a GET request. This means the origin servers are freed from processing these requests  the
outcome that we want from effective caching. After an hour though, the resource become stale.
Now, when a request arrives for a stale resource, the cache forwards it to the origin server  with
a If-None-Match directive along with the Etag to enquire if the resource, in our case the
weather report, is still valid. This is known as revalidation.
   There are two possible responses to this request.

1.	If the Etag in the request matches the value associated with the resource in the services,
the cached value is still valid. The origin server can therefore return a 304 (Not
Modified) response, as shown in Figure 56. No response body is needed as the cached
value is still current, thus saving bandwidth, especially for large resources. The response
may also include new cache directives to update the freshness of the cached resource.
2.	The origin server may ignore the revalidation request and respond with a 200 response
code and a response body and Etag representing the latest version of the weather report.
1.	Request:
2.	GET /upic.com/weather/Blackstone
3.	If-None-Match: "09:33:49
4.	Response:
5.	HTTP/1.1 304 Not Modified
    Figure 56 Validating an Etag
   In the service implementation, a mechanism is needed to support revalidation. In our
weather report example, one strategy is as follows:

1.	Generate new daily report: The weather report is constructed and stored in a database.
The service also creates a new cache entry that identifies the weather report resource and
associates an Etag with this version of the resource, for example {#resortname-weather,
Etag value}.
2.	GET requests: When a GET request arrives, return the weather report and the Etag.
This will also populate Web caches along the network response path,
3.	Conditional GET requests: Lookup the Etag value in cache at {#resortname-
weather} and return 304 if the value has not changed. If the cached Etag has changed,
return 200 along with the latest weather report and a new Etag value.
4.	Update weather report: A new version of the weather report is stored in the database
and the cached Etag value is modified to represent this new version of the response.

   When used effectively, Web caching can significantly reduce latencies and save network
bandwidth. This is especially true for large items such as images and documents. Further, as
Web caches handle requests rather than application services, this reduces the request load on
origin servers, creating additional capacity.
   Proxy caches such as Squid  and Varnish  are extensively deployed in the Internet. Scalable
applications therefore exploit the powerful facilities provided by HTTP caching to exploit his
caching infrastructure.
Summary and Further Reading
   Caching is an essential component of any scalable distribution. Caching stores information
that is requested by many clients in memory and serves this information as the results to client
requests. While the information is still valid, it can be served potentially millions of times
without the cost of recreation.
   Application caching using a distributed cache is the most common approach to caching in
scalable systems. This approach requires the application logic to check for cached values when a
client request arrives and return these if available. If the cache hit rate is high, with most
requests being satisfied with cached results, load on backend services and database can be
considerably reduced.
   The Internet also has a built in, multilevel caching infrastructure. Applications can exploit
this through the use of cache directives that are part of HTTP headers. These directives enable a
service to specify what information can be cached, for how long it should be cached, and
protocol for checking to see if a stale cache entry is still valid.

?

CHAPTER 7
__________________________

Asynchronous Messaging Systems
Introduction to Messaging
  Messaging Primitives

  Message Persistence

  Publish-Subscribe

Example: RabbitMQ
  Producer-Consumer example

  Persistent Messages

  Message Distribution

  Message Filtering

  RabbitMQ RPC
Clustering and Mirroring

Messaging Patterns
  Competing Consumers

  At Least Once Processing

  Poison Messages
Summary and Further Reading

?
CHAPTER 8
__________________________

Serverless Processing Systems
   Scalable systems experience widely varying patterns of usage. For some applications, load
may be high during business hours and low or non-existent during the evening. Other
applications, for example an online concert ticket sales system, might have low background
traffic for 99% of the time. When tickets for a major series of shows are released, the demand
can spike by 100x of normal load for a number of hours before dropping down to background
levels. Elastic load balancing, as described in Chapter XXX, is one approach for handling these
spikes. Another is cloud-based serverless computing, which well examine in this chapter.
The Attractions of Serverless
   The transition of major organizational IT systems from on-premise to public cloud
platforms deployments seems inexorable. Organizations from startups to Government agencies
to multinationals see clouds as digital transformation platforms and a foundational technology
to improve business continuity.
   Two of the great attractions of cloud platforms is their pay-as-you-go billing and ability to
rapidly scale up (and down) virtual resources to meet fluctuating workloads and data volumes.
This ability to scale of course doesnt come for free. Your applications need to be architected to
leverage the scalable services provided by cloud platforms. And of course, as we discussed in
Chapter XXX, cost and scale are indelibly connected. The more resources a system utilizes for
extended periods, the larger the cloud bills will be at the end of the month.
   And monthly clouds bills can be big. Really big. Even worse, unexpectedly big! Cases of
sticker shock for significant cloud overspend are rife  in one survey 69% of respondents
regularly overspent on their cloud budget by more than 25%.  One well known case spent
$500K on an Azure task before it was noticed. Reasons attributed for overspending are many,
including lack of deployment of auto-scaling solutions, poor long-term capacity planning, and
inadequate exploitation of cloud architectures leading to sub-optimal system footprints.
   On a cloud platform, architects are confronted with a myriad of architectural decisions.
These are broad, in terms of the overall architectural pattern or style the systems adopts  e.g.
microservices, n-tier, event driven  and narrow, specific to individual components and the
cloud services that the system is built upon. In this sense, architecturally significant decisions
pervade all aspects of the system design and deployment on the cloud. And the consequences of
these decisions are highly apparent when you receive your monthly cloud spending bill.
   Traditionally, cloud applications have been deployed on an Infrastructure-as-a-Service (IaaS)
platform utilizing virtual machines (VMs). In this case, you pay for the resources you deploy
regardless of how highly utilized they are. If load increases, the application spins up new virtual
machines to increase capacity, typically using the cloud-provided load balancing service. Your
costs are essentially proportional to the type of VMs you choose, the duration they are deployed
for, and the amount of data the application stores and transmits.
   Since around 2016, major cloud providers have offered an alternative to explicitly
provisioning virtual processing resources. Known as serverless platforms, they do not require any
compute resources to be statically provisioned. Using technologies such as AWS Lambda or
Google App Engine (GAE), the application code is loaded and executed on demand, when
requests arrive. If there are no active requests, there are essentially no resources in use and no
charges to meet. Serverless platforms also manage autoscaling (up and down) for you. As
simultaneous requests arrive, additional processing capacity is created to handle requests and,
ideally, provide consistently low response times. When request loads drop, additional processing
capacity is decommissioned, and no charges are incurred.
   Every serverless platform varies in the details of its implementation. For example, a limited
number of mainstream programming languages and application server frameworks are typically
supported. Platforms also provide several configuration settings that can be used to balance
performance, scalability and costs. In general, costs are proportional to the type of server
instance chosen to execute a request, the number of requests and processing duration for each
request, and/or how long each application server instance remains resident on the serverless
infrastructure.
   Welcome to the world of cloud deployments. Every platform is proprietary and different is
subtle ways. The devil is, as usual, in the details. So lets explore some of those devilish details
for the Google App Engine and AWS Lambda platforms
Google App Engine
  The Basics
   Google App Engine (GAE) was the first offering from Google as part of what is now
known as the Google Cloud Platform. It has been in general release since 2011, and enables
developers to upload and execute HTTP-based application services on Googles managed cloud
infrastructure.
   GAE supports developing applications in Go, Java, Python,  Node.js,  PHP, .NET, and
Ruby. To build an application on GAE, developers can utilize common HTTP-based
application frameworks that are built with the GAE runtime libraries provided by Google. For
example, in Python, applications can utilize Flask, Django and web2py, and in Java the primary
supported platform is servlets built on the Jetty JEE web container.
   Application execution is managed dynamically by GAE, which launches compute resources
to meet request demand. Applications generally access a managed persistent storage platform
such as Googles Firestore  or Google Cloud SQL , or interact with a messaging service like
Googles Cloud PubSub .
   GAE comes in two flavors, known as the standard environment and the flexible
environment. The basic difference is that the standard environment is more closely managed by
GAE, with development restrictions in terms of language versions supported, but is able to
scale rapidly in response to increased loads. The flexible environment, as its name hints at, gives
more options in terms of development capabilities that can be used, but is not as suitable to
rapid scaling. The next two subsections expand on these two alternatives, and in the rest of
chapter, well focus on the highly scalable standard environment.
  GAE Standard Environment
   In the standard environment, developers upload their application code to a GAE project
that is associated with a base URL. This code must define HTTP endpoints that can be invoked
by clients making requests to the project URL. When a request is received, GAE will route it to
a processing instance to execute the application code. These are known as resident instances for
the application and are the major component of the cost incurred for utilizing GAE.
   Each project configuration can specify a collection of parameters that control when GAE
loads a new instance and invokes a resident instance. The two simplest settings control the
minimum and maximum instances that GAE will have resident. The minimum can be zero,
which is perfect for applications that have periods of inactivity, as this incurs no costs. When a
request arrives, GAE dynamically loads an application instance and invokes the processing for
the endpoint. Multiple simultaneous requests can be sent to the same instance, up to some
configured limit (more on this when we discuss auto-scaling). GAE will then load additional
instances on demand until the specified maximum instances value is reached. By setting the
maximum, an application can put a lid on costs, albeit with the potential for increased latencies
if load continues to grow.
   Standard environment applications can be built in Go, Java, Python,  Node.js,  PHP, and
Ruby.  As GAE itself is responsible for loading the runtime environment for an application, it
restricts the supported versions  to a small number per programming language. The language
used also affects the time to load a new instance on GAE. For example, a lightweight runtime
environment such as Go will start on a new instance in less than a second. In comparison, a
heavier weight Java Virtual Machine is of the order of 1-3 seconds on average. This load time is
also influenced by the number of external libraries that the application incorporates.
    Hence, while there is variability across languages, loading new instances is uniformly fast.
Much faster than booting a virtual machine. This makes the standard environment extremely
well suited for applications that experience rapid spikes in load. GAE is able to quickly add new
resident instances as request volumes increase.  Requests are dynamically routed to instances
based on load, and hence assume a purely stateless application model to support effective load
distribution. Subsequently, instances are released with little delay once the load drops, again
reducing costs.
   GAE is an extremely powerful platform for scalable applications, and one well explore in
much more detail in the case study later in this chapter.

  GAE Flexible Environment

  AutoScaling
   Autoscaling is an option that you specify in the app.yaml file that is passed to GAE when
you upload your server code. An autoscaled application is managed by GAE according to a
collection of default parameter values, which you can override in your app.yaml. The basic
scheme is shown in Figure 1.
   GAE basically manages the number of deployed server instances for an application
based on incoming traffic load. If there are no incoming requests, then GAE will not
schedule any instances and you will pay nothing. When a request arrives, GAE deploys an
instance to process the request.
   Deploying an instance can take anything from a few 100 milliseconds to a few seconds
depending on the programming language you are using. This means latency can be high if
there are no resident instances. To mitigate the instance loading latency effects, you can
specify a minimum number of instances to keep available for processing requests. This of
course costs money.
   As the request load grows, the GAE scheduler will dynamically load more instances to
handle requests. Three parameters control precisely how this operates, namely:
   Target CPU Utilization:	Sets the CPU utilization threshold above which more
instances will be started to handle traffic. The range is 0.5 (50%) to 0.95 (95%). The default
is 0.6 (60%).
   Maximum Concurrent Requests: Sets the maximum number of concurrent requests an
instance can accept before the scheduler spawns a new instance. The default value is 10,
and the maximum is 80. The documentation doesnt state the minimum allowed value, but
presumably 1 would define a single-threaded service.
   Target Throughput Utilization:	This is used in conjunction with the value specified
for maximum concurrent requests to specify when a new instance is started. The range is
0.5 (50%) to 0.95 (95%). The default is 0.6 (60%). It works like this - when the number of
concurrent requests for an instance reaches a value equal to maximum concurrent requests
value multiplied by the target throughput utilization, the scheduler tries to start a new
instance.
   Got that? ??
   So by default, an instance will handle 10 x 0.6 = 6 concurrent requests before a new
instance is created. And if these 6 (or less) requests cause the CPU utilization for an
instance to go over 60%, the scheduler will also try and create a new instance.
   I think.

   Figure 1 GAE Autoscaling
   But wait, theres more!
   You can also specify values to control when GAE scales instances up based on the time
requests spend in the request pending queue, waiting to be dispatched to an instance for
processing. This maximum pending latency parameter is the maximum amount of time that
App Engine should allow a request to wait in the pending queue before starting additional
instances to handle requests to reduce latency. The default value is 30ms. The lower the
value, the quicker an application will scale up. And the more it will probably cost you.
   Theres also a minimum pending latency parameter, with a default value of zero. If you
are brave, how the minimum and maximum values work together is explained here.
   These autoscaling parameter settings give us the ability to fine tune a services behavior
to balance performance and cost. Which is great, isnt it?

AWS Lambda
Case Study: Balancing Scalability and Costs
  The Basic Conundrum
  Dont Accept the Defaults
  Choosing Parameter Values
  Comparing Configurations
  Lessons Learned
Summary and Further reading

?
ABOUT THE AUTHOR

   Insert author bio text here. Insert author bio text here Insert author bio text here Insert
author bio text here Insert author bio text here Insert author bio text here Insert author bio text

?
?
   Random words section
   Database connections for a single database are also a limitation. Each open connection takes
database resources, and even if we restrict each server accessing the database to opening 50
connections maximum, the 4000 available on our GCP SQL server will be exhausted when we
deploy 80 application servers. Were hitting limits in scalability for a single database instance.

   Its usually recommended that clients follow something akin to an exponential backoff
algorithm as they see errors. The client blocks for a brief initial wait time on the first failure, but
as the operation continues to fail, it waits proportionally to 2n, where n is the number of failures
that have occurred. By backing off exponentially, we can ensure that clients arent hammering
on a downed server and contributing to the problem.

   Exponential backoff has a long and interesting history in computer networking.

   Furthermore, its also a good idea to mix in an element of randomness. If a problem with a
server causes a large number of clients to fail at close to the same time, then even with back off,
their retry schedules could be aligned closely enough that the retries will hammer the troubled
server. This is known as the thundering herd problem.

   We can address thundering herd by adding some amount of random jitter to each clients
wait time. This will space out requests across all clients, and give the server some breathing
room to recover.

   Another solution is to keep the per-session data in a database. Generally, this is bad for
performance because it increases the load on the database: the database is best used to store
information less transient than per-session data. To prevent a database from becoming a single
point of failure, and to improve scalability, the database is often replicated across multiple
machines, and load balancing is used to spread the query load across those replicas. Microsoft's
ASP.net State Server technology is an example of a session database. All servers in a web farm
store their session data on State Server and any server in the farm can retrieve the data.

       https://www.youtube.com/watch?v=eNliOm9NtCM
       https://engineering.fb.com/data-infrastructure/scribe/
       https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-
in-a-single-repository/fulltext
      https://www.pornhub.com/insights/2019-year-in-review
       https://kogo.iheart.com/content/2020-04-23-youtube-celebrates-15th-anniversary-by-
featuring-first-video-ever-posted/
       http://pcmuseum.tripod.com/comphis4.html
       https://www.internetsociety.org/internet/history-internet/brief-history-internet/
       https://en.wikipedia.org/wiki/Usenet
       https://en.wikipedia.org/wiki/History_of_the_World_Wide_Web
       https://www.nngroup.com/articles/100-million-websites/
       https://en.wikipedia.org/wiki/Dot-com_bubble
       https://www.internetworldstats.com/stats.htm
       https://blog.twitter.com/engineering/en_us/topics/infrastructure/2017/the-infrastructure-
behind-twitter-scale.html
       https://en.wikipedia.org/wiki/Sydney_Harbour_Bridge
       https://en.wikipedia.org/wiki/Sydney_Harbour_Tunnel
       https://en.wikipedia.org/wiki/Auckland_Harbour_Bridge
       https://en.wikipedia.org/wiki/Auckland_Harbour_Bridge#'Nippon_clip-ons'
       https://en.wikipedia.org/wiki/HipHop_for_PHP
       https://www.bloomberg.com/news/articles/2014-09-24/obamacare-website-costs-exceed-2-
billion-study-finds
       http://www.informationweek.com/healthcare/policy-and-regulation/oregon-dumps-failed-
health-insurance-exchange/d/d-id/1234875

       https://www.researchgate.net/publication/318049054_Chapter_2_Hyperscalability_-
_The_Changing_Face_of_Software_Architecture
       https://en.wikipedia.org/wiki/Flask_(web_framework)
       Mark Richards and Neal Ford, Fundamentals of Software Architecture: An Engineering
Approach 1st Edition, OReilly Media, 2020
       https://aws.amazon.com/ec2/instance-types/
       https://en.wikipedia.org/wiki/Reverse_proxy
       https://redis.io/
       https://memcached.org/
       https://www.allthingsdistributed.com/2019/08/modern-applications-at-aws.html
       https://samnewman.io/patterns/architectural/bff/
       https://en.wikipedia.org/wiki/Intel_80386
       https://en.wikipedia.org/wiki/Amdahl%27s_law
       https://en.wikipedia.org/wiki/Dining_philosophers_problem
       https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/package-summary.html
       Except Vector and HashTable, which are legacy classes, thread safe and slow!
       https://en.wikipedia.org/wiki/Wavelength-
division_multiplexing#:~:text=In%20fiber%2Doptic%20communications%2C%20wavelength,%2C%20
colors)%20of%20laser%20light.
       https://en.wikipedia.org/wiki/Core_router
       https://en.wikipedia.org/wiki/Tier_1_network#List_of_Tier_1_networks
       https://www.google.com/intl/en/ipv6/statistics.html
       http://www.opengroup.org/dce/
       http://www.corba.org
       https://docs.oracle.com/javase/9/docs/specs/rmi/index.html
       http://erlang.org/doc/man/rpc.html
       https://golang.org/pkg/net/rpc/
       Fielding, Roy Thomas (2000). "Architectural Styles and the Design of Network-based Software
Architectures". Dissertation. University of California, Irvine.
       https://en.wikipedia.org/wiki/Packet_loss
  https://medium.com/baseds/modes-of-failure-part-1-6687504bfed6#
       https://en.wikipedia.org/wiki/Two_Generals%27_Problem
       Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson. 1985. Impossibility of distributed
consensus with one faulty process. J. ACM 32, 2 (April 1985), 374382.
       https://medium.com/@chrshmmmr/consensus-in-blockchain-systems-in-short-691fc7d1fefe
       https://ieeexplore.ieee.org/abstract/document/8123011
       www.ntp.org

       https://chrony.tuxfamily.org/
       https://engineering.fb.com/production-engineering/ntp-service/
       https://en.wikipedia.org/wiki/Roy_Fielding
       https://app.swaggerhub.com/help/tutorials/openapi-3-tutorial
       https://app.swaggerhub.com
       Node.js is a notable exception here as it is single threaded. However, it employs an
asynchronous programming model for blocking I-Os that supports handling many simultaneous
requests.
       https://tools.ietf.org/html/rfc6265
       https://www.oracle.com/java/technologies/java-ee-glance.html
       https://expressjs.com/
       https://palletsprojects.com/p/flask/
       http://tomcat.apache.org/
       See https://tomcat.apache.org/tomcat-9.0-doc/config/executor.html for default Tomcat
Executor configuration settings
       https://en.wikipedia.org/wiki/Java_Management_Extensions
       https://en.wikipedia.org/wiki/JConsole
       https://github.com/javamelody/javamelody/wiki
       https://en.wikipedia.org/wiki/OSI_model
       http://cbonte.github.io/haproxy-dconv/2.3/intro.html#3.3.5
       http://cbonte.github.io/haproxy-dconv/2.3/intro.html#3.3.6
       https://www.nginx.com/blog/microservices-at-netflix-architectural-best-practices/
       https://blog.twitter.com/engineering/en_us/topics/infrastructure/2017/the-infrastructure-
behind-twitter-scale.html
       https://memcached.org/
       https://redis.io/
       https://www.ehcache.org/documentation/3.3/caching-patterns.html#cache-aside
       https://www.alachisoft.com/resources/docs/ncache/prog-guide/server-side-api-
programming.html
        https://aws.amazon.com/dynamodb/dax/
       http://www.squid-cache.org/
       https://varnish-cache.org/
       https://cloud.google.com/firestore, formerly known as Google Cloud DataStore.
       https://cloud.google.com/sql/docs
       https://cloud.google.com/pubsub
       https://cloud.google.com/appengine/docs/the-appengine-environments
 Building Scalable
Distributed Systems

  Ian Gorton

   Copyright  2021 Ian Gorton
   All rights reserved.
   ISBN:

DEDICATION

   Insert dedication text here. Insert dedication text here. Insert dedication text here. Insert
dedication text here. Insert dedication text here. Insert dedication text here. Insert dedication
text here. Insert dedication text here. Insert dedication text here. Insert dedication text here.

TABLE OF CONTENTS
   Introduction to Scalable Systems	10
   What is Scalability?	10
   System scale in early 2020s: Examples	12
   How did we get here? A short history of system growth	13
   The 1980s	13
   1990-1995	14
   1996-2000	14
   2000-2006	14
   2007-2020 (today)	15
   Scalability Basic Design Principles	15
   Scalability and Costs	17
   Summary	19
   References	19
   Distributed Systems Architectures: A Whirlwind Tour	20
   Basic System Architecture	20
   Scale Out	22
   Scaling the Database with Caching	23
   Distributing the Database	25
   Multiple Processing Tiers	26
   Increasing Responsiveness	28
   Summary and Further Reading	30
   An Overview of Concurrent Systems	31
   Why Concurrency?	31
   Threads in Java	33
   Order of Thread Execution	35
   Problems with Thread  Race Conditions	36
   Problems with Thread  Deadlock	39
   Thread States	43
   Thread Coordination	44
   Thread Pools	47
   Barrier Synchronization	49
   Thread-Safe Collections	51
   Summary and Further Reading	52
   Exercises	52
   Distributed Systems Fundamentals	54
   Communications Basics	54
   Communications Hardware	55
   Communications Software	57
   Remote Method Invocation	60
   Partial Failures	64
   Consensus in Distributed Systems	68
   Time in Distributed Systems	70
   Summary and Further Reading	72
   Application Services	73
   Service Design	73
   Application Programming Interface (API)	73
   Designing Services	75
   State Management	77
   Applications Servers	80
   Horizontal Scaling	82
   Load Balancing	83
   Load Distribution Policies	84
   Health Monitoring	84
   Elasticity	84
   Session Affinity	85
   Summary and Further Reading	87
   Caching	88
   Application Caching	88
   Web Caching	92
   Cache-Control	93
   Expires and Last-Modified	93
   Etag	93
   Summary and Further Reading	95

ACKNOWLEDGMENTS

   Insert acknowledgments text here. Insert acknowledgments text here. Insert
acknowledgments text here. Insert acknowledgments text here. Insert acknowledgments text
here. Insert acknowledgments text here. Insert acknowledgments text here. Insert
acknowledgments text here. Insert acknowledgments text here. Insert acknowledgments text
here.

CHAPTER 1
__________________________
Introduction to Scalable Systems
   The last 20 years have seen unprecedented growth in the size, complexity and capacity of
software systems. This rate of growth is hardly likely to slow down in the next 20 years  what
these future systems will look like is close to unimaginable right now. The one thing we can
guarantee is that more and more software systems will need to be built with constant growth -
more requests, more data, more analysis - as a primary design driver.
   Scalable is the term used in software engineering to describe software systems that can
accommodate growth. In this chapter we will explore what precisely is meant by the ability to
scale  known, not surprisingly, as scalability. Well also describe a few examples that put hard
numbers on the capabilities and characteristics of contemporary applications and give a brief
history of the origins of the massive systems we routinely build today. Finally, we will describe
two general principles for achieving scalability that will recur in various forms throughout the
rest of this book and examine the indelible link between scalability and cost.
What is Scalability?
   Intuitively, scalability is a pretty straightforward concept. If we ask Wikipedia for a
definition, it tells us scalability is the property of a system to handle a growing amount of work
by adding resources to the system. We all know how we scale a highway system  we add more
traffic lanes so it can handle a greater number of vehicles. Some of my favorite people know
how to scale beer production  they add more capacity in terms of the number and size of
brewing vessels, the number of staff to perform and manage the brewing process, and the
number of kegs they can fill with tasty fresh brews. Think of any physical system  a transit
system, an airport, elevators in a building  and how we increase capacity is pretty obvious.
   Unlike physical systems, software is somewhat amorphous. It is not something you can point
at, see, touch, feel, and get a sense of how it behaves internally from external observation. Its a
digital artifact. At its core, the stream of 1s and 0s that make up executable code and data are
hard for anyone to tell apart. So, what does scalability mean in terms of a software system?
   Put very simply, and without getting into definition wars, scalability defines a software
systems capability to handle growth in some dimension of its operations. Examples of
operational dimensions are:

*	the number of simultaneous user or external (e.g. sensor) requests a system can process
*	the amount of data a system can effectively process and manage
*	the value that can be derived from the data a system stores

   For example, imagine a major supermarket chain is rapidly opening new stores and
increasing the number of self-checkout kiosks in every store. This requires the core supermarket
software systems to:

*	Handle increased volume from item sale scanning without decreased response time.
Instantaneous responses to item scans are necessary to keep customers happy.
*	Process and store the greater data volumes generated from increased sales. This data is
needed for inventory management, accounting, planning and likely many other
functions.
*	Derive real-time (e.g. hourly) sales data summaries from each store, region and country
and compare to historical trends. This trend data can help highlight unusual events in
regions (e.g. unexpected weather conditions, large crowds at events, etc.) and help the
stores affected quickly respond.
*	Evolve the stock ordering prediction subsystem to be able to correctly anticipate sales
(and hence the need for stock reordering) as the number of stores and customers grow

   These dimensions are effectively the scalability requirements of a system. If, over a year, the
supermarket chain opens 100 new stores and grows sales by 400 times (some of the new stores
are big!), then the software system needs to scale to provide the necessary processing capacity to
enable the supermarket to operate efficiently. If the systems dont scale, we could lose sales as
customers are unhappy. We might hold stock that will not be sold quickly, increasing costs. We
might miss opportunities to increase sales by responding to local circumstances with special
offerings. All these reduce customer satisfaction and profits. None are good for business.
   Successfully scaling is therefore crucial for our imaginary supermarkets business growth, and
is in fact the lifeblood of many modern internet applications. But for most business and
Government systems, scalability is not a primary quality requirement in the early stages of
development and deployment. New features to enhance usability and utility become the drivers
of our development cycles. As long as performance is adequate under normal loads, we keep
adding user-facing features to enhance the systems business value.
   Still, its not uncommon for systems to evolve into a state where enhanced performance and
scalability become a matter of urgency, or even survival. Attractive features and high utility
breed success, which brings more requests to handle and more data to manage. This often
heralds a tipping point, where design decisions that made sense under light loads are now
suddenly technical debt. External trigger events often cause these tipping points  look in the
March/April 2020 media at the many reports of Government Unemployment and supermarket
online ordering sites crashing under demand caused by the coronavirus pandemic.
   Increasing a systems capacity in some dimension by increasing resources is commonly called
scaling up or scaling out  well explore the difference between these later. In addition, unlike
physical systems, it is often equally important to be able to scale down the capacity of a system to
reduce costs. The canonical example of this is Netflix, which has a predictable regional diurnal
load that it needs to process. Simply, a lot more people are watching Netflix in any geographical
region at 9pm than are at 5am. This enables Netflix to reduce its processing resources during
times of lower load. This saves the cost of running the processing nodes that are used in the
Amazon cloud, as well as societally worthy things such as reducing data center power
consumption. Compare this to a highway. At night when few cars are on the road, we dont
retract lanes (except for repairs). The full road capacity is available for the few drivers to go as
fast as they like.
   Theres a lot more to consider about scalability in software systems, but lets come back to
these issues after examining the scale of some contemporary software systems circa 2020.
System scale in early 2020s: Examples
   Looking ahead in this technology game is always fraught with danger. In 2008 I wrote [1]:

   While petabyte datasets and gigabit data streams are today's frontiers for data-intensive applications, no
doubt 10 years from now we'll fondly reminisce about problems of this scale and be worrying about the difficulties
that looming exascale applications are posing.

   Reasonable sentiments, it is true, but exascale? Thats almost commonplace in todays world.
Google reported multiple exabytes of Gmail in 2014 , and by now, do all Google services
manage a yottabyte or more? I dont know. Im not even sure I know what a yottabyte is!
Google wont tell us about their storage, but I wouldnt bet against it. Similarly, how much data
do Amazon store in the various AWS data stores for their clients. And how many requests does,
say, DynamoDB process per second collectively, for all client applications supported. Think
about these things for too long and your head will explode.
   A great source of information that sometimes gives insights into contemporary operational
scales are the major Internet companys technical blogs. There are also Web sites analyzing
Internet traffic that are highly illustrative of traffic volumes. Lets take a couple of point in time
examples to illustrate a few things we do know today. Bear in mind these will look almost quaint
in a year or four.

*	Facebooks engineering blog describes Scribe , their solution for collecting, aggregating,
and delivering petabytes of log data per hour, with low latency and high throughput.
Facebooks computing infrastructure comprises millions of machines, each of which
generates log files that capture important events relating to system and application
health. Processing these log files, for example from a Web server, can give development
teams insights into their applications behavior and performance, and support fault
finding. Scribe is a custom buffered queuing solution that can transport logs from
servers at a rate of several terabytes per second and deliver them to downstream analysis
and data warehousing systems. That, my friends, is a lot of data!
*	You can see live Internet traffic for numerous services at www.internetlivestats.com. Dig
around and youll find statistics like Google handles around 3.5 billion search requests a
day, Instagram uploads about 65 million photos per day, and there is something like 1.7
billion web sites. It is a fun site with lots of information to amaze you. Note the data is
not really live, just estimates based on statistical analyses of multiple data sources.
*	In 2016 Google published a paper describing the characteristics of their code base .
Amongst the many startling facts reported is: The repository contains 86TBs of data, including
approximately two billion lines of code in nine million unique source files. Remember, this was
2016.

   Still, real, concrete data on the scale of the services provided by major Internet sites remain
shrouded in commercial-in-confidence secrecy. Luckily, we can get some deep insights into the
request and data volumes handled at Internet scale through the annual usage report from one
tech company. You can browse their incredibly detailed usage statistics here from 2019 here .
Its a fascinating glimpse into the capabilities of massive scale systems. Beware though, this is
Pornhub.com. The report is not for the squeamish. Heres one PG-13 illustrative data point 
they had 42 billion visits in 2019! Ill let interested readers browse the data in the report to their
hearts content. Some of the statistics will definitely make your eyes bulge!
How did we get here? A short history of system
growth
   I am sure many readers will have trouble believing there was civilized life without Internet
search, YouTube and social media. By coincidence, the day I type this sentence is the 15 year
anniversary of the first video being uploaded to YouTube . Only 15 years. Yep, it is hard for
even me to believe. Theres been a lot of wine under the bridge since then. I cant remember
how we survived!
   So, lets take a brief look back in time at how we arrived at the scale of todays systems. This
is from a personal perspective  one which started at college in 1981 when my class of 60 had
access to a shared lab of 8 state-of-the-art so-called microcomputers . By todays standards, micro
they were not.
  The 1980s
   An age dominated by mainframe and minicomputers. These were basically timeshared
multiuser systems where users interacted with the machines via dumb terminals. Personal
computers emerged in the early 1980s and developed throughout the decade to become useful
business and (relatively) powerful development machines. They were rarely networked however,
especially early in the decade. The first limited incarnation of the Internet emerged during this
time . By the end of the 1980s, development labs, universities and increasingly businesses had
email and access to exotic internet-based resources such as Usenet discussion forums   think of
a relatively primitive and incredibly polite reddit.
  1990-1995
   Personal computers and networking technology, both LANs and WANS, continued to
improve dramatically through this period. This created an environment ripe for the creation of
the World Wide Web (WWW) as we know it today. The catalyst was the HTTP/HTML
technology that had been pioneered at CERN by Tim Berners-Lee  during the 1980s. In 1993
CERN made the WWW technology available on a royalty-free basis. And the rest is history  a
platform for information sharing and money-making had been created. By 1995, the number of
web sites was tiny, but the seeds of the future were planted with companies like Yahoo! in 1994
and Amazon and eBay in 1995
  1996-2000
   During this period, the number of web sites grew from around 10,000 to 10 million , a truly
explosive growth period. Networking bandwidth and access also grew rapidly, with initially dial-
up modems for home users (yep, dial-up) and then early broadband technologies becoming
available.
   This surge in users with Internet access heralded a profound change in how we had to think
about building systems. Take for example a retail bank. Before providing online services, it was
possible to accurately predict the loads the banks business systems would experience. You
knew how many people worked in the bank and used the internal systems, how many
terminals/PCs were connected to the banks networks, how many ATMs you had to support,
and the number and nature of connections to other financial institutions. Armed with this
knowledge, we could build systems that support say a maximum of 3000 concurrent users, safe
in the knowledge that this number could not be exceeded. Growth would also be relatively slow,
and probably most of the time (eg outside business hours) the load would be a lot less than the
peak. This made our software design decisions and hardware provisioning a lot easier.
   Now imagine our retail bank decides to let all customers have Internet banking access. And
the bank has 5 million customers. What is our maximum load now? How will load be dispersed
during a business day? When are the peak periods? What happens if we run a limited time
promotion to try and sign up new customers? Suddenly our relatively simple and constrained
business systems environment is disrupted by the higher average and peak loads and
unpredictability you see from Internet-based user populations.
   During this period, companies like Amazon, eBay, Google, Yahoo! and the like were
pioneering many of the design principles and early versions of advanced technologies for highly
scalable systems. They had to, as their request loads and data volumes were growing
exponentially.
  2000-2006
   The late 1990s and early 2000s saw massive investments in, and technological innovations
from so called dot com companies, all looking to provide innovative and valuable online
businesses. Spending was huge, and not all investments were well targeted. This led to a little
event called the dot com crash  during 2000/2001. By 2002 the technology landscape was
littered with failed investments  anyone remember Pets.Com? Nope. Me neither. About 50%
of dot coms disappeared during this period. Of those that survived, albeit with much lower
valuations, many have become the staples we all know and use today.
   The number of web sites grew from around 10 to 80 million during this period, and new
service and business models emerged. In 2005, YouTube was launched. 2006 saw Facebook
become available to the public. In the same year, Amazon Web Services, which had low key
beginnings in 2004, relaunched with its S3 and EC2 services. The modern era of Internet-scale
computing and cloud-hosted systems was born.
  2007-2020 (today)
   We now live in a world with nearly 2 billion web sites, of which about 20% are active. There
are something like 4 billion Internet users . Huge data centers operated by public cloud
operators like AWS, GCP and Azure, along with a myriad of private data centers, for example
Twitters operational infrastructure , are scattered around the planet. Clouds host millions of
applications, with engineers provisioning and operating their computational and data storage
systems using sophisticated cloud management portals. Powerful, feature-rich cloud services
make it possible for us to build, deploy and scale our systems literally with a few clicks of a
mouse. All you must do is pay your cloud provider bill at the end of the month.
   This is the world that this book targets. A world where our applications need to exploit the
key principles for building scalable systems and leverage highly scalable infrastructure platforms.
Bear in mind, in modern applications, most of the code executed is not written by your
organization. It is part of the containers, databases, messaging systems and other components
that you compose into your application through API calls and build directives. This makes the
selection and use of these components at least as important as the design and development of
your own business logic. They are architectural decisions that are not easy to change.
Scalability Basic Design Principles
   As we have already discussed, the basic aim of scaling a system is to increase its capacity in
some application-specific dimension. A common dimension is increasing the number of
requests that a system can process in a given time period. This is known as the systems
throughput. Lets use an analogy to explore two basic principles we have available to us for
scaling our systems and increasing throughput.
   In 1932, one of the worlds great icons, the Sydney Harbor Bridge , was opened. Now it is a
fairly safe assumption that traffic volumes in 2020 are somewhat higher than in 1932. If you
have driven over the bridge at peak hour in the last 30 years, then you know that its capacity is
exceeded considerably every day. So how do we increase throughput on physical infrastructures
such as bridges?
   This issue became very prominent in Sydney in the 1980s, when it was realized that the
capacity of the harbor crossing had to be increased. The solution was the rather less iconic
Sydney Harbor tunnel , which essentially follows the same route underneath the harbor. This
provides 4 more lanes of traffic, and hence added roughly 1/3rd more capacity to harbor
crossings. In not too far away Auckland, their harbor bridge  also had a capacity problem as it
was built in 1959 with only 4 lanes. In essence, they adopted the same solution as Sydney,
namely, to increase capacity. But rather than build a tunnel, they ingeniously doubled the
number of lanes by expanding the bridge with the hilariously named  Nippon Clipons , which
widened the bridge on each side. Ask a Kiwi to say Nippon Clipons and you will understand
why this is funny.
   These examples illustrate the first strategy we have in software systems to increase capacity.
We basically replicate the software processing resources to provide more capacity to handle
requests and thus increase throughput, as shown in Figure 1. These replicated processing
resources are analogous to the lane ways on bridges, providing a mostly independent processing
pathway for a stream of arriving requests. Luckily, in cloud-based software systems, replication
can be achieved at the click of a mouse, and we can effectively replicate our processing
resources thousands of times. We have it a lot easier than bridge builders in that respect.

   Figure 1 Increasing Capacity through Replication
   The second strategy for scalability can also be illustrated with our bridge example. In Sydney,
some observant person realized that in the mornings a lot more vehicles cross the bridge from
north to south, and in the afternoon we see the reverse pattern. A smart solution was therefore
devised  allocate more of the lanes to the high demand direction in the morning, and sometime
in the afternoon, switch this around. This effectively increased the capacity of the bridge
without allocating any new resources  we optimized the resources we already had available.
   We can follow this same approach in software to scale our systems. If we can somehow
optimize our processing, by maybe using more efficient algorithms, adding extra indexes in our
databases to speed up queries, or even rewriting our server in a faster programming language,
we can increase our capacity without increasing our resources. The canonical example of this is
Facebooks creation of (the now discontinued)  HipHop for PHP , which increased the speed
of Facebooks web page generation by up to 6 times by compiling PHP code to C++.
   Well revisit these two design principles  namely replication and optimization - many times
in the remainder of this book. You will see that there are many complex implications of
adopting these principles that arise from the fact that we are building distributed systems.
Distributed systems have properties that make building scalable systems interesting, where
interesting in this context has both positive and negative connotations.
Scalability and Costs
   Lets take a trivial hypothetical example to examine the relationship between scalability and
costs. Assume we have a Web-based (e.g. web server and database) system that can service a
load of 100 concurrent requests with a mean response time of 1 second. We get a business
requirement to scale up this system to handle 1000 concurrent requests with the same response
time. Without making any changes, a simple load test of this system reveals the performance
shown in Figure 2 (left). As the request load increases, we see the mean response time steadily
grow to 10 seconds with the projected load. Clearly this is not scalable and cannot satisfy our
requirements in its current deployment configuration.

   Figure 2 Scaling an application. (Left)  non-scalable performance. (Right)  scalable performance
   Clearly some engineering effort is needed in order to achieve the required performance.
Figure 2 (right) shows the systems performance after it has been modified. It now provides the
specified response time with 1000 concurrent requests. Hence, we have successfully scaled the
system. Party time!
   A major question looms however. Namely, how much effort and resources were required to
achieve this performance? Perhaps it was simply a case of scaling up by running the Web server
on a more powerful (virtual) machine. Performing such reprovisioning on a cloud might take 30
minutes at most. Slightly more complex would be reconfiguring the system to scale out and run
multiple instances of the Web server to increase capacity. Again, this should be a simple, low
cost configuration change for the application, with no code changes needed. These would be
excellent outcomes.
   However, scaling a system isnt always so easy. The reasons for this are many and varied, but
heres some possibilities:
*	the database becomes less responsive with 1000 requests per second, requiring an
upgrade to a new machine
*	the Web server generates a lot of content dynamically and this reduces response time
under load. A possible solution is to alter the code to more efficiently generate the
content, thus reducing processing time per request.
*	the request load creates hot spots in the database when many requests try to access and
update the same records simultaneously. This requires a schema redesign and
subsequent reloading of the database, as well as code changes to the data access layer.
*	the Web server framework that was selected emphasized ease of development over
scalability. The model it enforces means that the code simply cannot be scaled to meet
the request load requirements, and a complete rewrite is required. Another framework?
Another programming language even?

   Theres a myriad of other potential causes, but hopefully these illustrate the increasing effort
that might be required as we move from possibility (1) to possibility (4).
   Now lets assume option (1), upgrading the database server, requires 15 hours of effort and a
thousand dollars extra cloud costs per month for a more powerful server. This is not
prohibitively expensive. And lets assume option (4), a rewrite of the Web application layer,
requires 10,000 hours of development due to implementing in a new language (e.g. Java instead
of Ruby). Options (2) and (3) fall somewhere in between options (1) and (4). The cost of 10,000
hours of development is seriously significant. Even worse, while the development is underway,
the application may be losing market share and hence money due to its inability to satisfy client
requests loads. These kinds of situations can cause systems and businesses to fail.
   This simple scenario illustrates how the dimensions of resource and effort costs are
inextricably tied to scalability. If a system is not designed intrinsically to scale, then the
downstream costs and resources of increasing its capacity to meet requirements may be massive.
For some applications, such as Healthcare.gov , these (more than $2 billion) costs are borne
and the system is modified to eventually meet business needs. For others, such as Oregons
health care exchange , an inability to scale rapidly at low cost can be an expensive ($303million)
death knell.
   We would never expect someone would attempt to scale up the capacity of a suburban home
to become a 50 floor office building. The home doesnt have the architecture, materials and
foundations for this to be even a remote possibility without being completely demolished and
rebuilt. Similarly, we shouldnt expect software systems that do not employ scalable
architectures, mechanisms and technologies to be quickly evolved to meet greater capacity
needs. The foundations of scale need to be built in from the beginning, with the recognition
that the components will evolve over time. By employing design and development principles
that promote scalability, we can more rapidly and cheaply scale up systems to meet rapidly
growing demands.
   Software systems that can be scaled exponentially while costs grow linearly are known as
hyperscale systems, defined as:
   Hyper scalable systems exhibit exponential growth in computational and storage capabilities while
exhibiting linear growth rates in the costs of resources required to build, operate, support and evolve the required
software and hardware resources.
    You can read more about hyperscale systems in this article  [3].

Summary
   The ability to scale an application quickly and cost-effectively should be a defining quality of
the software architecture of contemporary Internet-facing applications. We have two basic ways
to achieve scalability, namely increasing system capacity, typically through replication, and
performance optimization of system components. The rest of this book will delve deeply into
how these two basic principles manifest themselves in constructing scalable distributed systems.
Get ready for a wild ride.
References

1.	Ian Gorton, Paul Greenfield, Alex Szalay, and Roy Williams. 2008. Data-Intensive
Computing in the 21st Century. Computer 41, 4 (April 2008), 3032.
2.	Rachel Potvin and Josh Levenberg. 2016. Why Google stores billions of lines of code
in a single repository. Commun. ACM 59, 7 (July 2016), 7887.
3.	Ian Gorton (2017). Chapter 2. Hyperscalability  The Changing Face of Software
Architecture. 10.1016/B978-0-12-805467-3.00002-8.
?

CHAPTER 2
__________________________

Distributed Systems Architectures:
A Whirlwind Tour
   In this chapter well introduce some of the fundamental approaches to scaling a software
system. The type of systems this book is oriented towards are the internet-facing systems we all
utilize every day. Ill let you name your favorite. These systems accept requests from users
through Web and mobile interfaces, store and retrieve data based on user requests or events
(e.g. a GPS-based system), and have some intelligent features such as providing recommendations
or providing notifications based on previous user interactions.
   Well start with a simple system design and show how it can be scaled. In the process, several
concepts will be introduced that well cover in much more detail later in this book. Hence this
chapter just gives a broad overview of these concepts and how they aid in scalability  truly a
whirlwind tour!
Basic System Architecture
   Virtually all massive scale systems start off small and grow due to their success. Its common,
and sensible, to start with a development framework such as Ruby on Rails or Django or
equivalent, which promotes rapid development to get a system quickly up and running.  A
typical, very simple software architecture for starter systems which closely resembles what you
get with rapid development frameworks is shown in Figure 3. This comprises a client tier,
application service tier, and a database tier. If you use Rails or equivalent, you also get a
framework which hardwires a Model-View-Controller (MVC) pattern for Web application
processing and an Object-Relational Mapper (ORM) that generates SQL queries.
   With this architecture, users submit requests to the application from their mobile app or
Web browser. The magic of Internet networking (see Chapter 4) delivers these requests to the
application service which is running on a machine hosted in some corporate or commercial
cloud data center. Communications uses a standard network protocol, typically HTTP.
   The application service runs code that supports an application programming interface (API)
that clients use to format data and send HTTP requests to. Upon receipt of a request, the
service executes the code associated with the requested API. In the process, it may read from or
write to a database, depending on the semantics of the API. When the request is complete, the
service sends the results to the client to display in their app or browser.

   Figure 3 Basic Multi-Tier Distributed Systems Architecture
   Many systems conceptually look exactly like this. The application service code exploits a
server execution environment that enables multiple requests from multiple users to be
processed simultaneously. Theres a myriad of these application server technologies  JEE and
Spring for Java, Flask for Python  that are widely used in this scenario. This approach leads to
what is generally known as a monolithic architecture . Monoliths grow in complexity as the
application becomes more feature rich. This eventually makes it hard to modify and test rapidly,
and the execution footprint can become extremely heavyweight as all the API implementations
run in the same application service.
   Still, if request loads stay relatively low, this application architecture can suffice. The service
has the capacity to process requests with consistently low latency. But if request loads keep
growing, this means latencies will grow as the service has insufficient CPU/memory capacity for
the concurrent request volume and hence requests will take longer to process. In these
circumstances, our single server is overloaded and has become a bottleneck.
   In this case, the first strategy for scaling is usually to scale up the application service
hardware. For example, if your application  is running on AWS, you might upgrade your server
from a modest t3.xlarge instance with 4 (virtual) CPUs and 16GBs of memory to a t3.2xlarge
instance which doubles the number of CPUs and memory available for the application .
   Scale up is simple. It gets many real-world applications a long way to supporting larger
workloads. It obviously just costs more money for hardware, but thats scaling for you.
   Its inevitable however for many applications the load will grow to a level which will swamp
a single server node, no matter how many CPUs and how much memory you have. Thats when
you need a new strategy  namely scaling out, or horizontal scaling, that we touched on in
Chapter 1.
Scale Out
   Scaling out relies on the ability to replicate a service in the architecture and run multiple
copies on multiple server nodes. Requests from clients are distributed across the replicas so that
in theory, if we have N replicas, each server node processes {#requests/N}. This simple
strategy increases an applications capacity and hence scalability.
   To successfully scale out an application, we need two fundamental elements in our design.
As illustrated in Figure 4, these are:

1)	Load balancer: All user requests are sent to a load balancer, which chooses a service
replica to process the request. Various strategies exist for choosing a target service, all
with the core aim of keeping each resource equally busy. The load balancer also relays
the responses from the service back to the client. Most load balancers belong to a class
of Internet components known as reverse proxies , which control access to server
resources for client requests. As an intermediary, reverse proxies add an extra network
hop for a request, and hence need to be extremely low latency to minimize the
overheads they introduce. There are many off-the-shelf load balancing solutions as well
as cloud-provider specific ones, and well cover the general characteristics of these in
much more detail in Chapter XXX.
2)	Stateless services: For load balancing to be effective and share requests evenly, the
load balancer must be free to send consecutive requests from the same client to different
service instances for processing. This means the API implementations in the services
must retain no knowledge, or state, associated with an individual clients session. When a
user accesses an application, a user session is created by the service and a unique session
identified is managed internally to identify the sequence of user interactions and track
session state. A classic example of session state is a shopping cart. To use a load balancer
effectively, the data representing the current contents of a users cart must be stored
somewhere  typically a data store  such that any service replica can access this state
when it receives a request as part of a user session. In Figure 4 this is labeled as a Session
Store.

   Figure 4 Scale out Architecture
   Scale out is attractive as, in theory, you can keep adding new (virtual) hardware and services
to handle increased request loads and keep request latencies consistent and low. As soon as you
see latencies rising, you deploy another server instance. This requires no code changes and
hence is relatively cheap  you just pay for the hardware you deploy.
   Scale out has another highly attractive feature. If one of the services fails, the requests it is
processing will be lost. But as the failed service manages no session state, these requests can be
simply reissued by the client and sent to another service instance for processing. This means the
application is resilient to failures in the service software and hardware, thus enhancing the
applications availability. Availability is a key feature of distributed systems, and one we will
discuss in depth in Chapter XXX.
   Unfortunately, as with any engineering solution, simple scaling out has limits. As you add
new service instances, the request processing capacity grows, potentially infinitely. At some
stage however, reality will bite and the capability of your single database to provide low latency
query responses will diminish. Slow queries will mean longer response times for clients. If
requests keep arriving faster than they are being processed, some system component will fail
due to resource exhaustion and clients will see exceptions and request timeouts. Essentially your
database has become a bottleneck that you must engineer away in order to scale your
application further.
Scaling the Database with Caching
   Scaling up by increasing the number of CPUs, memory and disks in a database server can go
a long way to scaling a system. For example, at the time of writing Google Cloud Platform can
provision a SQL database on a db-n1-highmem-96 node, which has 96 vCPUs, 624GB of memory,
30TBs of disk and can support 4000 connections. This will cost somewhere between $6K and
$16K per year, which sounds a good deal to me! Scaling up is a very common database
scalability strategy.
   Large databases need constant care and attention from highly skilled database administrators
to keep them tuned and running fast. Theres a lot of wizardry in this job  e.g. query tuning,
disk partitioning, indexing, on-node caching  and hence database administrators are valuable
people that you want to be very nice to. They can make you application services highly
responsive indeed.
   In conjunction with scale up, a highly effective approach is querying the database as
infrequently as possible in your services. This can be achieved by employing distributed caching
in the service tier. Caching stores recently retrieved and commonly accessed database results in
memory so they can be quickly retrieved without placing a burden on the database. For data that
is frequently read and changes rarely, your processing logic can be modified to first check a
distributed cache, such as a Redis  or memcached  store. These cache technologies are
essentially distributed Key-Value stores with very simple APIs. This scheme is illustrated in
Figure 5. Note that the Session Store from Figure 4 has disappeared. This is because we can use a
general-purpose distributed cache to store session identifiers along with application data.
   Accessing the cache requires a remote call from your service, but if the data you need is in
the cache, on a fast network this is far less expensive than querying the database instance.
Introducing a caching layer also requires your processing logic to be modified to check for
cached data. If what you want is not in the cache, your code must still query the database and
load the results into the cache as well as return it to the caller. You also need to decide when to
remove or invalidate cached results  this depends on your applications tolerance to serving
stale results to clients and the volume of data you cache.

   Figure 5 Introducing Distributed Caching
   A well-designed caching scheme can be absolutely invaluable in scaling a system. Caching
works great for data that rarely changes and is accessed frequently, such as inventory, event and
contact data. If you can handle a large percentage, like 80% or more, of read requests from your
cache, then you effectively buy extra capacity at your databases as they are not involved in
handling requests.
   Still, many systems need to rapidly access terabyte and larger data stores that make a single
database effectively prohibitive. In these systems, a distributed database is needed.
Distributing the Database
   There are more distributed database technologies around in 2020 than you probably want to
imagine. Its a complex area, and one well cover extensively later in Chapter XXX. In very
general terms, there are two major categories:

1)	Distributed SQL stores from major vendors such as Oracle and IBM. These enable
organizations to scale out their SQL database relatively seamlessly by storing the data
across multiple disks that are queried by multiple database engine replicas. These
multiple engines logically appear to the application as a single database, hence
minimizing code changes.
2)	Distributed so-called NoSQL stores from a whole array of vendors. These products use
a variety of data models and query languages to distribute data across multiple nodes
running the database engine, each with their own locally attached storage. Again, the
location of the data is transparent to the application, and typically controlled by the
design of the data model using hashing functions on database keys. Leading products in
this category are Cassandra, MongoDB and Neo4j.

   Figure 6 Scaling the Data Tier using a Distributed Database
   Figure 6 shows how our architecture incorporates a distributed database. As the data
volumes grow, a distributed database has features to enable the number of storage nodes to be
increased.  As nodes are added (or removed), the data managed across all nodes is rebalanced to
attempt to ensure the processing and storage capacity of each node is equally utilized.
   Distributed databases also promote availability. They support replicating each data storage
node so if one fails or cannot be accessed due to network problems, another copy of the data is
available. The models utilized for replication and the trade-offs these require (spoiler 
consistency) are covered in later chapters.
   If you are utilizing a major cloud provider, there are also two deployment choices for your
data tier. You can deploy your own virtual resources and build, configure, and administer your
own distributed database servers. Alternatively, you can utilize cloud-hosted databases. The
latter simplifies the administrative effort associated with managing, monitoring and scaling the
database, as many of these tasks essentially become the responsibility of the cloud provider you
choose. As usual, the no free lunch principle applies.
Multiple Processing Tiers
   Any realistic system that we need to scale will have many different services that interact to
process a request. For example, accessing a Web page on the Amazon.com web site can require
in excess of 100 different services being called before a response is returned to the user .
   The beauty of the stateless, load balanced, cached architecture we are elaborating in this
chapter is that we can extend the core design principles and build a multi-tiered application. In
fulfilling a request, a service can call one or more dependent services, which in turn are
replicated and load-balanced. A simple example is shown in Figure 7. There are many nuances
in how the services interact, and how applications ensure rapid responses from dependent
services. Again, well cover these in detail in later chapters.

   Figure 7 Scaling Processing Capacity with Multiple Tiers
   This design also promotes having different, load balanced services at each tier in the
architecture. For example, Figure 8 illustrates two replicated Internet-facing services that both
utilized a core service that provides database access. Each service is load balanced and employs
caching to provide high performance and reliability. This design is often used to provide a
service for Web clients and a service for mobile clients, each of which can be scaled
independently based on the load they experience. Its commonly called the Backend For
Frontend (BFF) pattern .

   Figure 8 Scalable Architecture with Multiple Services
   In addition, by breaking the application into multiple independent services, we can scale each
based on the service demand. If for example we see an increasing volume of requests from
mobile users and decreasing volumes from Web users, we can provision different numbers of
instances for each service to satisfy demand. This is a major advantage of refactoring monolithic
applications into multiple independent services, which can be separately built, tested, deployed
and scaled.
Increasing Responsiveness
   Most client application requests expect a response. A user might want to see all auction
items for a given product category or see the real estate that is available for sale in a given
location. In these examples, the client sends a request and waits until a response is received.
This time interval between sending the request and receiving the result is the latency of the
request. We can decrease latencies by using caching and precalculated responses, but many
requests will still result in a database access.
   A similar scenario exists for requests that update data in an application. If a user updates
their delivery address immediately prior to placing an order, the new delivery address must be
persisted so that the user can confirm the address before they hit the purchase button. The
latency in this case includes the time for the database write, which is confirmed by the response
the user receives.
   Some update requests however can be successfully responded to without fully persisting the
data in a database. For example, the skiers and snowboarders amongst you will be familiar with
lift ticket scanning systems that check you have a valid pass to ride the lifts that day. They also
record which lifts you take, the time you get on, and so on. Nerdy skier/snowboarders can then
use the resorts mobile app to see how many lifts they ride in a day.
   As a person waits to get on a lift, a scanner device validates the pass using an RFID chip
reader. The information about the rider, lift, and time are then sent over the Internet to a data
capture service operated by the ski resort. The lift rider doesnt have to wait for this to occur, as
the latency could slow down their loading process. Theres also no expectation from the lift
rider that they can instantly use their app to ensure this data has been captured. They just get on
the lift, talk smack with their friends, and plan their next run.
   Service implementations can exploit this type of scenario to improve responsiveness. The
data about the event is sent to the service, which acknowledges receipt and concurrently stores
the data in a remote queue for subsequent writing to the database. Writing a message to a queue
is much faster than writing to a database, and this enables the request to be successfully
acknowledged much more quickly. Another service is deployed to read messages from the
queue and write the data to the database. When the user checks their lift rides  maybe 3 hours
or 3 days later  the data has been persisted successfully. The basic architecture to implement
this approach is illustrated in Figure 9.

   Figure 9 Increasing Responsiveness with Queueing
   Whenever the results of a write operation are not immediately needed, an application can use
this approach to improve responsiveness and hence scalability. Many queueing technologies
exist that applications can utilize, and well discuss how these operate in later chapters. These
queueing platforms all provide asynchronous communications. A producer service writes to the
queue, which acts as temporary storage, while another consumer service removes messages from
the queue and makes the necessary updates to, in our example, a database that stores skier lift
ride details.
   The key is that the data eventually gets persisted. Eventually typically means a few seconds at
most but use cases that employ this design should be resilient to longer delays without
impacting the user experience.
Summary and Further Reading
   This chapter has provided a whirlwind tour of the major approaches we can utilize to scale
out a system as a collection of communicating services and distributed databases. Much detail
has been brushed over, and as you no doubt know, in software systems the devil is in the detail.
Subsequent chapters will therefore progressively start to explore these details, starting with
concurrent and distributed systems fundamentals. Next there are sections on the application
service and data management tiers as portrayed in the basic distributed systems architecture
blueprint described in this chapter.
   Another area this chapter has skirted around is the subject of software architecture. Weve
used the term services for distributed components in an architecture that implement application
business logic and database access. These services are independently deployed processes that
communicate using remote communications mechanisms such as HTTP. In architectural terms,
these services are most closely mirrored by those in the Service Oriented Architecture (SOA)
pattern, an established architectural approach for building distributed systems. A more modern
evolution of this approach revolves around microservices. These tend to be more cohesive,
encapsulated services that promote continuous development and deployment.
   Well touch on these differences in a later chapter. If youd like a much more in depth
discussion of these, and software architecture issues in general, then Mark Richards and Neal
Fords book is an excellent place to start.

Mark Richards and Neal Ford, Fundamentals of Software Architecture: An Engineering
Approach 1st Edition, OReilly Media, 2020

   Finally, theres a class of big data software architectures that address some of the issues that
come to the fore with very large data collections. One of the most prominent is data
reprocessing. This occurs when data that has already been stored and analyzed needs to be re-
analyzed due to code changes. This reprocessing may occurs due to software fixes, or the
introduction of new algorithms that can derive more insights from the original raw data. Theres
a good discussion of the Lambda and Kappa architectures, both of which are prominent in this
space, in this article.

   Jay Krepps, Questioning the Lambda Architecture,
https://www.oreilly.com/radar/questioning-the-lambda-architecture/
?
CHAPTER 3
__________________________

An Overview of Concurrent
Systems

   Scaling a system naturally involves adding multiple independently moving parts. We run our
servers on multiple machines, our databases across multiple storage nodes, all in the quest of
adding more capacity. Consequently, our solutions are distributed across multiple locations, with
each processing events concurrently.
   Any distributed system is hence by definition a concurrent system, even if each node is
processing events one at a time. The behavior of the various nodes has to be coordinated in
order to make the application behave as desired. As well see later, coordinating nodes in a
distributed system is fraught with dangers. Luckily, our industry has matured sufficiently to
provide complex, powerful software frameworks that hide many of these distributed system
nasties (most of the time!) from our applications.
   This chapter is concerned with concurrent behavior in our systems on a single node. By
explicitly writing our software to perform multiple actions concurrently, we can optimize the
processing on a single node, and hence increase our processing capacity both locally and system
wide. Well use the Java 7.0 concurrency capabilities for examples, as these are at a lower level of
abstraction than those introduced in Java 8.0. Knowing how concurrent systems operate closer
to the machine is useful foundational knowledge when building concurrent and distributed
systems.
   A final point. This chapter is a concurrency primer. It wont teach you everything you need
to know to build complex, high performance concurrent systems. It will also be useful if your
experience writing concurrent programs is rusty, or you have written concurrent code in another
programming language. The further reading section points you to more comprehensive
coverage of this topic for those who wish to delve deeper.
Why Concurrency?
   Think of a busy coffee shop. If everyone orders a simple coffee, then the barista can quickly
and consistently deliver each drink. Suddenly, the person in front of you orders a soy, vanilla, no
sugar, quadruple shot iced brew. Everyone in line sighs and starts reading their social media, In
two minutes the line is out of the door.
    Processing requests in Web applications is analogous to our coffee example. In a coffee
shop, we enlist the help of a new barista to simultaneously make coffees on a different machine
to keep the line length in control and serve customers quickly. In software, to make applications
responsive, we need to somehow process requests in our server an overlapping manner.
   In the good old days of computing, each CPU was only able to execute a single machine
instruction at any instant. If our server application runs on such a CPU, why do we need to
structure our software systems to execute multiple instructions concurrently? It all seems slightly
pointless.
   There is actually a very good reason. Virtually every program does more than just execute
machine instructions. For example, when a program attempts to read from a file or send a
message on the network, it must interact with the hardware subsystem (disk, network card) that
is peripheral to the CPU. Reading data from a modern hard disk takes around 10 milliseconds
(ms). During this time, the program must wait for the data to be available for processing.
   Now, even an ancient  CPU such as a circa 1988 Intel 80386  can execute more than 10
million instructions per second (mips). 10ms is 1/100th of a second. How many instructions
could our 80386 execute in 1/100th second. Do the math. Its a lot! A lot of wasted processing
capacity, in fact.
   This is how operating systems such as Linux can run multiple programs on a single CPU.
While one program is waiting for an input-output (I-O) event, the operating system schedules
another program to execute. By explicitly structuring our software to have multiple activities
that can be executed in parallel, the operating system can schedule tasks that have work to do
while others wait for I-O. Well see in more detail how this works with Java later in this chapter.
   In 2001, IBM introduced the worlds first multicore processor, a chip with two CPUs  see
Figure 10. Today, even my laptop has 16 CPUs, or cores as they are commonly known. With a
multicore chip, a software system that is structured to have multiple parallel activities can be
executed in parallel. Well, up the number of available cores, anyway. In this way, we can fully
utilize the processing resources on a multicore chip, and hence increase our applications
capacity.

   Figure 10 Simplified view of a multicore processor
   The primary way to structure a software system as concurrent activities is to use threads.
Every programming language has its own threading mechanism. The underlying semantics of all
these mechanisms are similar  there are only a few primary threading models in mainstream use
 but obviously the syntax varies by language. In the following sections, well see how threads
are supported in Java.
Threads in Java
   Every software process has a single thread of execution by default. This is the thread that the
operating system manages when it schedules the process for execution. In Java, the main()
function you specify as the entry point to your code defines the behavior of this thread. This
single thread has access to the programs environment and resources such as open file handles
and network connections. As the program calls methods in objects instantiated in the code, the
runtime stack is used to pass parameters and manage variable scopes. Standard programming
language run time stuff, basically. This is a sequential process.
   In your systems, you can use programming language features to create and execute additional
threads. Each thread is an independent sequence of execution and has its own runtime stack to
manage local object creation and method calls. Each thread also has access to the process
global data and environment. A simple depiction of this scheme is shown in Figure 11.

   Figure 11 Comparing a single and multithreaded process
   In Java, we can define a thread using a class that implements the Runnable interface and
defines a run() method. A simple example is depicted in Figure 12.
1.	class NamingThread implements Runnable {
2.
3.	  private String name;
4.
5.	 	public NamingThread(String threadName) {
6.		  name = threadName ;
7.	    System.out.println("Constructor called: " + threadName) ;
8.	  }
9.
10.	  public void run() {
11.		  //Display info about this  thread
12.	    System.out.println("Run called : " + name);
13.	    System.out.println(name + " : " + Thread.currentThread());
14.	    // and now terminate  ....
15.		}
16.	}
17.
   Figure 12 A Simple Java Thread class
   To execute the thread, we need to construct a Thread object using an instance of our
Runnable and call the start() method to invoke the code in its own execution context. This is
shown in Figure 13, along with the output of running the code. Note this example has two
threads  the main() thread and our own NamingThread. The main thread starts the
NamingThread, which executes asynchronously, and then waits for 1 second to give our
run() method in NamingThread ample time to complete. This order of execution can be seen
from examining the outputs in Figure 13.
   For illustration, we also call the static currentThread() method, which returns a string
containing:

*	The system generated thread identifier
*	The thread priority, which by default is 5 for all threads. Well cover priorities later.
*	The identifier of the parent thread  in this example both parent threads are main

   Note to instantiate a thread, we call the start() method, not the run() method we define
in the Runnable. The start() method contains the internal system magic to create the
execution context for a separate thread to execute (e.g. see Figure 11). If we call run() directly,
the code will execute, but no new thread will be created. The run() method will execute as part
of the main thread, just like any other Java method invocation that you know and love. You will
still have a single threaded code.
1.	public static void main(String[] args) {
2.
3.	  NamingThread name0 = new NamingThread("My first thread");
4.
5.	  //Create the thread
6.	  Thread t0 = new Thread (name0);
7.
8.	  // start the threads
9.	  t0.start();
10.
11.	  //delay the main thread for a second (1000 miliiseconds)
12.	  try {
13.	    Thread.currentThread().sleep(1000);
14.	  } catch (InterruptedException e) {
15.	      }
16.
17.	      //Display info about the main thread and terminate
18.	      System.out.println(Thread.currentThread());
19.	    }
20.
21.	===EXECUTION OUTPUT===
22.	Constructor called: My first thread
23.	Run called : My first thread
24.	My first thread : Thread[Thread-0,5,main]
25.	Thread[main,5,main]

Figure 13 Creating and executing a thread
   In the example, we use sleep() to make sure the NamimgThread will terminate before the
main thread. Coordinating two threads by delaying for an absolute time period (e.g. 1 second in
our example) is not a very robust mechanism. What if for some reason - a slower CPU, a long
delay reading disk, additional complex logic in the method  our thread doesnt terminate in the
expected time frame? In this case, main will terminate first  this is not we intend. In general, if
you are using absolute times for thread coordination, you are doing it wrong. Almost always.
Like 99.99999% of the time.
   A simple and robust mechanism for one thread to wait until another has completed its work
is to use the join() method. In Figure 13, we could replace the try-catch block with:

   t0.join();

   This method causes the calling thread (in our case, main) to block until the thread referenced
by t0 terminates. If the referenced thread has terminated before the call to join(), then the
method call returns immediately. In this way we can coordinate, or synchronize, the behavior of
multiple threads. Synchronization of multiple threads is in fact the major focus of rest of this
chapter.
Order of Thread Execution
   TL;DR The system scheduler (in Java, this lives in the JVM) controls the order of thread
execution. From the programmers perspective, the order of execution is non-deterministic. Get
used to that term, well use it a lot. The concept of non-determinism is fundamental to
understanding multithreaded code.
   Lets illustrate this by building on our earlier example. Instead of creating a single
NamingThread, lets create and start up a few. 3 in fact, as shown in Figure 14:
1.	NamingThread name0 = new NamingThread("thread0");
2.	      NamingThread name1 = new NamingThread("thread1");
3.	      NamingThread name2 = new NamingThread("thread2");
4.
5.	      //Create the threads
6.	      Thread t0 = new Thread (name0);
7.	      Thread t1 = new Thread (name1);
8.	      Thread t2 = new Thread (name2);
9.
10.	      // start the threads
11.	      t0.start();
12.	      t1.start();
13.	      t2.start();
14.
15.	===EXECUTION OUTPUT===
16.	Run called : thread0
17.	thread0 : Thread[Thread-0,5,main]
18.	Run called : thread2
19.	Run called : thread1
20.	thread1 : Thread[Thread-1,5,main]
21.	thread2 : Thread[Thread-2,5,main]
22.	Thread[main,5,main]
   Figure 14 Multiple threads exhibiting non-deterministic behavior
   The output shown in Figure 14 is a sample from just one execution. You can see the code
starts three threads sequentially, namely t0, t1 and t2 (lines 11-13). Looking at the execution
trace, we see thread t0 completes (line 17) before the others start. Next t2s run() method is
called (line 18) followed by t1s run() method, even though t1 was started before t2. Thread t1
then runs to completion (line 20) before t1, and eventually the main thread and the program
terminate.
   This is just one possible order of execution. If we run this program again, we will almost
certainly see a different execution trace. This is because the JVM scheduler is deciding which
thread to execute, and for how long. Simply, once the scheduler has given a thread an execution
time slot on the CPU, it interrupts the thread and schedules another one to run. This ensures
each threads is given an opportunity to make progress. Hence the threads run independently
and asynchronously until completion, and the scheduler decides which thread runs when based
on a scheduling algorithm.
   We will examine the basic scheduling algorithm used later in this chapter. But for now, there
is a major implication for programmers, namely, regardless of the order of thread execution,
your code should produce correct results. Sounds easy?
   Read on.
Problems with Thread  Race Conditions
   Non-deterministic execution of threads implies that the code statements that comprise the
threads:

*	Will execute sequentially as defined for each thread
*	Can be overlapped in any order across threads, as how many statements are executed for
each thread execution slot is the scheduler.

   Hence, when many threads are executed on a single processor, their execution is interleaved.
The executes some steps from one thread, then performs some steps from another, and so on.
If we are executing on a multicore CPU, then we can execute one thread per core. The
statements of each thread execution are still however interleaved in a non-deterministic manner.
   Now, if every thread simply does its own thing, and is completely independent, this is not a
problem. Each thread executes until it terminates, as in our trivial example in Figure 14. Piece of
cake! Why are these thread things meant to be complex?
   Unfortunately, totally independent threads are not how most multithreaded systems behave.
If you refer back to Figure 11, you will see that multiple threads share the global data within a
process. In Java this is both global and static data.
   Threads can use shared data structures to coordinate their work and communicate status
across threads. For example, we may have threads handling requests from a Web clients, one
thread per request. We also want to keep a running total of how many requests we process each
day. When a thread completes a request, it increments a global RequestCounter object that all
threads share and update after each request. At the end of the day, we know how many requests
were processed. A lovely solution indeed.
   Figure 15 shows a very simple implementation that mimics our example scenario by creating
50k threads to update a shared counter. Note we use a lambda function for brevity to create the
threads, and a dont do this at home 5 second delay in main to allow the threads to finish.
   What you can do at home is run this code a few times and see what results you get. In 10
executions my mean was 49995. I didnt once get the correct answer, namely 50000. Weird.
   Why?
1.	public class RequestCounter {
2.	  final static private int NUMTHREADS = 50000;
3.	  private int count = 0;
4.
5.	  public  void inc() {
6.	    count++;
7.	  }
8.
9.	  public int getVal() {
10.	    return this.count;
11.	  }
12.
13.	  public static void main(String[] args) throws InterruptedException
{
14.	    final RequestCounter counter = new RequestCounter();
15.
16.	    for (int i = 0; i < NUMTHREADS; i++) {
17.	      // lambda runnable creation
18.	      Runnable thread = () -> {counter.inc(); };
19.		    new Thread(thread).start();
20.	    }
21.
22.	    Thread.sleep(5000);
23.	    System.out.println("Value should be " + NUMTHREADS + "It is: " +
counter.getVal());
24.	  }
25.	}
   Figure 15 Example of a race condition
   The answer lies in how abstract, high-level programming language statements, in Java in this
case, are executed on a machine. To perform an increment of a counter, the CPU must (1) load
the current value into a register, (2) increment the register value, and (3) write the results back to
the original memory location.
   As Figure 16 shows, at the machine level these three operations are not indistinguishable, or
as more commonly known, atomic. One thread can load the value into the register, but before it
writes the incremented value back, the scheduler interrupts it and allows another thread to start.
This thread loads the old value of the counter and writes back the incremented value.
Eventually the original thread executes again, and writes back its incremented value, which just
happens to be the same as what is already in memory.
   Weve lost an update. From our 10 tests of the code in Figure 15, we see this is happening
on average 5 times in 50000 increments.  Hence such events are rare, but even if it happens 1
time in 10 million, you still have an incorrect result.

   Figure 16 Increments are not atomic at the machine level
   This is called a race condition. Race conditions can occur whenever multiple threads make
changes to some shared state, in this case a simple counter. Essentially, different interleaving sof
the threads can produce different results. Race conditions are insidious, evil errors, because their
occurrence is typically rare, and they can be hard to detect as most of the time the answer will
be correct. Try running the code in Figure 15 with 1000 threads instead of 50000, and you will
see this in action. I got the correct answer four times out of 5.
   Same code. Occasionally different results. Like I said  race conditions evil! Luckily,
eradicating them is straightforward if you take a few precautions.
   The key is to identify and protect critical sections. A critical section is a section of code that
updates shared data structures, and hence must be executed atomically if accessed by multiple
threads. Our example of incrementing a shared counter is an example of a critical section. What
about removing an item from a list? We need to delete the head node of the list, and move the
reference to the head of the list from the removed node to the next node in the list. Both
operations must be performed atomically to maintain the integrity of the list. Its a critical
section.
   In Java, the synchronized keyword defines a critical section. If use to decorate a method,
then when multiple threads attempt to call that method on the same shared object, only one is
permitted to enter the critical section. All others block until the thread exits the synchronized
method, at which point the scheduler chooses the next thread to execute the critical section. We
say the execution of the critical section is serialized, as only one thread at a time can be
executing the code inside it.
   To fix the example in Figure 15, we therefore just need to identify the inc() method as a
critical section, ie:

   synchronized public void inc() {
       count++;
     }

   Test it out as many times as you like. Youll always get the correct answer. Slightly more
formally, this means any interleaving of the threads thar the scheduler throws at us will always
produce the correct results.
   The synchronized keyword can also be applied to blocks of statements within a method. For
example, we could rewrite the above example as:

   public void inc() {
        synchronized(this){
              count++;
           }
   }

   Underneath the covers, every Java object has a monitor lock, sometimes known as an
intrinsic lock, as part of its runtime representation. To enter a synchronized method or block of
statements as shown above, a thread must acquire the monitor lock. Only one thread can own
the lock at any time, and hence execution is serialized. This, very basically, is how Java
implements critical sections.
   Aa a rule of thumb, we should keep critical sections as small as possible so that the serialized
code is minimized. This can have positive impacts on performance and hence scalability. Well
return to this topic later, but if you are interested, have a read about Amdahls Law  for an
explanation on why this is desirable.
Problems with Thread  Deadlock
   To ensure correct results in multithreaded code, we have seen we have to restrict non-
determinism to serialize access to critical sections. This avoids race conditions. However, if we
are not careful, we can write code that restricts non-determinism so much that our program
stops. This is formally known as a deadlock.
   A deadlock occurs when two threads acquire exclusive access to a resource that the other
thread also needs to make progress. The scenario below shows how this can occur:

   Two threads sharing access to two shared variables via synchronized blocks
1.	thread 1: enters critical section A
2.	thread 2: enters critical section B
3.	thread 1: blocks on entry to critical section B
4.	thread 2: blocks on entry to critical section A
5.	Both threads wait forever ?

   A deadlock, also known as a deadly embrace, causes a program to stop. It doesnt take a
vivid imagination to realize that this can cause all sorts of undesirable outcomes. Im happily
texting away while my autonomous vehicle drives me to the bar. Suddenly, the code deadlocks.
It wont end well.
   Deadlocks occur in more subtle circumstances than the simple example above. The classic
example is the Dining Philosophers  problem. The story goes like this.
   Five philosophers sit around a shared table. Being philosophers, they spend a lot of time
thinking deeply. In between bouts of deep thinking, they replenish their brain functions by
eating from a plate of food that sits in front of them. Hence a philosopher is either eating or
thinking, or transitioning between these two states.
   In addition, the philosophers must all be very close, highly dexterous and all COVID19
vaccinated friends, as they share chop sticks to eat with. Only five chopsticks are on the table,
placed between each philosopher. When one wishes to eat, they follow a protocol of picking up
their left chopstick first, then their right chopstick. Once they are ready to think again, they first
return the right chopstick, then the left.

   Figure 17 The Dining Philosophers Problem
   Figure 17 depicts our all-female philosophers, each identified by a unique number. As each is
either concurrently eating or thinking, we can model each philosopher as a thread. The code is
shown in Figure 18. The shared chopsticks are represented by instances of the Java Object
class. As only one object can hold the monitor lock on an object, they are used as entry
conditions to the critical sections in which the philosophers acquire the chopsticks they need to
eat. After eating, the chopsticks are returned to the table and the lock is released on each so that
neighboring philosophers can eat whenever they are ready.
1.	public class Philosopher implements Runnable {
2.
3.	  private final Object leftChopStick;
4.	  private final Object rightChopStick;
5.
6.	  Philosopher(Object leftChopStick, Object rightChopStick) {
7.	    this.leftChopStick = leftChopStick;
8.	    this.rightChopStick = rightChopStick;
9.	  }
10.	  private void LogEvent(String event) throws InterruptedException {
11.	    System.out.println(Thread.currentThread()
12.	                                  .getName() + " " + event);
13.	    Thread.sleep(1000);
14.	  }
15.
16.	  public void run() {
17.	    try {
18.	      while (true) {
19.	        LogEvent(": Thinking deeply");
20.	        synchronized (leftChopStick) {
21.	          LogEvent( ": Picked up left chop stick");
22.	          synchronized (rightChopStick) {
23.	            LogEvent(": Picked up right chopstick  eating");
24.	            LogEvent(": Put down right chopstick");
25.	          }
26.	          LogEvent(": Put down left chopstick. Ate too much");
27.	        }
28.	      } // end while
29.	    } catch (InterruptedException e) {
30.	       Thread.currentThread().interrupt();
31.	  }
32.	 }
33.	}
   Figure 18 A Philosopher thread
   To bring our philosophers to life, we must instantiate a thread for each and give each
philosopher access to its neighboring chopsticks. This is done through the thread constructor
call on line 16 in Figure 19. In the for loop we create five philosophers and start these as
independent threads, where each chopstick is accessible to two threads, one as a left chopstick,
and one as a right.
1.	private final static int NUMCHOPSTICKS = 5 ;
2.	private final static int NUMPHILOSOPHERS = 5;
3.	public static void main(String[] args) throws Exception {
4.
5.	  final Philosopher[] ph = new Philosopher[NUMPHILOSOPHERS];
6.	  Object[] chopSticks = new Object[NUMCHOPSTICKS];
7.
8.	  for (int i = 0; i < NUMCHOPSTICKS; i++) {
9.	    chopSticks[i] = new Object();
10.	  }
11.
12.	  for (int i = 0; i < NUMPHILOSOPHERS; i++) {
13.	    Object leftChopStick = chopSticks[i];
14.	    Object rightChopStick = chopSticks[(i + 1) % chopSticks.length];
15.
16.	    ph[i] = new Philosopher(leftChopStick, rightChopStick);
17.	            }
18.
19.	    Thread th = new Thread(ph[i], "Philosopher " + (i + 1));
20.	    th.start();
21.	  }
22.	}
   Figure 19 Dining Philosophers - deadlocked version
   Running this code produces the following output on my first attempt. This was lucky. If you
run the code you will almost certainly see different outputs, but the final outcome will be the
same.

   Philosopher 4 : Thinking deeply
   Philosopher 5 : Thinking deeply
   Philosopher 1 : Thinking deeply
   Philosopher 2 : Thinking deeply
   Philosopher 3 : Thinking deeply
   Philosopher 4 : Picked up left chop stick
   Philosopher 1 : Picked up left chop stick
   Philosopher 3 : Picked up left chop stick
   Philosopher 5 : Picked up left chop stick
   Philosopher 2 : Picked up left chop stick

   10 lines of output, then  nothing! We have a deadlock. This is a classic circular waiting
deadlock. Imagine the following scenario:

1.	Each philosopher indulges in a long thinking session
2.	Simultaneously, they all decide they are hungry and reach for their left chop stick.
3.	No philosopher can eat (proceed) as none can pick up their right chop stick

   The philosophers in this situation would figure out some way to proceed by putting down a
chop stick or two until one or more can eat. We can sometimes do this is our software by using
timeouts on blocking operations. When the timeout expires a thread releases the critical section
and retries, allowing other blocked threads a chance to proceed. This is not optimal though, as
blocked threads hurt performance, and setting timeout values in an inexact science.
   Its much better however to design a solution to be deadlock free. This means that one or
more threads will always be able to make progress. With circular wait deadlocks, this can be
achieved by imposing a resource allocation protocol on the shared resources, so that threads will
not always request resources in the same order.
   In the Dining Philosophers problem, we can do this by making sure one of our philosophers
picks up their right chop stick first. Lets assume we instruct Philosopher 4 to do this.  This
leads to a possible sequence of operations such as below:

   Philosopher 0 picks up left chopstick (chopStick[0]) then right (chopStick[1])
   Philosopher 1 picks up left chopstick (chopStick[1]) then right (chopStick[2])
   Philosopher 2 picks up left chopstick (chopStick[2]) then right (chopStick[3])
   Philosopher 3 picks up left chopstick (chopStick[3]) then right (chopStick[4])
   Philosopher 4 picks up left chopstick (chopStick[0]) then right (chopStick[4])

   In this example, Philosopher 4 must block, as Philosopher 0 already has acquired access to
chopstick[0]. With Philosopher 4 blocked, Philosopher 3 is assured access to chopstick[4] and
can then proceed to satisfy their appetite.
   The fix for the Dining Philosophers solution is shown in Figure 20.
1.	if (i == NUMPHILOSOPHERS - 1) {
2.	  // The last philosopher picks up the right fork first
3.	  ph[i] = new Philosopher(rightChopStick, leftChopStick);
4.	} else {
5.	  // all others pick up the left chop stick first
6.	  ph[i] = new Philosopher(leftChopStick, rightChopStick);
7.	}
8.	            }
    Figure 20 Solving the Dining Philosophers deadlock
   More formally we are imposing an ordering on the acquisition of shared resources, such that:

   chopStick[0]< chopStick[1]< chopStick[2]< chopStick[3]< chopStick[4]

   This means each thread will always attempt to acquire chopstick[0] before chopstick[1], and
chopstick[1] before chopstick[2], and so on. For philosopher 4, this means it will attempt to
acquire chopstick[0] before chopstick[4], thus breaking the potential for a circular wait deadlock.
   Deadlocks are a complicated topic and this section has just scratched the surface. Youll see
deadlocks in many deployed systems, and well revisit them later when discussing concurrent
database accessed and locking.

Thread States
   Lets briefly describe the various states that a thread can have during its lifecycle.
Multithreaded systems have a system scheduler that decides which threads to run when. In Java,
the scheduler is known as a preemptive, priority-based scheduler. In short this means it chooses
to execute the highest priority thread which wishes to run.
   Every thread has a priority (by default 5, range 0 to 10). A thread inherits its priority from its
parent thread. Higher priority threads get scheduled more frequently than lower priority threads,
but in most applications having all threads as the default priority suffices.
   Scheduling is a JVM-specific implementation detail based on the Java specification. But in
general schedulers behave as follows.
   The scheduler cycles threads through four states, based on their behavior. These are:
   Created: A thread object has been created but its start() method has not been invoked.
Once start() in invoked, the thread enters the runnable state.
   Runnable: A thread is able to run. The scheduler will choose which thread(s) to execute in a
FIFO manner. Threads then execute until they block (e.g. on a synchronized statement),
execute a yield(), suspend() or sleep() statement,  or the run() method terminates, or
are preempted by the scheduler. Preemption occurs when a higher priority thread becomes
runnable, or when a system-specific timeslice value expires. Timeslicing allows the scheduler to
ensure that all threads eventually get chance to execute  no execution hungry threads can hog
the CPU.
   Blocked: A thread is blocked if it is waiting for a lock, a notification event to occur (e.g.
sleep timer to expire, resume() method executed), or is waiting for a network or disk request to
complete. When the event a blocked thread is waiting for occurs, it moves back to the runnable
state.
   Terminated: A threads run() method has completed or it has called the stop() method. The
thread will no longer be scheduled.
   An illustration of this scheme is in Figure 21. The scheduler effectively maintains FIFO
queue in the Runnable state for each thread priority. High priority threads are used typically to
respond to events (eg an emergency timer), and execute for a short period of time. Low priority
threads are used for background, ongoing tasks like checking for corruption of files on disk
through recalculating checksums.

   Figure 21 Threads states and transitions

Thread Coordination
   There are many problems that require threads with different roles to coordinate their
activities. Imagine a collection of threads that accept a document, do some processing on that
document (e.g. generate a pdf), and then send the processed document to a shared printer pool.
Each printer can only print one document at a time, so it reads from a shared print queue,
printing documents in the order they arrive.
   This printing problem is an illustration of the classic producer-consumer problem. Producers
generate and send messages via a shared FIFO buffer to consumers. Consumers retrieve these
messages, process them, and then go to try get more work from the buffer. A simple illustration
of this problem is in Figure 22. Its a bit like a 24 hour, 365 day restaurant in New York, the
kitchen keeps producing and the wait staff collect the food and deliver to hungry diners.
Forever.
   Like virtually all real things, the buffer has a limited capacity. Producers generate new items,
but if the buffer is full, they must wait until some item(s) have been consumer before they can
add the new item to the buffer.  Similarly, if the consumers are consuming faster then the
producers are producing, them they must wait if there are no items in the buffer, and somehow
get alerted when new items arrive.

   Figure 22 The Producer Consumer Problem
   One way for a producer to wait for space in the buffer, or a consumer to wait for an item, is
to keep retrying an operation. A producer could sleep for a second, and then retry the put
operation until it succeeds. A consumer could do likewise.
   This solution is called polling, or busy waiting. It works fine, but as the second name implies,
each producer and consumer are using resources (CPU, memory, maybe network?) each time it
retries and fails. If this is not a concern, then cool, but in scalable systems were always aiming
to optimize resource usage, and polling can be wasteful.
   A better solution is for producers and consumers to block until their desired operation, put
or get respectively, can succeed. Blocked threads consume no resources and hence provide an
efficient solution. To facilitate this, thread programming models provide blocking operations
that enable threads to signal to other threads when an event occurs. With the producer-
consumer problem, the basic scheme is as follows:

*	When a producer adds an item to the buffer, it sends a signal to any blocked consumers
to notify them that there is an item in the buffer
*	When a consumer retrieves an item from the buffer, it sends a signal to any blocked
producers to notify them there is capacity in the buffer for new items.

   In Java, the two basic primitives are wait() and notify(). Briefly, they work like this:
*	A thread may call wait() within a synchronized block if some condition it requires to
hold is not true. For example, a thread may attempt to retrieve a message from a buffer,
but if the buffer has no messages to retrieve, it calls wait() and blocks until another
thread adds a message,  sets the condition to true, and calls notify() on the same object.
*	notify() wakes up a thread that has called wait() on the object.

   These Java primitives are used to implement guarded blocks. Guarded blocks use a condition
as a guard that must hold before a thread resumes the execution. The code snippet below shows
how the guard condition, empty, is used to block a thread that is attempting to retrieve a message
from an empty buffer.
1.	while (empty) {
2.	  try {
3.	    System.out.println("Waiting for a message");
4.	    wait();
5.	  } catch (InterruptedException e) {}
6.	}
    When another thread adds a message to the buffer, it executes notify() as in the code
fragment below.
1.	// Store message.
2.	this.message = message;
3.	empty = false;
4.	// Notify consumer that message is available
5.	notify();
    The full implementation of this example is given in the code examples. There are a number
of variations of the wait() and notify() methods, but these go beyond the scope of what we can
cover in this overview. And luckily, Java provides us with abstractions that hide this complexity
from out code.
   An example that is pertinent to the producer-consumer problem is the BlockingQueue
interface in java.util.concurrent.BlockingQueue. A BlockingQueue implementation
provides a thread-safe object that can be used as the buffer in a producer-consumer scenario.
There are 5 different implementations of the BlockingQueue interface. Lets use one of these,
the LinkedBlockingQueue, to implement the producer-consumer. This is shown in Figure 23.
1.	class ProducerConsumer {
2.	   public static void main(String[] args)
3.	     BlockingQueue buffer = new LinkedBlockingQueue();
4.	     Producer p = new Producer(buffer);
5.	     Consumer c = new Consumer(buffer);
6.	     new Thread(p).start();
7.	     new Thread(c).start();
8.	   }
9.	 }
10.
11.	class Producer implements Runnable {
12.	   private boolean active = true;
13.	   private final BlockingQueue buffer;
14.	   public Producer(BlockingQueue q) { buffer = q; }
15.	   public void run() {
16.
17.	     try {
18.	       while (active) { buffer.put(produce()); }
19.	     } catch (InterruptedException ex) { // handle exception}
20.	   }
21.	   Object produce() { // details omitted, sets active=false }
22.	 }
23.
24.	 class Consumer implements Runnable {
25.	   private boolean active = true;
26.	   private final BlockingQueue buffer;
27.	   public Consumer(BlockingQueue q) { buffer = q; }
28.	   public void run() {
29.
30.	     try {
31.	       while (active) { consume(buffer.take()); }
32.	     } catch (InterruptedException ex) { // handle exception }
33.	   }
34.	   void consume(Object x) {  // details omitted, sets active=false }
35.	 }
36.
   Figure 23 Producer-Consumer with a LinkedBlockingQueue
   This solution absolves the programmer from having to be concerned with the
implementation of coordinating access to the shared buffer, and greatly simplifies the code.
   The java.util.concurrent  package is a treasure trove for building multithreaded Java
solutions. The following sections will briefly highlight a few more powerful and extremely useful
capabilities.
Thread Pools
   Many multithreaded systems need to create and manage a collection of threads that perform
similar tasks. For example, in the producer-consumer problem, we can have a collection of
producer threads and a collection of consumer threads, all simultaneously adding and removing
items, with coordinated access to the shared buffer.
   These collections are known as thread pools. Thread pools comprise several worker threads,
which typically perform a similar purpose and are managed as a collection. We could create a
pool of producer threads which wait for an item to process, write the final product to the
buffer, and then wait to accept another item to process. When we stop producing items, the
pool can be shutdown in a safe manner, so no partially processed items are lost through an
unanticipated exception.
   In java.util.concurrent, thread pools are supported by the ExecutorService interface.
This extends the base Executor interface with a set of methods to manage and terminate
threads in pool. A simple producer-consumer example using a fixed size thread pool  is shown
in Figure 24 and Figure 25. The Producer class in Figure 24 is a Runnable that sends a single
message to the buffer and then terminates. The Consumer simply takes messages from the
buffer until an empty string is received, upon which it terminates.

1.	class Producer implements Runnable {
2.
3.	  private final BlockingQueue buffer;
4.
5.	  public Producer(BlockingQueue q) { buffer = q; }
6.
7.	  @Override
8.	  public void run() {
9.
10.	  try {
11.	    sleep(1000);
12.	    buffer.put("hello world");
13.
14.	  } catch (InterruptedException ex) {
15.	    // handle exception
16.	  }
17.	 }
18.	}
19.
20.	class Consumer implements Runnable {
21.	  private final BlockingQueue buffer;
22.
23.	  public Consumer(BlockingQueue q) { buffer = q; }
24.
25.	  @Override
26.	   public void run() {
27.	      boolean active = true;
28.	      while (active) {
29.	          try {
30.	             String  s = (String) buffer.take();
31.	             System.out.println(s);
32.	             if (s.equals("")) active = false;
33.	          } catch (InterruptedException ex) {
34.	              / handle exception
35.	          }
36.	      } /
37.	      System.out.println("Consumer terminating");
38.	    }
39.	 }
40.
   Figure 24 Producer and Consumer for thread pool implementation
   In Figure 25, we create a single consumer to take messages from the buffer. We then create
fixed size thread pool of size 5 to manage our producers. This causes the JVM to pre-allocate
five threads that can be used to execute any Runnable objects that are executed by the pool.
   In the for() loop, we then use the ExecutorService to run 20 producers. As there are
only 5 threads available in the thread pool, only a maximum of 5 producers will be executed
simultaneously. All others are placed in wait queue which is managed by the thread pool. When
a producer terminates, the next Runnable in the wait queue is executed using any an available
thread in the pool.
   Once we have requested all the producers to be executed by the thread pool, we call the
shutdown() method on the pool. This tells the ExecutorService not to accept any more tasks
to run. We next call  the awaitTermination() method, which blocks the calling thread until
all the threads managed by the thread pool are idle and no more work is waiting in the wait
queue. Once awaitTermination() returns, we know all messages have been sent to the
buffer, and hence send an empty string to the buffer which will act as a termination value for
the consumer.
1.	public static void main(String[] args) throws
InterruptedException
2.	  {
3.	    BlockingQueue buffer = new LinkedBlockingQueue();
4.
5.	    //start a single consumer
6.	    (new Thread(new Consumer(buffer))).start();
7.
8.	    ExecutorService producerPool = Executors.newFixedThreadPool(5);
9.	    for (int i = 0; i < 20; i++)
10.	      {
11.	        Producer producer = new Producer(buffer) ;
12.	        System.out.println("Producer created" );
13.	        producerPool.execute(producer);
14.	      }
15.
16.	      producerPool.shutdown();
17.	      producerPool.awaitTermination(10, TimeUnit.SECONDS);
18.
19.	      //send termination message to consumer
20.	      buffer.put("");
21.	    }
   Figure 25 Thread pool-based Producer Consumer solution
   Like most topics in this chapter, theres many more sophisticated features in the Executor
framework that can be used to create multithreaded programs. This description has just covered
the basics. Thread pools are important as they enable our systems to rationalize the use of
resources for threads. Every thread consumes memory, for example the stack size for a thread is
typically around 1MB. Also, when we switch execution context to run a new thread, this
consumes CPU cycles. If our systems create threads in an undisciplined manner, we will
eventually run out of memory and the system will crash. Thread pools allow us to control the
number of threads we create, and utilize them efficiently.
   Well revisit thread pools throughout the remainder of this book, as they are a key concept
for efficient and scalable management of ever the increasing request loads that servers must
satisfy.

Barrier Synchronization
   I had a high school friend whose family, at dinner times, would not allow anyone to start
eating until the whole family was seated at the table. I thought this was weird, but many years
later it serves as a good analogy for the concept known as barrier synchronization. Eating
commenced only after all family members arrived at the table.
   Multithreaded systems often need to follow such a pattern of behavior. Imagine a
multithreaded image processing system. An image arrives and a non-overlapping segment of the
image is passed to each thread to perform some transformation upon  think Instagram filters
on sterioids. The image is only fully processed when all threads have completed. In software
systems, we use a mechanism called barrier synchronization to achieve this style of thread
coordination.
   The general scheme is shown in Figure 26. In this example, the main() thread creates four
new threads and all proceed independently until they reach the point of execution defined by
the barrier. As each thread arrives, it blocks. When all threads have arrived at this point, the
barrier is released, and each thread can continue with its processing.

   Figure 26 Barrier Synchronization
   Java providers three primitives for barrier synchronization. Lets see how one of the three,
namely the CountDownLatch, works.
   When you create a CountDownLatch, you pass a value to its constructor that represents the
number of threads that must bock at the barrier before they are all allowed to continue. This is
called in the thread which is managing the barrier points for the system  in Figure 26this would
be main().

   CountDownLatch  nextPhaseSignal = new CountDownLatch(numThreads);

   Next we create the worker threads that will perform some actions and then block at the
barrier until they all complete. To do this, we need to pass each thread a reference to
CountDownLatch.

   for (int i = 0; i < numThreads; i++) {
               Thread worker = new Thread(new
WorkerThread(nextPhaseSignal));
               worker.start();
           }

   After launching the worker threads, the main() thread will call the .await() method to
block until the latch is triggered by the worker threads.

   nextPhaseSignal.await();

   Each worker thread will complete its task and before exiting call the .countDown() method
on the latch. This decrements the latch value. When the last thread calls .countDown() and the
latch value becomes zero, all threads that have called .await() on the latch transition from the
blocked to the runnable state. In our example this would be the main()  thread. At this stage we
are assured that all workers have completed their assigned task.

   nextPhaseSignal.countDown();

   Any subsequent calls to .countDown() will return immediately as the latch has been
effectively triggered. Note .countDown() is non-blocking, which is a useful property for
applications in which threads have more work to do after reaching the barrier.
   Our example illustrates using a CountDownLatch to block a single thread until a collection
of threads have completed their work. We can invert this use case with a latch however if we
initialize its value to one. Multiple threads could call .await() and block until another thread
calls .countDown()  to release all waiting threads. This example is analogous to a simple gate,
which one thread opens to allow a collection of others to continue.
   CountDownLatch is a simple barrier synchronizer. Its a single use tool, as the initializer
value cannot be reset. More sophisticated features are provided by the CyclicBarrier and
Phaser classes in Java and hopefully armed with the knowledge of how barrier synchronization
works from this section, these will be straightforward to understand.
Thread-Safe Collections
   Many programmers, once they delve into the wonders of multithreaded programs, are
surprised to discover that the collections in the java.util package  are not thread safe. Why, I
hear you ask? The answer, luckily, is simple. Its to do with performance. Calling synchronized
methods incurs overheads. Hence to attain faster execution for single threaded programs, the
collections are not thread-safe.
   If you want to share an ArrayList, Map or your favorite data structure from java.util
across multiple threads, you must ensure modifications to the structure are placed in critical
sections. This approach places the burden on the client of the collection to safely make updates,
and hence is error prone  a programmer might forget to make modifications in a
synchronized block.
   Its always safer to use inherently thread-safe collections in out multithreaded code. For this
reason, the Java collections framework provides a factory method that creates a thread-safe
version of java.util collections. Heres an example of creating a thread-safe list.

   List<String> list = Collections.synchronizedList(new ArrayList<>());

   Whats really happening here is that we are creating a wrapper around the base collection
class, which has synchronized methods. These delegate the actual work to the original class, in
a thread-safe manner of course. You can use this approach for any collection in the java.util
package, and the general form is:

   Collections.synchronized.(collection)

   where . is List, Map, Set, and so on.
   Of course, when using the synchronized wrappers, you pay the performance penalty for
acquiring the monitor lock and serializing access from multiple threads. This means the whole
collection is locked while a single thread makes a modification, greatly limiting concurrent
performance (remember Amdahls Law?). For this reason, Java 5.0 included the concurrent
collections package, namely java.util.concurrent. It contains a rich collection of classes
specifically designed for efficient multithreaded access.
   In fact weve already seen one of these classes  the LinkedBlockingQueue. This uses a
locking mechanism that enables items to be added to and removed from the queue in parallel.
This finer grain locking mechanism utilizes the java.util.concurrent.lock.Lock class
rather than the monitor lock approach. This allows multiple locks to be utilized on the same
collection, hence enabling safe concurrent access.
   Another extremely useful collection that provides this finer-grain locking is the
ConcurrentHashMap. This provides the same methods as the non-thread safe Hashtable, but
allows non-blocking reads and concurrent writes based on a concurrencyLevel value you can
pass to the constructor (the default value is 16).

   ConcurrentHashMap (int initialCapacity, float loadFactor,
                        int concurrencyLevel)

   Internally, the hash table is divided into individually lockable segment, often known as
shards. This means updates can be made concurrently to hash table entries in different shards of
the collection, increasing performance.
   There are a number of subtle issues concerning iterators and collections, but these are
beyond the scope of this chapter. Well however investigate some of these in the exercises at the
end of the chapter.
Summary and Further Reading
   This chapter has only brushed the surface of concurrency in general and its support in Java.
The best book to continue learning more about the basic concepts of concurrency in is the
classic Java Concurrency in Practice by Brian Goetz et al. If you understand everything in this
book, youll be writing pretty great concurrent code.
   Java concurrency support has moved on considerably however since Java 5.  In the world of
Java 12 (or whatever version is current when you read this), there are new features such as
CompleteableFutures, lambda expressions and parallel streams. The functional programming
style introduced in Java 8.0 makes it easy to create concurrent solutions without directly creating
and managing threads. A good source of knowledge for Java 8.0 features is Mastering
Concurrency Programming with Java 8 by Javier Fernndez Gonzlez.

   Other excellent sources include:
*	Doug Lea, Concurrent Programming in Java: Design Principles and Patterns, 2nd
Edition
*	Raoul-Gabriel Urma, Mario Fusco, and Alan Mycroft, Java 8 in Action: Lambdas,
Streams, and functional-style programming, manning Publications, 1st Edition, 2014.

Exercises

   volatile
   thread safe v non thread safe collection  performance, including CHP sharding
   copyonwrite example  immutable, slow

   collections, iterators

CHAPTER 4
__________________________

Distributed Systems Fundamentals

   Scaling a system naturally involves adding multiple independently moving parts. We run our
software components on multiple machines and our databases across multiple storage nodes, all
in the quest of adding more capacity. Consequently, our solutions are distributed across multiple
machines in multiple locations, with each machine processing events concurrently, and
exchanging messages over a network.
   This fundamental nature of distributed systems has some profound implications on the way
we design, build and operate our solutions. This chapter provides the basic nature of the beast
information you need to know to appreciate the issues and complexities of distributed software
systems. We briefly cover communications networks hardware and software, how to deal with
the implications of communications failures, distributed coordination, and the complex issue of
time in distributed systems.
Communications Basics
   Every distributed system has software components that communicate over a network. If a
mobile banking app requests the users current bank account balance, a (simplified) sequence of
communications occurs along the lines of:

1.	The cell phone app sends a request over the cellular network addressed to the bank to
retrieve the users bank balance
2.	The request is routed across the internet to where the banks web servers are located.
3.	The banks web server authenticates the request (checks its really the supposed user)
and sends a request to a database server for the account balance.
4.	The database server reads the account balance from disk and returns it to the web server
5.	The web server sends the balance in a reply message addressed to the app, which is
routed over the internet and the cellular network until the balance magically appears on
the screen of the mobile device

   It almost sounds simple when you read the above, but in reality, theres a huge amount of
complexity hidden beneath this sequence of communications. Lets examine some of these in
the following sections.
  Communications Hardware
   The bank balance request will inevitably traverse multiple different networking technologies
and devices. The global internet is a heterogeneous machine, comprising different types of
network communications channels and devices that shuttle many millions of messages a second
across networks to their intended destinations.
   Different types of communications channels exist. The most obvious categorization is wired
versus wireless. For each category there are multiple network transmission hardware
technologies that can ship bits from one machine to another. Each technology has different
characteristics, and the ones we typically care about are speed and range.
   For physically wired networks, the two most common types are local area networks (LANs)
and wide area networks (WANs). LANs are networks that can connect devices at building
scale, being able to transmit data over a small number (e.g. 1-2) of kilometers. Contemporary
LANs can transport between 100 megabits per second (Mbps) to 1 gigabits per second (Gbps).
This is known as the networks bandwidth, or capacity. The time taken to transmit a message
across a LAN  the networks latency  is sub-millisecond with modern LAN technologies.
   WANs are networks that traverse the globe and make up what we collectively call the
internet. These long-distance connections are the high speed data pipelines connecting cites
and countries and continents with fiber optic cables. These cables support a networking
technology known as wavelength division multiplexing  which makes it possible to transmit up
171 Gbps over 400 different channels, giving more than 70 Terabits per second (Tbps) of total
bandwidth for a single fiber link. The fiber cables that span the world normally comprise four or
more strands of fiber, giving bandwidth capacity of hundreds of Tbps for each cable.
   Latency is more complicated with WANs however. WANs transmit data over 100s to 1000s
of kilometers, and the maximum speed that the data can travel is the theoretical speed of light.
In reality, fiber optic cables cant reach the speed of light, but do get pretty close to it as we can
see in Table 1.

Path
Distance
Time -
Speed of
Light
Time - Fiber
Optic Cable
New York to
San Francisco
4,148 km
14 ms
21 ms
New York to
London
5,585 km
19 ms
28 ms
New York to
Sydney
15,993 km
53 ms
80 ms
   Table 1 WAN Speeds
   Actual times will be slower than this as the data needs to pass through networking
equipment known as routers . Routers are responsible for transmitting data on the physical
network connections to ensure data is transmitted across the internet from source to
destination. Routers are specialized, high speed devices that can handle several hundred Gbps of
network traffic, pulling data off incoming connections and sending the data out to different
outgoing network connections based on their destination. Routers at the core of the internet
comprise racks of these devices and hence can process 10s to hundreds of Tbps. This is how
you and 1000s of your friends get to watch a steady video stream on Netflix.
   Wireless technologies have different range and bandwidth characteristics. Wi-Fi routers that
we are all familiar with in our homes and offices are wireless ethernet networks and use 802.11
protocols to send and receive data. The most widely used Wi-Fi protocol, 802.11ac, allows for
maximum (theoretical) data rates of up to 5,400Mbps. The most recent 802.11ax protocol, also
known as Wi-Fi 6, is an evolution of 802.11ac technology that promises increased throughput
speeds of up to 9.6Gbps. The range of Wi-Fi routers is of the order of 10s of meters, and of
course is affected by physical impediments like walls and floors.
   Cellular wireless technology uses radio waves to send data from our phones to routers
mounted on cell towers, which are generally connected by wires to the core internet for message
routing. Each cellular technology introduces improved bandwidth and other dimensions of
performance. The most common technology at the time of writing is 4G LTE wireless
broadband. 4G LTE is around 10 times faster than the older 3G, able to handle sustained
download speeds around 10 Mbps (peak download speeds are nearer 50 Mbps) and upload
speeds between 2 and 5 Mbps.
   Emerging 5G cellular networks promise 10x bandwidth improvements over existing 4G,
with 1-2 millisecond latencies between devices and cell towers. This is a great improvement over
4G latencies which are in the 20-40 millisecond range. The trade-off is range. 5G base station
range operates at about 500m maximum, whereas 4G provides reliable reception at distances of
10-15kms.
   This whole collection of different hardware types for networking comes together in the
global internet. The internet is heterogeneous network, with many different operators globally
and every type of hardware imaginable. Figure 27 (from Wikipedia ) shows a simplified view of
the major components that comprise the internet. Tier 1 networks are the global high-speed
internet backbone. There are around 20 Tier 1 Internet Service Providers (ISPs) who manage
and control global traffic. Tier 2 ISPs are typically regional (e.g. one country), have lower
bandwidth than Tier 1 ISPs, and deliver content to customers through Tier 3 ISPs. Tier 3 ISPs
are the ones that charge your exorbitant fees for your home internet every month.

   Figure 27 Simplified view of the Internet (from Wikipedia)
   Theres a lot more complexity to how the internet works than described here. That
complexity is beyond the scope of this chapter. From a distributed systems software
perspective, we need to understand more about the magic that enables all this hardware to
route messages from say my cell phone, to my bank and back. This is where the IP protocol
comes in.

  Communications Software
   Software systems on the internet communicate using the Internet Protocol suite. The
Internet Protocol suite specifies host addressing, data transmission formats, message routing
and delivery characteristics. There are four abstract layers, which contain related protocols that
support the functionality required at that layer. These are, from lowest to highest:

1)	the data link layer, specifying communication methods for data across a single network
segment. This is implemented by the device drivers and network cards that live inside
your devices.
2)	the internet layer specifies addressing and routing protocols that make it possible for
traffic to traverse the independently managed and controlled networks that comprise the
internet. This is the IP protocol in the internet protocol suite.
3)	the transport layer, specifying protocols for reliable and best-effort host-to-host
communications. This is where the well-known TCP and UDP protocols live.
4)	the application layer, which comprises several application level protocols such as HTTP
and SCP.

   Each of the higher layer protocols builds on the features of the lower layers. The enables the
IP suite to support end to end data communications across the internet. In the following, well
briefly cover the IP protocol for host discovery and message routing, and the TCP and UDP
transport protocols that can be utilized by distributed applications.
   Internet Protocol (IP)
   IP defines how hosts are assigned addresses on the internet and how messages are
transmitted between two hosts who know each others addresses.
   Every device on the internet has its own address. These are known as Internet Protocol (IP)
addresses. The location of an IP address can be found using an internet wide directory service
known as Domain Naming Service (DNS). DNS is a widely distributed, hierarchical database
that acts as the address book of the internet.
   The technology currently used to assign IP addresses, known as Internet Protocol version 4
(IPv4), will eventually be replaced by its successor, IPv6. IPv4 is a 32-bit addressing scheme that
before long will run out of addresses due to the number of devices connecting to the internet.
IPv6 is a 128-bit scheme that will offer an (almost) infinite number of IP addresses. As an
indicator, in July 2020 about 33% of the traffic processed by Google.com  is IPv6.
   DNS servers are organized hierarchically. A small number of root DNS servers, which are
highly replicated, are the starting point for resolving and an IP address. When an internet
browser tries to find a web site, a network host known as the local DNS server that is managed
by your employer or ISP, will contact a root server with the requested host name. The root
server replies with a referral to a so-called authoritative DNS server that manages name resolution
for, in our banking example, .com addresses. There is an authoritative name server for each top-
level internet domain (e.g. .com, .org, .net, etc).
   Next the local DNS server will query the .com DNS server, which will reply with the address
of the DNS server which knows about all the IP addresses managed by mybank.com. This DNS
is queried, and it returns the actual IP address we need to communicate with the application.
The overall scheme is illustrated in Figure 28.

   Figure 28 Example DNS Lookup for mybank.com
   The whole DNS database is highly geographically replicated so there are no single points of
failure, and requests are spread across multiple physical servers. Local DNS servers also
remember the IP addresses of recently contacted hosts, which is possible as IP addresses dont
change very often. This means the complete name resolution process doesnt occur for every
site we contact.
   Armed with a destination IP address, a host can start sending data across the network as a
series of IP packets. IP has the task of delivering data from the source to the destination host
based on the IP addresses in the packet headers. IP defines a packet structure that contains the
data to be delivered, along with header data including source and destination IP addresses. Data
sent by an application is broken up into a series of packets which are independently transmitted
across the Internet.
   IP is known as a best-effort delivery protocol. This means it does not attempt to compensate
for the various error conditions that can occur during packet transmission. Possible
transmission errors include data corruption, packet loss and duplication. In addition, as every
packet is routed from source to destination independently, different packets may be delivered to
the same destination via different network paths, resulting in out-of-order delivery to the
receiver. Treating every packet independently is known as packet-switching. This allows the
network to dynamically respond to conditions such as link failure and congestion, and hence is a
defining characteristic of the internet.
   Because of this design, the IP is unreliable. If two hosts require reliable data transmission,
they need to add additional features to make this occur. This is where the next layer in the IP
protocol suite, the transport layer, enters the scene.

   Transmission Control Protocol (TCP)
   Once an application or browser has discovered the IP address of the server it wishes to
communicate with, it can send messages using the transport protocol API. This is achieved
using Transmission Control Protocol (TCP) or User Datagram Protocol (UDP), which are the
established standard transport protocols for the IP network stack.
   Distributed applications can choose which of these protocols to use. APIs for both TCP and
UDP are widely available in mainstream programming languages such as Java, Python and C++.
In reality, use of these APIs is not common as higher-level programming abstractions hide the
details from most applications. In fact, the IP protocol suite application layer contains several of
these application level APIs, including HTTP, which is very widely used in mainstream
distributed systems. Still, its important to understand TCP, UDP and their differences.
   Most requests on the internet are sent using TCP. TCP is:

*	connection-oriented
*	stream-oriented
*	reliable

   TCP is a connection-oriented protocol. Before any messages are exchanged between
applications, TCP uses a 3-step handshake to establish a two-way connection between the client
and server applications. The connection stays open until the TCP client calls close to terminate
the connection with the TCP server. The server responds by acknowledging the close request
before the connection is dropped.
   Once a connection is established, a client sends a sequence of requests to the server as a data
stream. When a data stream is sent over TCP, it is broken up into individual network packets,
with a maximum packet size of 65535 bytes. Each packet contains a source and destination
address, which is used by the underlying IP protocol to route the messages across the network.
   The internet is a packet-switched network, which means every packet is individually routed
across the network. The route each packet traverses can vary dynamically based on the
conditions in the network, such as link congestion or failure. This means the packets may not
arrive at the server in the same order they are sent from the client. To solve this problem, a TCP
sender includes a sequence number in each packet so the receiver can reassemble packets into a
stream that is identical to the order they were sent.
   Reliability is needed as network packets can be lost or delayed during transmission between
sender and receiver. To achieve reliable packet delivery, TCP uses a cumulative
acknowledgement mechanism. This means a receiver will periodically send an acknowledgement
packet that contains the highest sequence number of the received packets. This implicitly
acknowledges all packets sent with a lower sequence number, meaning all have been successfully
received. If a sender doesnt receive an acknowledgement within a timeout period, the packet is
resent.
   TCP has many other features, such as checksums to check packet integrity, and dynamic
flow control to ensure a sender doesnt overwhelm a slow receiver by sending data too quickly.
Along with connection establishment and acknowledgments, this makes TCP a relatively
heavyweight protocol, which trades off reliable over efficiency.
   This is where UDP comes into the picture. UDP is a simple connectionless protocol, which
exposes the user's program to any unreliability of the underlying network. There is no guarantee
of in order delivery, or even delivery for that matter. It can be thought of as a thin veneer (layer)
on top of the underlying IP protocol, and deliberately trades off raw performance over
reliability. This however is highly appropriate for many modern applications where the odd lost
packet has very little effect. Think streaming movies, video conferencing and gaming, where one
lost packet is unlikely to be perceptible by a user.

   Figure 29 Comparing TCP and UDP
   Figure 29 depicts some of the major differences between TCP and UDP. TCP incorporates
a connection establishment 3-packet handshake, and piggybacks acknowledgements (ACK) of
packets so that any packet loss can be handled by the protocol. Theres also a TCP connection
close phase involving a 4-way handshake that is not shown in the diagram. UDP dispenses with
connection establishment, tear down, acknowledgements and retries. Hence applications using
UDP need to be tolerant of packet loss and client or server failures and behave accordingly.
Remote Method Invocation
   Its perfectly feasible to write our distributed applications using APIs that interact directly
with transport layer protocols like TCP. If we use for example the TCP sockets library, we can
create a connection, known as a socket, between a client and a server and exchange data over
that connection.
   A socket is basically a pipe between the client and server. Once the socket is created, the
client sends data to the server in a stream. In our bank example, the client might request a
balance for the users checking account. Ignoring specific language issues (and security!!), the
client might send a message payload as follows over a socket to the server:

   {balance, 000169990}

   In this message, balance represents the operation we want the server to execute, and
000169990 is the bank account number.
   In the server, we need to know that the first string in the message is the operation identifier,
and based on this value being balance, the second is the bank account number. The server
then uses these values to presumably query a database, retrieve the balance and send back the
results, perhaps as a message formatted with the account number and current balance, as below:

   {000169990, 220.77}

   In any complex system, the server will support many operations. In mybank.com, we might
have for example login, transfer, address, statement, transactions, and so on. Each
will be followed by different message payloads that the server needs to interpret correctly to
fulfill the clients request.
   What we are defining here is an application specific protocol. As long as we send the necessary
values in the correct order for each operation, the server will be able to respond correctly. If we
have an erroneous client that doesnt adhere to our application protocol, well, our server needs
to do thorough error checking.
   Stepping back, if we were defining the mybank.com server in an object-oriented language
such as Java, we would have each operation it can process as a method, which is passed an
appropriate parameter list for that operation, as shown in Figure 30.
1.	// Simple mybank.com server interface
2.	public interface MyBank {
3.	    public float balance  (String accNo);
4.	    public boolean  statement(String month) ;
5.	    // other operations
6.	 }
   Figure 30 Simple mybank.com server interface
   There are several advantages of having such an interface, namely:
*	Calls from the client to the server can be statically checked by the compiler to ensure
they are of the correct type
*	Changes in the server interface (e.g. add a new parameter) force changes in the client
code to adhere to the new method signature
*	The interface is clearly defined by the class definition, and hence straightforward for a
client programmer to utilize

   These benefits of an explicit interface are of course well known in sequential programming.
The whole discipline of object-oriented design is pretty much based upon these foundations,
where an interface defines a contract between the caller and callee. Compared to the implicit,
application protocol we need to program to with sockets, the advantages are significant.
   This fact was recognized reasonably early in the evolution of distributed systems. Since the
early 1990s, we have seen an evolution of technologies that enable us to define explicit server
interfaces and call these across the network using essentially the same syntax as we would in a
sequential program. A summary of the major approaches is given in Table 2. Collectively they
are known as Remote Procedure Call (RPC) or Remote Method Invocation (RMI) technologies.

Technology
Dates
Main features
Distributed Computing
Environment (DCE)
Early
1990s
DCE RPC provides a standardized approach for
client-server systems. Primary languages were
C/C++.
Common Object Request
Broker Architecture
(CORBA)
Early
1990s
Facilitates language-neutral client-server
communications based on an object-oriented
Interface Definition Language (IDL). Primary
language support in C/C++, Java, Python, Ada.
Java Remote Method
Invocation (RMI)
Late
1990s
A pure Java-based remote method invocation
that facilitates distributed client-server systems
with the same semantics as Java objects.
XML Web Services
2000
Supports client-server communications based on
HTTP and XML. Servers define their remote
interface in the Web Services Description
Language (WSDL)
   Table 2 Summary of major RPC/RMI Technologies
   While the syntax and semantics of these RPC/RMI technologies vary, the essence of how
each operates is the same. Lets continue with our Java example of mybank.com to use this as
an example of the whole classification.
   Using Java RMI, we can trivially make our MyBank interface in Figure 30 a remote interface,
as shown in Figure 31.
1.	import java.rmi.*;
2.	// Simple mybank.com server interface
3.	public interface MyBank extends Remote{
4.	    public float balance  (String accNo)
5.	         throws RemoteException;
6.	    public boolean  statement(String month)
7.	         throws RemoteException ;
8.	    // other operations
9.	 }
   Figure 31 Java RMI Example
   The empty java.rmi.Remote interface serves as a marker to inform the Java compiler we
are creating an RMI server. In addition, each method must throw
java.rmi.RemoteException. These exceptions represent errors that can occur when a
distributed call between two objects is invoked over a network. The most common reasons for
such an exception would be a communications failure or the server object having crashed.
   We then must provide a class that implements this remote interface. Figure 32 shows an
extract of the server implementation - the complete code for this example is in the Chapter 4
code repository. Points to note are:
*	The server extends the UnicastRemoteObject class. This essentially provides the
functionality to instantiate remotely callable object.
*	Once the server object is constructed, its availability must be advertised to remote
clients. This is achieved by storing a reference to the object in the RMI Registry, and
associating a logical name with it  in this example, MyBankServer. The registry is a
simple directory service that enables clients to look up the location (network address and
object reference) of and obtain a reference to an RMI server.
1.	public class MyBankServer extends UnicastRemoteObject
2.	                          implements MyBank  {
3.	            // constructor/method implementations omitted
4.	   public static void main(String args[]){
5.	        try{
6.	          MyBankServer server=new MyBankServer();
7.	          // create a registry in local JVM on default port
8.	          Registry registry = LocateRegistry.createRegistry(1099);
9.	          registry.bind("MyBankServer", server);
10.	          System.out.println("server ready");
11.	        }catch(Exception e){
12.	                 // code omitted for brevity}
13.	        }
14.	   }
    Figure 32 Implementing an RMI Server
   An extract from the client code to connect to the server is shown in Figure 33. It obtains a
reference to the remote object by performing a lookup operation (line 3) in the RMI Registry
and specifying the logical name that identifies the server. The reference returned by the lookup
operation can then be used to call the server object in the same manner a local object. However
there is a difference  the client must be ready to catch a RemoteException that will be thrown
by the Java runtime when the server object cannot be reached.
1.	 // obtain a remote reference to the server
2.	 MyBank bankServer=
3.	        (MyBank)Naming.lookup("rmi://localhost:1099/MyBankServer");
4.	 //now we can call the server
5.	 System.out.println(bankServer.balance("00169990"));
    Figure 33 Implementing an RMI Client
   Figure 34 depicts the call sequence amongst the components that comprise a RMI system.
The Stub and Skeleton are compiler-generated objects that facilitate the remote communications.
The skeleton is a TCP network endpoint (host, port) that listens for calls to the associated
server. The sequence of operations is as follows:

1.	When the server reference is stored in the RMI Registry, the entry contains the
client stub that can be used to make remote calls to the server.
2.	The client queries the registry, and the stub for the server is returned.
3.	The client stub accepts a method call to the server interface from the client
4.	The stub transforms the request into a network request to the server host. This
transformation process is known as marshalling.
5.	The skeleton accepts network requests from the client, and unmarshalls the network
packet data into a valid call to the server  object implementation
6.	The skeleton waits for the method to return a result
7.	The skeleton marshalls the method results into a network reply packet that is sent
the client
8.	The stub unmarshalls the data passes the result to the client call site

   Figure 34 Schematic depicting the call sequence for establishing a connection and making a call to a RMI
server object
   This Java RMI example illustrates the basics that are used for implementing any RPC/RMI
mechanism, even in modern languages like Erlang  and Go . Regardless of implementation,
the basic attraction of these approaches to is to provide an abstract calling mechanism that
provides  location transparency for clients making remote server calls.
   RPC/RMI is not without its flaws. Marshalling and unmarshalling can become inefficient for
complex object parameters. Cross language marshalling  client in one language, server in
another  can cause problems due to types being represented differently in different languages,
causing subtle incompatibilities. And if a remote method signature changes, all clients need to
obtain a new compatible stub which can be cumbersome in large deployments.
   For these reasons, most modern systems are built around simpler protocols based on HTTP
and using JSON for parameter representation. Instead of operation names, HTTP verbs (PUT,
GET, POST, etc) have associated semantics that are mapped to a specific URL. This approach
originated in the work by Roy Fielding on the REST approach . REST has a set of semantics
that comprise a RESTful architecture style, and in reality, most systems do not adhere to these.
Well discuss REST and HTTP API mechanisms in the Chapter 5.
Partial Failures
   The components of distributed systems communicate over a network. In communications
technology terminology, the shared local and wide area networks that our systems communicate
over are known as asynchronous networks. With asynchronous networks:

*	nodes can choose to send data to other nodes at any time
*	The network is half-duplex, meaning that one node sends a request and must wait for a
response from the other. These are two separate communications.
*	The time for data to be communicated between nodes is variable, due to reasons like
network congestion, dynamic packet routing and transient network connection failures
*	The receiving node may not be available due to a software or machine crash
*	Data can be lost. In wireless networks, packets can be corrupted and hence dropped due
to weak signals or interference. Internet routers can drop packets during congestion .
*	Nodes do not have identical internal clocks, hence they are not synchronized

   What does this mean for our applications? Well, put simply, when a client sends a request to
server, how long does it wait until it receives a reply? Is the server node just being slow? Is the
network congested and the packet has been dropped? If the client doesnt get a reply, what
should it do?
   Lets explore these scenarios in detail. The core problem is known as handling partial
failures, and the general situation is depicted in Figure 35.

   Figure 35 Handling Partial Failures
   When a client node sends a network request to a server node and expects a response, the
following outcomes may occur:

1.	The request succeeds and a rapid response is received. All is good.
2.	The destination IP address lookup may fail. In this case the client rapidly receives a
error message.
3.	The IP address is valid but the destination node or target server process has failed.
Again the sender will receive an error message.
4.	The request is received by the target server, which fails while processing the request and
no response is ever sent.
5.	The request is received by the target server, which is heavily loaded. It processes the
request but takes a long time (e.g 24 seconds!) to respond
6.	The request is received by the target server and a response is sent. However, the
response is not received by the client due to a network failure.

   Numbers (1) to (3) are easy for the client to handle, as a response is received rapidly. A result
from the server or an error message  either allows the client to proceed. Failures that can be
detected quickly are easy to deal with.
   Numbers (4) to (6) pose a problem for the client. It has no insight into the reason why a
response has not been received. From the clients perspective, these three outcomes look the
same. It cannot know, without waiting forever, whether the response will arrive eventually, or
never arrive. And waiting forever doesnt get much work done. More insidiously, the client
cannot know if the operation succeeded and server or network failure caused the result to never
arrive, or if the request is on its way - delayed simply due to congestion in the network/server.
These faults are collectively known as crash faults .
   The typical solution that clients adopt to handle crash faults is to resend the request after a
configured timeout period. This however is fraught with danger, as Figure 36 illustrates.  The
client sends a request to the server to deposit money in a bank account. When it receives no
response after a timeout period, it resends the request. What is the resulting balance? The server
may have applied the deposit, and it may not, depending on the partial failure scenario.

   Figure 36 Client retries a request after timeout
   The chance that the deposit may occur twice is a fine outcome for the customer. The bank
though is unlikely to be amused by this possibility. Hence we need a way to ensure in our server
operations that retried, duplicate requests from clients only result in the request being applied
once. This is necessary to maintain correct application semantics. This property is known as
idempotence. Idempotent operations can be applied multiple times without changing the result
beyond the initial application. This means that for the example in Figure 36, the client can retry
the request as many times as it likes, and the account will only be increased by $100.
   Requests that make no persistent state changes are naturally idempotent. This means all read
requests are inherently safe and no extra work is needed in the server. Updates are a different
matter. The system needs to devise a mechanism such that duplicate client requests can be
detected by the server, and they do not cause any state changes. In API terms, these endpoints
cause mutation of the server state and must be idempotent.
   The general approach to building idempotent operations is as follows:
*	Clients include a unique idempotence-key in all requests that mutate state. The key identifies
a single operation from the specific client or event source. It is usually a composite of a
user identifier, such as the session key, and a unique value such as a local timestamp,
UUID or a sequence number.
*	When the server receives a request, it checks to see if it has previously seen the
idempotence key value by reading from a database that is uniquely designed for
implementing idempotence. If the key is not in the database, this is a new request. The
server therefore performs the business logic to update the application state. It also stores
the idempotence key in a database to indicate that the operation has been successfully
applied.
*	If the idempotence key is in the database, this indicates that this request is a retry from
the client and hence should not be processed. In this case the server returns a valid
response for the operation so that (hopefully) the client wont retry again.

   The database used to store idempotence keys can be implemented in, for example:
*	a separate database table or collection in the transactional database used for the
application data
*	a dedicated database that provides very low latency lookups, such as a simple key-value
store

   Unlike application data, idempotence keys dont have to be retained forever. Once a client
receives an acknowledgement of a success for an individual operation, the idempotence key can
be discarded. The simplest way to achieve this is to automatically remove idempotence keys
from the store after a specific time period, such as 60 minutes or 24 hours, depending on
application needs and request volumes.
   In addition, an idempotent API implementation must ensure that the application state is
modified, and the idempotence key is stored. Both must occur for success. If the application
state is modified and, due to some failure, the idempotent key is not stored, then a retry will
cause the operation to be applied twice. If the idempotence key is stored but for some reason
the application state is not modified, then the operation has not been applied. If a retry arrives,
it will be filtered out as duplicate as the idempotence key already exists, and the update will be
lost.
   The implication here is that the updates to the application state and idempotence key store
must both occur, or neither must occur. If you know your databases, youll recognize this as a
requirement for transactional semantics. (Well discuss how distributed transaction are achieved
in Chapter XXX) This will ensure exactly-once semantics, which guarantees that all messages will
always be processed exactly once  what we need for idempotence.
   Exactly once does not mean that there are no message transmission failures, retries and no
application crashes. These are all inevitable. The important thing is that the retries eventually
succeed and the result is the always the same.
   Well return to the issue of communications delivery guarantees in later chapters. As Figure
37 illustrates, theres a spectrum of semantics, each with different guarantees and performance
characteristics. At most once delivery is fast and unreliable  this is what the UDP protocol
provides. At least once delivery is the guarantee provided by TCP/IP, meaning duplicates are
inevitable. Exactly-once delivery, as weve discussed here, requires guarding against duplicates and
hence trades off reliability against slower performance.
   As well see, some advanced communications mechanisms can provide our applications with
exactly once semantics. However, these dont operate at Internet scale because of the
performance implications. That is why, as our applications are built on the at least once
semantics of TCP/IP, we must implement exactly once semantics in our APIs that cause state
mutation.

   Figure 37 Communications Delivery Guarantees
Consensus in Distributed Systems
   Crash faults have another implication for the way we build distributed systems. This is best
illustrated by the Two Generals Problem , which is illustrated in Figure 38.
   Imagine a city under siege by two armies. The armies lie on opposite sides of the city, and
the terrain surrounding the city is difficult to travel through and visible to snipers in the city. In
order to overwhelm the city, its crucial that both armies attack at the same time. This will
stretch the citys defenses and make victory more likely for the attackers. If only one army
attacks, then they will likely be repelled. How can the two generals reach agreement on the exact
time to attack, such that both generals know for certain that agreement has been reached? They
both need certainty that the other army will attack at the agreed time.
   To coordinate an attack, the first general sends a messenger to the other, with instructions to
attack at a specific time. As the messenger may be captured or killed by snipers, the sending
general cannot be certain the message has arrived unless they get an acknowledgement
messenger from the second general. Of course, the acknowledgement messenger may be
captured or killed, so even if the original messenger does get through, the first general may
never know. And even if the acknowledgement message arrives, how does the second general
know this, unless they get an acknowledgement from the first general?
   Hopefully the problem is apparent. With messengers being randomly captured or
extinguished, there is no guarantee the two generals will ever reach consensus on the attack
time. In fact, it can be proven that it is not possible to guarantee agreement will be reached. There
are solutions that increase the likelihood of reaching consensus. For example, Game of Thrones
style, each general may send 100 different messengers every time, thus increasing the probability
that at least one will make the perilous journey to the other friendly army and successfully
deliver the message.

   Figure 38 The Two Generals Problem
   The Two Generals problem is analogous to two nodes in a distributed system wishing to
reach agreement on some state, such as the value of a data item that can be updated at either.
Partial failures are analogous to losing messages and acknowledgements. Messages may be lost
or delayed for an indeterminate period of time.
   In fact it can be demonstrated that consensus on an asynchronous network in the presence
of crash faults, where messages can be delayed but not lost, is impossible to achieve within
bounded time. This is known as the FLP Impossibility Theorem .
   Luckily, this is only theoretical limitation, demonstrating its not possible to guarantee
consensus will be reached with unbounded message delays on an asynchronous network. In
reality, distributed systems reach consensus all the time. This is possible because while our
networks are asynchronous, we can establish sensible practical bounds on message delays and
retry after a timeout period. FLP is therefore a worst-case scenario, and as well discuss
algorithms for establishing consensus when we discuss distributed databases.
   Finally, we should note the issue of Byzantine failures. Imagine extending the Two Generals
problem to N Generals, who need to agree on a time to attack. However, in this scenario,
traitorous messengers may change the value of the time of the attack, or a traitorous general
may sense false information to other generals. This class of malicious failures are known as
Byzantine faults and are particularly sinister in distributed systems. Luckily, the systems we
discuss in this book typically live behind well-protected, secure enterprise networks and
administrative environments. This means we can practically exclude handling Byzantine faults.
Algorithms that do address malicious behavior exist, and if you are interested in a practical
example, take a look at Blockchain technologies  and BitCoin .
Time in Distributed Systems
   Every node in a distributed system has its own internal clock. If all the clocks on every
machine were perfectly synchronized, we could always simply compare the timestamps on
events across nodes to determine the precise order they occurred in. If this were reality, many of
the problems well discuss with distributed systems would pretty much go away.
   Unfortunately, this is not the case. Clocks on individual nodes drift due to environmental
conditions like changes in temperature or voltage. The amount of drift varies on every machine,
but values like 10-20 seconds a day are not uncommon. Or with my current coffee machine,
about 15 minutes a day!
   If left unchecked, clock drift would render the time on a node meaningless  like my coffee
machine if I dont correct it every few days. To address this problem, a number of time services
exist. A time service represents an accurate time source, such as a GPS or atomic clock, which
can be used to reset the clock on a node to correct for drift on packet-switched, variable-latency
data networks.
   The most widely used time service is NTP , which provides a hierarchically organized
collection of time servers spanning the globe. The root servers, of which there are around 300
worldwide, are the most accurate. Time servers in the next level of the hierarchy (approximately
20,000) synchronize to within a few milliseconds of the root server periodically, and so on
throughout the hierarchy, a with a maximum of 15 levels. Globally there are more than 175,000
NTP servers.
   Using the NTP protocol, a node in an application running the NTP client can synchronize
to a NTP server. The time on a node is set by a UDP message exchange with one or more NTP
servers. Messages are timestamped and through the message exchange the time taken for transit
is estimated. This becomes a factor in the algorithm used by NTP to establish what the time on
the client should be reset to.  A simple NTP configuration is shown in Figure 39. On a LAN,
machines can synchronize to an NTP server within a small number of milliseconds accuracy.

   Figure 39 Illustrating using the NTP Service
   One interesting effect of NTP synchronization for our applications is that the resetting of
the clock can move the local node time forwards or backwards. This means that if our
application is measuring the time taken for events to occur (e.g. to calculate event latencies), it is
possible that the end time of the event may be earlier than the start time if the ntp protocol has
set the local time backwards.
   In fact, a node has two clocks. These are:
	Time of Day Clock: This represents the number of milliseconds since midnight
January 1970. In Java, you can get the current time using
System.currentTimeMillis(). This is the clock that can be reset by NTP, and
hence may jump forwards or backwards if it is a long way ahead of NTP time.
	Monotonic Clock: This represents the amount of time (in seconds and nanoseconds)
since an unspecified point in the past, such as the last time the system was restarted. It
will only ever move forward, however it again may not be a totally accurate measure of
elapsed time because it stalls during a virtual machine suspension. In Java, you can get
the current monotonic clock time using System.nanoTime().
.
   Applications can use an NTP service to ensure the clocks on every node in the system are
closely synchronized. Its typical for an application to resynchronize clocks on anything from a
one hour to one day time interval. This ensures the clocks remain close in value. Still, if an
application really needs to precisely know the order of events that occur on different nodes,
clock drift is going to make this fraught with danger.
   There are other time services that provide higher accuracy than NTP. Chrony  supports the
NTP protocol but provides much higher accuracy and greater scalability than NTP  the reason
it has recently been adopted by Facebook . Amazon has built the Amazon Time Sync Service
by installing GPS and atomic clocks in its data centers. This service is available for free to all
Amazon cloud customers.
   The take away from this discussion is that our applications cannot rely on timestamps of
events on different node s to represent the actual order of these events. Clock drift even by a
second or two makes cross-node timestamps meaningless to compare. The implications of this
will become clear when we start to discuss distributed databases in detail.

Summary and Further Reading
   This chapter has covered a lot of ground to explain some of the fundamental issues
communications and time in distributed systems. The key issues that should resonate with the
you, dear reader, are as follows:
1.	Communications in distributed systems can transparently traverse many different types
of underlying physical networks  e.g. Wi-Fi, wireless, WANs and LANs.
2.	Communication latencies are highly variable, and influenced by the physical distance
between nodes, and transient network congestion.
3.	Communications can fail due to network communications fabric and router failures
that make nodes unavailable, and individual node failure.
4.	The sockets library has two variants, one that supports TCP/IP for reliable connection-
based communications (SOCK_STREAM), and one for unreliable UDP datagram-based
communications (SOCK_DGRAM).
5.	RMI/RPC technologies build on TCP/IP sockets to provide abstractions for client-
server communications that mirror making local method/procedure calls.
6.	Achieving agreement, or consensus on state across multiple nodes in the presence of
crash faults is not possible in bounded time on asynchronous networks. Luckily, real
networks, especially LANs, are fast and mostly reliable, meaning we can devise
algorithms that achieve consensus in practice.
7.	There is no reliable global time source that nodes in an application can rely upon to
synchronize their behavior. Clocks on individual nodes vary and cannot be used for
meaningful comparisons.

   These issues will pervade the discussions in the rest of this book. Many of the unique
problems and solutions that are adopted in distributed systems stem from these fundamentals.
Theres no escaping them!
   An excellent source for more detailed, more theoretical coverage of all aspects of distributed
systems is George Colouris et at, Distributed Systems: Concepts and Design, 5th Edition, Pearson, 2011.
   Likewise for computer networking, youll find out all you wanted to know and no doubt
more in James Kurose, Keith Ross, Computer Networking: A Top-Down Approach, 7th Edition, Pearson
2017.

CHAPTER 5
__________________________

Application Services
   In this chapter, were going to focus on the pertinent issues in achieving scalability for the
services tier in an application. Well explore API and service design and describe the salient
features of application servers that provide the execution environment for services. Well also
elaborate on topics such as horizontal scaling, load balancing and state management that we
introduced in Chapter 2.
Service Design
   In the simplest case, an application comprises one Internet facing service that persists data to
a local data store, as shown in Figure 40. Clients interact with the service through its published
API, which is accessible across the Internet.
   Lets look at the API and service implementation in more detail.

   Figure 40 A Simple Service
  Application Programming Interface (API)

   An API defines a contract between the client and server. The API specifies the types of
requests that are possible, the data that is needed to accompany the requests, and the results that
will be obtained. APIs have many different variations, as we explored in RPC/RMI discussions
in Chapter 4. While there remains some API diversity in modern applications, the predominant
style relies on HTTP APIs. These are typically, although not particularly accurately, classified as
RESTful.
   REST is actually an architectural style that was defined by Roy Fielding in his PhD thesis .
A great source of knowledge on RESTful APIs and the various degrees to which Web
technologies can be exploited is REST in Practice by Webber, Parastatidis, and Robinson. Here well
just briefly touch on the HTTP CRUD API pattern. This pattern does not fully implement the
principles of REST, but it is widely adopted in Internet systems today.
   CRUD stands for Create, Read, Update, Delete. A CRUD API specifies how clients perform
these operations in a specific business context. For example a user might create a profile, read
catalog items, update their shopping cart and delete items from their order. A HTTP CRUD API
makes these operations possible using four core HTTP verbs, as shown in Table 3.

Verb
Uniform Resource Identifier
Example
Purpose
POST
/skico.com/skiers/{skierID}/{date}
Create a new ski day record for a skier
GET
/skico.com/skiers/(skierID)
Get the profile information for a skier,
returned in a JSON response payload
PUT
/skico.com/skiers/{skierID}
Update skier profile
DELETE
/skico.com/skiers/{skierID}
Delete a skiers profile as they didnt
renew their pass!

   Table 3 HTTP CRUD Verbs
   A HTTP CRUD API applies HTTP verbs on resources identified by Uniform Resource
Identifiers (URIs). In Table 3 for example, a URI that identifies skier 768934 would be:

   /skico.com/skiers/768934

   A HTTP GET request to this resource would return the complete profile information for a
skier in the response payload, such as name, address, number of days visited, and so on. If a
client subsequently sends a HTTP PUT request to this URI, we are expressing the intent to
update the resource for skier 768934  in this example it would be the skiers profile. The PUT
request would provide the complete representation for the skiers profile as returned by the
GET request. Again, this would be as a payload with the request. Payloads are typically
formatted as JSON, although XML and other formats are also possible. If a client sends a
DELETE request to the same URI, then the skiers profile will be deleted.
   Hence the combination of the HTTP verb and URI define the semantics of the API
operation. Resources, represented by URIs, are conceptually like objects in Object Oriented
Design (OOD) or entities in Entity-Relationship (ER) model. Resource identification and
modeling hence follows similar methods to OOD and ER modeling. The focus however is on
resources that need to be exposed to clients in the API. The Further Reading section at the end
of this chapter points to useful sources of information for resource design.
   HTTP APIs can be specified using a notation called OpenAPI . At the time of writing the
latest version is 3.0. A tool called SwaggerHub  is the de facto standard to specify APIs in
OpenAPI. The specification is defined in YAML, and an example is show in Figure 41. It
defines the GET operation on the URI /resorts. If the operation is successful, a 200 response
code is returned along with a list of resorts in a format defined by a JSON schema that appears
later in the specification. If for some weird reason, the query to get a list of resorts operated by
skico.com returns no entries, a 404 response code is returned along with an error message that
is also defined by a JSON schema.
1.	paths:
2.	  /resorts:
3.	    get:
4.	      tags:
5.	        - resorts
6.	      summary: get a list of ski resorts in the database
7.	      operationId: getResorts
8.	      responses:
9.	        '200':
10.	          description: successful operation
11.	          content:
12.	            application/json:
13.	              schema:
14.	                $ref: '#/components/schemas/ResortsList'
15.	        '404':
16.	          description: Resorts not found. Unlikely unless we go broke
17.	          content:
18.	            application/json:
19.	              schema:
20.	                $ref: '#/components/schemas/responseMsg'
21.	         500:
22.	            $ref: "#/responses/Standard500ErrorResponse"
    Figure 41 OpenAPI Example
   It is also possible for the client to see a 500 Internal Error response code. This indicates
that the server encountered an unexpected condition that prevented it from fulfilling the
request. This error code is a generic "catch-all" response. It may be thrown if the server has
crashed, or if the server is temporarily unavailable due to a transient network error.

  Designing Services
   An application server container receives requests and routes them to the appropriate handler
function to process the request. The handler is defined by the application service and
implements the business logic required to generate results from the request. As multiple
simultaneous requests arrive at a service instance, each is typically  allocated a thread context to
execute the request.
   The sophistication of the routing functionality varies widely by technology platform and
language. For example, in Express.js, the container calls a specified function for requests that
match an API signature  known as a route path - and HTTP method. Figure 42 illustrates this
with a method that will be called when the client sends a GET request for a specific skiers
profile, as identified by the value of :skierID.
1.	app.get('/skiers/:skierID', function (req, res) {
2.	  // process the GET request
3.	  ProcessRequest(req.params)
4.	}
    Figure 42 Express.js request routing example
In Java, the Spring framework provides an equally sophisticated method routing technique. It
leverages a set of annotations that define dependencies and implement dependency injection to
simplify the service code. Figure 43 shows an example of annotations usage, namely:

	@RestController  identifies the class as a controller that implements an API and
automatically serializes the return object into the HttpResponse returned from the API.
	@GetMapping  maps the API signature to the specific method, and defines the format of
the response body
	@PathVariable  identifies the parameter as value that originates in the path for URI that
maps to this method
1.	@RestController
2.	public class SkierController {
3.
4.	    @GetMapping("/skiers/{skierID}",
5.	                produces = application/json)
6.	    public Profile GetSkierProfile(
7.	                        @PathVariable String skierID,
8.	                        ) {
9.	          // DB query method omitted for brevity
10.	        return GetProfileFromDB(skierID);
11.	    }
12.	}
    Figure 43 Spring method routing example
   Another Java technology, JEE servlets, also provide annotations, as shown in Figure 44, but
these are simplistic compared to Spring and other higher-level frameworks. The @WebServlet
annotation identifies the base pattern for the URI which should cause a particular servlet to be
invoked. This is /skiers in our example. The class that implements the API method must
extend the HttpServlet abstract class from the javax.servlet.http package and override
at least one method that implements a HTTP request.  The HTTP verbs map to methods as
follows:

	doGet: HTTP GET requests
	doPost: HTTP POST requests
	doPut: HTTP PUT requests
	doDelete: for HTTP DELETE requests

   Each method is passed as parameters a HttpServletRequest and HttpServletResponse
object. The servlet container creates the HttpServletRequest object, which contains members
that represent the components of the incoming HTTP request. This object contains the
complete URI path for the call, and it is the servlets responsibility to explicitly parse and
validate this, and extract path and query parameters if valid. Likewise, the servlet must explicitly
set the properties of the response using the HttpServletResponse object.
   Servlets therefore require more code from the application service programmer to implement.
However, they are likely to provide a more efficient implementation as there is less plumbing
involved compared to the more powerful annotation approaches of Spring et al. A good rule of
thumb is that if a framework is easier to use, it is likely to be less efficient. This is a classic
performance versus ease-of-use trade-off. Well see lots of these in this book
1.	import javax.servlet.http.*;
2.	@WebServlet(
3.	    name = SkiersServlet,
4.	    urlPatterns = /skiers
5.	)
6.	public class SkierServlet extends HttpServlet (
7.
8.	protected void doGet(HttpServletRequest request,
9.	                     HttpServletResponse response) {
10.	  // handles requests to /skiers/{skierID}
11.	  try {
12.	     // extract skierID from the request URI (not shown for brevity)
13.	     String skierID  = getSkierIDFromRequest(request);
14.	     if(skierID == null) {
15.	        // request was poorly formatted, return error code
16.	        response.setStatus(HttpServletResponse.SC_BAD_REQUEST);    }
17.	     else {
18.	        // read the skier profile from the database
19.	        Profile profile = GetSkierProfile (skierID);
20.	        // add skier profile as JSON to HTTP response and return 200
21.	        response.setContentType("application/json");
22.	        response.getWriter().write(gson.toJson(Profile);
23.	        response.setStatus(HttpServletResponse.SC_OK);
24.	     } catch(Exception ex) {
25.	         response.setStatus
26.	           (HttpServletResponse.SC_INTERNAL_SERVER_ERROR);    }
27.
28.	       }
29.	} }
    Figure 44 JEE Servlet Example
  State Management

   State management is a tricky, nuanced topic. The bottom line is that service implementations
that need to scale should avoid storing conversational state. What on earth does that mean?
Lets start by examining the topic of state management with HTTP.
   HTTP is known as stateless protocol. This means each request is executed independently,
without any knowledge of the requests that were executed before it from the same client.
Statelessness implies that every request needs to be self-contained, with sufficient information
provided by the client for the Web server to satisfy the request regardless of previous activity
from that client.
   The picture is a little more complicated that this simple description portrays, however. For
example:
	The underlying socket connection between a client and server is kept open so that the
overheads of connection creation are amortized across multiple requests from a client.
This is the default behavior for versions HTTP/1 and above.
	HTTP supports cookies, which are known as the HTTP State Management
Mechanism . Gives it away really!
	HTTP/2 supports streams, compression, and encryption, all of which require state
management

   So, originally HTTP was stateless, but perhaps not anymore? Armed with this confusion (!),
lets look at application services APIs built on top of HTTP.
   When a user or application connects to a service, it will typically send a series of requests to
retrieve and update information. Conversational state represents any information that is retained
between requests such that the subsequent request can assume the service has retained
knowledge about the previous interactions. Lets explore a simple example.
   In our skier service API, a user may request their profile by submitting a GET request to the
following URI:

   /skico.com/skiers/768934

   They may then use their app to modify their phone number and send a PUT request to a
URI designed for updating this field:

   /skico.com/skiers/phoneno/4123131169

   As this URI does not identify the skier, the service must know the unique identifier of the
skier, namely 768934. Hence for this PUT operation to succeed, the service must have retained
conversational state from the previous GET request.
   Implementing this approach is relatively straightforward. When the service receives the initial
GET request, it creates a session state object that uniquely identifies the client connection. In
reality, this is often performed when a user first connects to or logs in to a service. The service
can then read the skier profile from the database and utilize the session state object to store
conversational state  in our example this would be skierID. When the subsequent PUT
request arrives from the client it uses the session state object to look up the skierID associated
with this session and uses that to update the skiers phone number.
   Services that maintain conversational state are known as stateful services. Stateful services
are attractive from a design perspective as they can minimize the number of times a service
retrieves data (state) from the database and reduce the amount of data that is passed between
clients and the services. For services with light request loads they can make eminent sense and
are promoted by many frameworks to make services easy to build and deploy. For example, JEE
servlets support session management using the HttpSession object, and similar capabilities are
offered by the Session object in ASP.NET.
   As we scale our service implementations however, the stateful approach becomes
problematic. For a single service instance, we have two problems to consider:

1.	If we have multiple client sessions all maintaining session state, this will utilize available
service memory. The amount of memory utilized will be proportional the number of
clients we are maintaining state for. If a sudden spike of requests arrive, how can we be
certain we will not exhaust available memory and cause the service to fail?
2.	We also must be mindful about how long to keep session state available. A client may
stop sending requests but not cleanly close their connection to allow the state to be
reclaimed. All session management approaches support a default session time out. If we
set this to a short time interval, clients may see their state disappear unexpectedly. If we
set the session time out period to be too long, we may degrade service perform as it
runs low on resources.

   In contrast, stateless services do not assume that any conversational state from previous calls
has been preserved. The service should not maintain any knowledge from earlier requests, so
that each request can be processed individually. This requires the client to provide all the
necessary information for the service to process the request and provide a response.
   If we consider our example above, we could transform the PUT request to be stateless by
incorporating the skierID in the URI:

   /skico.com/skiers/768934/phoneno/4123131169

   In reality, this is a pretty dumb API design. As we discussed in the service API section, the
API should in fact transfer the complete resource  the skier profile  with the GET response
and PUT request. This means the GET request for the skier should retrieve and return the
complete skier profile so that the client app can display and modify any data items in the profile
as the user wishes. The complete updated profile is then sent as the payload in the subsequent
PUT operation, and used by the service to update the resource, that is persisted in a database.
This is shown in Figure 45.

   Figure 45 Stateless API Example
   Any scalable service will need stateless APIs. If a service needs to retain state pertaining to
client sessions  the classic shopping cart example - it must be stored externally to the service.
This invariably means an external data store.
   Well revisit this topic later in this chapter during the discussion of horizontal scaling. Thats
when stateless services really come to the fore.
Applications Servers
   Application servers are the heart of a scalable application, hosting the business services that
comprise an application. Their basic role is to accept requests from clients, apply application
logic to the requests, and reply to the client with the request results. Clients may be external or
internal, as in other services in the application that require to use the functionality of a specific
service.
   The technological landscape of application servers is broad and complex, depending on the
language you want to use and the specific capabilities that each offer. In Java, the Java
Enterprise Edition (JEE)  defines a comprehensive, feature rich standards-based platform for
application servers, with multiple different vendor and open source implementations.
   In other languages, the Express.js  server supports Node, Flask supports Python , and in
GoLang a service can be created by incorporating the net/http package. These
implementations are much more minimal and lightweight than JEE and are typically classified
are Web application frameworks. In Java, the Apache Tomcat server  is a somewhat equivalent
technology. Tomcat is open source implementation of a subset of the JEE platform, namely the
Java Servlet, JavaServer Pages, Java Expression Language and Java WebSocket technologies.
   Figure 46 depicts a simplified view of the anatomy of Tomcat. Tomcat implements a servlet
container, which is an execution environment for application-defined servlets. Application
defined Servlets are loaded into this container, which provides lifecycle management and a
multithreaded runtime environment.
   Request arrive at the IP address of the server, which is listening for traffic on specific ports.
For example, by default Tomcat listens on port 8080 for HTTP requests and 8443 for HTTPS
requests. Incoming requests are processed by one or more listener threads. These create a
TCP/IP socket connection between the client and server. If network requests arrive at a
frequency that cannot be processed by the TCP listener, pending requests are queued up in the
Sockets Backlog. The size of the backlog is operating system dependent. In most Linux versions
the default is 100.
   Once a connection is established, the TCP requests are marshalled by, in this example, a
HTTP Connector which generates the HTTP request that the application can process. The HTTP
request is then dispatched to an application service thread to process. Application container
threads are managed in a thread pool, essentially a Java Executor, which by default in Tomcat
is a minimum size of 25 threads and a maximum of 200. If there are no available threads to
handle a request, the container maintains them in a queue of runnable tasks and dispatches these
as soon as a thread becomes available. This queue by default is size Integer.MAX_VALUE  that
is, essentially unbounded . If a thread remains idle for by default, 60 seconds, it is killed to free
up resources in the JVM.

   Figure 46 Anatomy of a Web application server
   For each request, the method that corresponds with the HTTP request is invoked in a
thread. The servlet method processes the HTTP request headers, executes the business logic,
and constructs a response that is marshalled by the container back to a TCP/IP packet and sent
over the network to the client.
   In processing the business logic, servlets often need to query an external database. This
requires each thread executing the servlet methods to obtain a database connection and execute
database queries. As database connections are limited resources and consume resources in both
the client and database server, a fixed size connection pool is typically utilized. The pool hands
out open connection to requesting threads on demand.
   When a servlet wishes to submit a query to the database, it therefore requests an open
connection from the pool. If one is available, access to the connection is granted to the servlet
until it indicates it has completed its work. At that stage the connection is returned to the pool
and made available for another servlet to utilize. As the thread pool is typically larger than the
connection pool, a servlet may request a connection when none are available. The connection
pool maintains a request queue and hands out open connections on a FIFO basis, and threads
in the queue are blocked until there is availability.
   An application server framework such as Tomcat is hence highly configurable to different
handle different workloads. For example, the size of the thread and database connection pools
can be specified in configuration files that are read at startup.
   The complete Tomcat container environment runs within a single JVM, and hence
processing capacity is limited by the number of vCPUs available and the amount of memory
allocated as heap size. Each allocated thread consumes memory, and the various queues in the
request processing pipeline consume resources while requests are waiting. This means that
request latency will be governed by both the request processing time in the servlet and the time
spent waiting in queues for threads and connections to become available.
   In a heavily loaded server with many threads, context switching may start to degrade
perform, and available memory may be become limited. If perform degrades, queues grow as
requests wait for resources. This consumes more memory. If more requests are received than
can be queued up and processed by the server, then new TCP/IP connections will be refused,
and clients will see errors. Eventually, an overloaded server will run out of resources and start
throwing exceptions and crash.
   Time spent tuning configuration parameters to efficiently handle anticipated loads is rarely
wasted. A rule-of-thumb is that CPU utilization that consistently exceeds the 70-80% range is a
signal of overload. Similar insights exist for memory usage. Once any resource gets close to full
utilization, systems tend to exhibit less predictable performance, as more time is spent, for
example thread context switching and garbage collecting. This inevitably effects latencies and
throughput.
   Monitoring tools available with Web application frameworks enable engineers to gather a
range of important metrics, including latencies, active requests, queue sizes and so on. These are
invaluable for carrying out data-driven experiments that lead to performance optimization.
   Java-based application frameworks such as Tomcat will invariably support the JMX  (Java
Management Extensions) framework, which is a standard part of the Java Standard Edition
platform. JMX enables frameworks to expose monitoring information based on the capabilities
of MBeans (Managed Beans), which represent a resource of interest (e.g. thread, database
connections usage). This enables an eco-system of tools to offer capabilities for monitoring
JMX-supported. These range from JConsole  which is available in the JDK by default, to
powerful open source technologies such as JavaMelody  and many expensive commercial
offerings.
Horizontal Scaling
   A core principle of scaling a system is being able to easily add new processing capacity to
handle increased load. For most systems, a simple and effective approach is deploying multiple
instances of stateless server resources and using a load balancer to distribute the requests across
these instances. This is known as horizontal scaling and illustrated in Figure 47.

   Figure 47 Simple Load Balancing Example
   These two ingredients, namely stateless service replicas and a load balancer, are both
necessary. Lets explain why.
   Service replicas are deployed on their own (virtual) hardware. Hence if we have two replicas,
we double our processing capacity. If we have ten replicas, we have 10x capacity. This enables
our system to handle increased loads. The aim of horizontal scaling is to create a system
processing capacity that is the sum of the total resources available
   The servers need to be stateless, so that any request can be sent to any service replica to
handle. This decision is made by the load balancer, which can use various policies to distribute
requests. If the load balancer can keep each service replica equally busy, then we are effectively
using the processing capacity provided by the service replicas.
   Horizontal scaling also increases availability. With one service instance, if it fails, the service
is unavailable. This is known as a single point of failure (SPoF)  a bad thing, and one to avoid
in any scalable distributed system. Multiple replicas increase availability. If one replica fails,
requests can be directed to any  they are stateless, remember  replica. The system will have
reduced capacity until the failed server is replaced, but it will still be available. Which is
important. The ability to scale is crucial, but if a system is unavailable, then the most scalable
system ever built is still somewhat ineffective!
Load Balancing
   Load balancing aims to effectively utilize the capacity of a collection of services to optimize
the response time for each request. This is achieved by distributing requests across the available
services as evenly as possible and avoiding overloading some services while underutilizing
others. Clients send requests to the IP address of the load balancer, which redirects requests to
target services, and relays the results back to the client. This means clients never contact the
services directly, which is also beneficial for security as the services can live behind a security
perimeter and not be exposed to the Internet.
   Load balancers may act at the network level or the application level. These are often called Layer 4
and Layer 7 load balancers respectively. These names refer to network transport layer at Layer 4
in the Open Systems Interconnection (OSI) Reference Model , and the application layer at
Layer 7. The OSI model defines network functions in seven abstract layers. Each layer defines
standards for how data is packaged and transported. Lets explore the differences between the
two techniques.
   Network level load balancers distribute requests at the network connection level, operating
on individual TCP or UDP packets.  Routing decisions are made on the basis of client IP
addresses. Once a target service is chosen, the load balancer uses a technique called Network
Address Translation (NAT). This changes the destination IP address in the client request packet
from that of the load balancer to that of the chosen target. When a response is received from
the target, the load balancer changes the source address recorded in the packet header from the
targets IP address to its own. Network load balancers are relatively simple as they operate on
the individual packet level. This means they are extremely fast, as they provide few features
beyond choosing a target service and performing NAT functionality.
   In contrast, application level load balancers reassemble the complete HTTP request and base
their routing decisions on the values of the HTTP headers and on the actual contents of the
message. For example, a load balancer can be configured to send all POST requests to a subset
of available services, or distribute requests based on a query string in the URI. Application load
balancers are sophisticated reverse proxies. The richer capabilities they offer means they are
slightly slower than network load balancers, but the powerful features they offer can be utilized
to more than make up for the overheads incurred.
   In general, a load balancer has the following features which are explained in the following
sections:

1.	Load distribution policies
2.	Health monitoring
3.	Elasticity
4.	Session affinity

  Load Distribution Policies
   Load distribution policies dictate how the load balancer chooses a target service to process a
request. Any load balancer worth its salt will offer several load distribution policies  HAProxy
offers 10 in fact . The following are probably the most commonly supported across all load
balancers:
	round-robin: the load balancer distributes requests to available servers in a round-robin
fashion
	least connections:  the load balancer distributes new requests to the server with the least
open connections
	HTTP header field: the load balancer directs requests based on the contents of a specific
HTTP header field. For example all requests with the header field X-Client-
Location:US,Seattle could be routed to a specific set of servers.
	HTTP operation: the load balancer directs requests based on the HTTP verb in the
request

   Load balancers will also allow services to be allocated weights. For example, standard service
instances in the load balancing pool may have 4 vCPUs and each is allocated a weight of 1. If a
service with 8 vCPUs is added, it can be assigned a weight of 2 so the load balancer will send
twice as many requests its way.

  Health Monitoring
   A load balancer will periodically sends pings and attempts connections to test the health of
each service in the load balancing pool. These tests are called health checks. If a service
becomes unresponsive or fails connection attempts, it will be removed from the load balancing
pool and  no requests will be sent to that host. If the connection to the service has experienced
a transient failure, the load balancer will reincorporate the service once it becomes available and
healthy. If, however it has failed, the service will be removed from the load balancer target pool.
  Elasticity
   Spikes in request loads can cause the service capacity available to a load balancer to become
saturated, leading to longer response times and eventually request and connection failures.
Elasticity is the capability of a load balancer to dynamically provision new service capacity to
handle an increase in requests. As load increases, the load balancer starts up new resources and
directs requests to these, and as load decreases the load balancer stops services that are no
longer needed.
   An example of elastic load balancing is the Amazon Web Services (AWS) Auto-Scaling
groups. An Auto Scaling group is a collection of services available to a load balancer that is
defined with a minimum and maximum size. The load balancer will ensure the group always has
the minimum numbers of services available, and the group will never exceed the maximum
number. The actual number available at any time will depends on the client request load the
load balancer is handling for the services in the group. This scheme is illustrated in Figure 48.

   Figure 48 Elastic Load Balancing
   Typically, there are two ways to control the number of services in a group. The first is based
on a schedule, when the request load increases and decreases are predictable. For example, you
may have an online entertainment guide and publish the weekend events for a set of major cities
at 6pm on Thursday. This generates a higher load until Sunday at noon. An Auto Scaling group
could easily be configured to provision new services at 6pm Thursday and reduce the group size
to the minimum at noon Sunday.
   If increased load spikes are not predictable, elasticity can be controlled dynamically with a set
of configuration parameters. The most commonly used parameter is average CPU utilization
across the active services. For example, an Auto Scaling group can be defined to maintain
average CPU utilization at 70%. If this value is exceeded, the load balancer will start one or
more new services instances until the 70% threshold is reached. Instances need time to start 
often a minute or more - and hence a warmup period can be defined until the new instance is
considered to be contributing to the groups capacity. When the group average CPU utilization
drops below 70%, scale in or scale down will start and instances will be automatically stopped and
removed from the pool.
   Elasticity is a key feature that allows services to scale dynamically as demand grows. For
highly scalable systems, it is a mandatory capability. Like all advanced capabilities however, it
brings new issues for us to consider in terms of downstream capacity and costs. Well discuss
these in Chapter XXXX.

  Session Affinity
   Session affinity, or sticky sessions, are a load balancer feature for stateful services. With
sticky sessions, the load balancer sends all requests from the same client to the same service
instance. This enables the service to maintain in-memory state about each specific client session.
   There are various ways to implement sticky sessions. For example, HAProxy provides a
comprehensive set of capabilities to maintain client requests on the same service in the face of
service additions, removals and failures . AWS Elastic Load Balancing generates an HTTP
cookie that identifies the service a clients session is associated with. This cookie is returned to
the client, which must send it in subsequent request to ensure session affinity is maintained.
   Sticky sessions can be problematic for highly scalable systems. They lead to a load imbalance
problem, in which, over time, clients are not evenly distributed across services. This is illustrated
in Figure 49, where two clients are connected to one service while another service remains idle.

   Figure 49 Load Imbalance with Sticky Sessions
   Load imbalance occurs because client sessions last for varying amounts of time. Even if
sessions are evenly distributed initially, some will terminate quickly while others will persist. In a
lightly loaded system, this tends to not be an issue. However, in a system with millions of
sessions being created and destroyed constantly, load imbalance is inevitable. This will lead to
some services being underutilized, while others are overwhelmed and may potentially fail due to
resource exhaustion.
   Stateful services have other downsides. When a service inevitably fails, how do the clients
connected to that server recover the state that was being managed? If a service instance
becomes slow due to high load, how do clients respond? In general stateful servers create
problems that in large scale systems are difficult to design around and manage.
   Stateless services have none of these downsides. If one fails, clients get an exception and
retry, with their request routed to another live service. If a service is slow due to a transient
network outage, the load balancer takes it out of the service group until it passes health checks
of fails. All state is either externalized or provided by the client in each request, so service
failures can be handled easily by the load balancer.
   Stateless services enhance scalability, simplify failure scenarios and ease the burden of service
management. For scalable applications, these advantages far outweigh the disadvantages, and
hence their adoption in most major Internet sites such as Netflix.
Summary and Further Reading
   Services are the heart of a scalable software system. They define the contract defined as an
API that specifies their capabilities to clients. Services are defined and execute in an application
server container environment that hosts the service code and routes incoming API requests to
the appropriate processing logic. Application servers are highly programming language
dependent, but in general provide a multithreaded programming model that allows services to
process many requests simultaneously. If the threads in the container thread pool are all utilized,
the application server queues up requests until a thread becomes available.
   As request loads grow on a service, we can scale it out horizontally using a load balancer to
distribute requests across multiple instances. This architecture also provides high availability as
the multiple service configuration means the application can tolerate failures of individual
instances. The service instances are managed as pool by the load balancer, which utilizes a load
distribution policy to choose a target service for each request. Stateless services scale easily and
simplify failure scenarios by allowing the load balancer to simply resend requests to responsive
targets. Although most load balancers will support stateful services using a feature called sticky
sessions, stateful services make load balancing and handling failures more complex. Hence, they
are not recommended for highly scalable services.
   API design is a topic of great complexity and debate. An excellent overview of basic API
design and resource modeling is https://www.thoughtworks.com/insights/blog/rest-api-
design-resource-modeling.
   The Java Enterprise Edition (JEE) is an established and widely deployed server-side
technology. It has a wide range of abstractions for building rich and powerful services. The
Oracle tutorial is an excellent starting place for appreciating this platform -
https://docs.oracle.com/javaee/7/tutorial/.
   Much of the knowledge and information about load balancers is buried in the
documentation provided by the technology suppliers. You choose your load balancer and then
dive into the manuals. For an excellent, broad perspective on the complete field of load
balancing, this is a good resource.
   Tony Bourke, Server Load Balancing, OReilly Publishing?
CHAPTER 6
__________________________

Caching
   Caching is an essential ingredient of a scalable system. Caching makes the results of
expensive queries and computations available for reuse by subsequent requests at low cost. By
not having to reconstruct the cached results for every single request, the capacity of the system
is increased, and it is hence able to scale to handle greater workloads.
   Caches exist in many places in an application. The CPUs that run applications have multi-
level, fast hardware caches to reduce relatively slow main memory accesses. Database engines
can make use of main memory to cache the contents of the data store in memory so that in
many cases queries do not have to touch relatively slow disks.
   This chapter covers:
	application based caching, in which service business logic incorporates the caching and
access of precomputed results
	Web based caching, which exploits mechanisms built into the HTTP protocol to enable
caching of results within the infrastructure provided by the Internet.
Application Caching
   Application caching is designed to improve request responsiveness by storing the results of
queries and computations in memory so they can be subsequently served by later requests. For
example, think of an online newspaper site. Once posted, articles change infrequently and hence
can be cached and on first access and reused on all subsequent requests until the article is
updated.
   In general, caching relieves databases of heavy read traffic, as many queries can be served
directly from the cache. It also reduces computation costs for objects that are expensive to
construct, for example those needing queries that span several different databases. The net
effect is to reduce the computational load on our services and databases and create head room
for more requests.
   Caching requires additional resources, and hence cost, to store cached results. However, well
designed caching schemes are low cost compared to upgrading database and service nodes to
cope with higher request loads. As an indication of the value of caches, approximately 3% of
infrastructure at Twitter is dedicated to application level caches.  At Twitter scale, that is a lot
of infrastructure!
   Application level caching exploits dedicated distributed cache engines. The two predominant
technologies in this area are memcached  and Redis . Both are essentially distributed in-
memory key-value stores designed for arbitrary data (strings, objects) from the results of
database queries or downstream service API calls. The cache appears to services as a single
store, and objects are allocated to individual cache servers using hashing on the object key.
   The basic scheme is shown in Figure 50. The service first checks the cache to see if the data
it requires is available. If so, it returns the cached contents as the results  we have what is
known as a cache hit. If the data is not in the cache  a cache miss - the service retrieves the
requested data from the database and writes the query results to the cache so it is available for
subsequent client requests without querying the database.

   Figure 50 Application Level Caching
   Lets return to our mythical winter resort business for a use case. At a busy resort, skiers and
boarders can use their mobile app to get an estimate of the lift wait times across the resort. This
enables them to plan and avoid congested areas where they will have to wait to ride a lift for say
15 minutes (or sometimes more!).
   Every time a skier loads a lift, a message is sent to the companys service that collects data
about skier traffic patterns. Using this data, the system can estimate lift wait times from the
number of skiers who ride a lift and the rate they are arriving. This is an expensive calculation as
it requires aggregating potentially 10s of thousands of lift ride records and performing the wait
time calculation. For this reason, once the results are calculated, they are deemed valid for five
minutes. Only after this time has elapsed is a new calculation performed and results produced.
   Figure 51 shows an example of how a stateless LiftWaitService might work. When a
request arrives, the service first checks the cache to see if the latest wait times are available. If
they are, the results are immediately returned to the client. If the results are not in the cache, the
service calls a downstream service which can perform the lift wait calculations and returns them
as a List. These results are then stored in the cache and then returned to the client.
   Cache access requires a key with which to associate the results with. In this example we
construct the key with the string liftwaittimes: concatenated with the resort identifier that
is passed by the client to the service. This key is hashed by the cache to identify the server where
the cached value resides. When we write a new value to the cache (line 8), we pass a value of
300 seconds as a parameter to the put operation. This is known as a time to live, or TTL value. It
tells the cache that after 300 seconds this key-value pair should be evicted from the cache as the
value is no longer relevant.
   When the cache value is evicted, the next request will calculate the new values and store
them in the cache. But while the cache value is valid, all requests will utilize it, meaning there is
no need to perform the expensive lift wait time calculation for every call. Hence if we get N
requests in a 5 minute period, N-1 are served from the cache. Imagine if N is 20,000. This is a
lot of expensive calculations saved.
1.	public class LiftWaitService {
2.
3.	  public List getLiftWaits(String resort) {
4.	    List liftWaitTimes = cache.get(liftwaittimes: + resort);
5.	      if (liftWaitTimes == null) {
6.	         liftWaitTimes = skiCo.getLiftWaitTimes(resort);
7.	         // add result to cache, expire in 300 seconds
8.	         cache.put("liftwaittimes:" + resort, liftWaitTimes, 300);
9.	      }
10.	    return liftWaitTimes;
11.	  }
12.	}
    Figure 51 Caching Example
   Using an expiry time like the TTL is a common way to invalidate cache contents. It ensures a
service doesnt deliver stale, out of date results to a client. It also enables the system to have
some control over cache contents, which are typically limited. If cached items are not flushed
periodically, the cache may fill up. In this case, a cache will adopt a policy such as least recently
used or least accessed to choose cached values to evict and create space for more current, timely
results.
   Application caching can provide significant throughput boosts, reduced latencies, and
increased client application responsiveness. The key to achieving these desirable qualities is to
satisfy as many requests as possible from the cache. This is known as the cache hit rate. The
general design principle is to maximize the cache hit rate and minimize the cache miss rate 
when a request cannot be satisfied by cached items. When a cache miss occurs, the request must
be satisfied through querying databases or downstream services. The results of the request can
then be written to the cache and hence be available for further accesses.
   Theres no hard and fast rule on what the cache hit rate should be, as it depends on the cost
of constructing the cache contents and the update rate of cached items. Ideal cache designs have
many more reads than updates. This is because when an item must be updated, the application
needs to invalidate cache entries that are now stale because of the update. This means the next
request will result in a cache miss.
   When items are updated regularly, the cost of cache misses can negate the benefits of the
cache. Service designers therefore need to carefully consider query and update patterns an
application experiences, and construct caching mechanisms that yield the most benefit. It is also
crucial to monitor the cache usage once a service is in production to ensure the hit and miss
rates are in line with design expectations. Caches will provide both management utilities and
APIs to enable monitoring of the cache usage characteristics. For example, memcached is
accessible through a telnet session. A large number of statistics are available, including the hit
and miss counts as shown in the snippet in Figure 52.
1.	STAT get_hits 98567
2.	STAT get_misses 11001
3.	STAT evictions 0
    Figure 52 Example of memcached monitoring output
   Application level caching is also known as the cache-aside pattern . The name references the
fact that the application code effectively bypasses the data storage systems if the required
request results are available. This contrasts with other caching patterns, in which the application
always reads from and writes to the cache. These are known as read-through, write-through and write-
behind caches as explained below:

	Read-through: The application satisfies all requests by accessing the cache. If the data
required is not available in the cache, a loader in invoked to access the data systems and
store the results in the cache for the application to utilize.
	Write-through: The application always writes updates to the cache. When the cache is
updated, a writer is invoked to write the new cache values to the database. When the
database is updated, the application can complete the request.
	Write-behind: Like write-through, except the application does not wait for the value to
written to the database from the cache. This increases request responsiveness at the
expense of possible lost updates if the cache server crashes before a database update is
completed. This is also known as a write-back cache, and internally is the strategy used
by most database engines.

   The beauty of these caching approaches is that they simplify application logic. Applications
always utilize the cache for reads and writes, and the cache provides the magic to ensure the
cache interacts appropriately with the backend storage systems. This contrasts with the cache-
side pattern, in which application logic must be cognizant of cache misses.
   The application still needs to make this magic happen of course. These strategies require a
cache technology which can be augmented with an application-specific handler that performs
database reads and writes when the application accesses the cache. For example, NCache
supports provider interfaces that the application implements. These are invoked automatically
on cache misses for read-through caches and on writes for write-through caches. Other such
caches are essentially dedicated database caches, and hence require cache access to be identical
to the underlying database model. An example of this is Amazons DynamoDB Accelerator
(DAX).  DAX sits between the application code and DynamoDB, and transparently acts as a
high-speed in memory cache to reduce database access times.
   One significant advantage of the cache-aside strategy is that it is resilient to cache failure. In
such circumstances, as the cache is unavailable, all requests are essentially handled as a cache
miss. Performance will suffer, but services will still be able to satisfy requests. In addition,
scaling cache-aside platforms such as Redis and Memcached is straightforward due their simple,
distributed key-value store model. For these reasons, the cache-aside pattern is the primary
approach seen in massively scalable systems.
Web Caching
   One of the reasons that Web sites are so highly responsive is that the Internet is littered with
Web caches. Web caches stores a copy of a given resource for a defined time period. The caches
intercept client requests and if they have a requested resource cached locally, it returns the copy
rather than forwarding the request to the target service. Hence many requests can be satisfied
without placing a burden on the service. Also, as the caches are closer to the client, the requests
will have lower latencies.
   Figure 53 gives an overview of the Web caching architecture. Multiple levels of caches exist,
starting from the clients local Web browser cache, local proxy caches within organizations and
Internet Service Providers, and reverse proxy caches that exist within the services execution
domain. Web browser caches are also known as private caches (for a single user) and proxy
caches are shared caches that support requests from multiple users.
   Caches typically store the results of GET requests only, and the cache key is the URI of the
associated GET. When a client sends a GET request, it may be intercepted by one or more
caches along the request path. Any cache with a fresh copy of the requested resource may
respond to the request. If no cached content is found, the request is served by the service
endpoint, which is also called the origin server.

   Figure 53 Web Caches in the Internet
   Services can control what results are cached and for how long they are stored by using
HTTP caching directives. Services set these directives in various HTTP response headers, as
shown in the simple example in Figure 54. We will describe these directives in the following
subsections.
1.	Response:
2.	HTTP/1.1 200 OK Content-Length: 9842
3.	Content-Type: application/json
4.	Cache-Control: public
5.	Date: Fri, 26 Mar 2019 09:33:49 GMT
6.	Expires: Fri, 26 Mar 2019 09:38:49 GMT
   Figure 54 Example HTTP Response with caching directives
  Cache-Control
   The Cache-Control HTTP header can be used by client requests and service responses to
specify how the caching should be utilized for the resources of interest.

*	no-store: Specifies that a resource from a request response should not be cached. This
is typically used for sensitive data that needs to be retrieved from the origin servers each
request.
*	no-cache: Specifies that a cached resource must be revalidated with an origin server
before use. We discuss revalidation in the Etag subsection below.
*	private: Specifies a resource can be cached only by a user-specific device such as a
Web browser
*	public: Specifies a resource can be cached by any proxy server
*	max-age: defines the length of time in seconds a cached copy of a resource should be
retained. After expiration, a cache must refresh the resource by sending a request to the
origin server.

  Expires and Last-Modified
   The Expires and Last-Modified HTTP headers interact with the max-age directive to
control how long cached data is retained.
   Caches have limited storage resources and hence must periodically evict items from memory
to create space. To influence cache eviction, services can specify how long resources in the
cache should remain valid, or fresh. Once this time period for a cached resource expires, it
becomes stale and may become a candidate for eviction. When a request arrives for a fresh
resource, the cache serves the locally stored results without contacting the origin server.
   Freshness is calculated using a combination of header values. The "Cache-Control: max-
age=N" header is the primary directive, and this value the specifies the freshness period in
seconds.
   If max-age is not specified, the Expires header is checked next. If this header exists, then it
is used to calculate the freshness period. As a last resort, the Last-Modified header can specify
the freshness lifetime based on a heuristic calculation that the cache can support. This is usually
calculated as (Date header value  Last-Modified header value)*0.1.

  Etag
   HTTP provides another directive that controls cache item freshness. This is known as an
Etag. An Etag is an opaque value that van be used by a cache to check if a cached resource is
still valid. Lets explain this using another winter sports example.
   A ski resort posts a weather report at 6am every day during the ski season. If the weather
changes during the day, the resort updates the report. Sometimes this happens two or three
times each day, and sometimes not at all if the weather is stable. When a request arrives for the
weather report, the service responds with a maximum age to define cache freshness, and also an
ETag that represents the version of the weather report that was last issued. This is shown in
Figure 55, which tells a cache to treat the weather report resource as fresh for at least 3600
seconds, or 60 minutes.
1.	Request:
2.	GET /skico.com/weather/Blackstone
3.
4.	Response:
5.	HTTP/1.1 200 OK Content-Length: ...
6.	Content-Type: application/json
7.	Date: Fri, 26 Mar 2019 09:33:49 GMT
8.	Cache-Control: public, max-age=3600
9.	ETag: 09:33:49"
<!-- Content omitted -->

   Figure 55 HTTP Etag Example
   For the next hour, the cache simply serves this cached weather report to all clients who issue
a GET request. This means the origin servers are freed from processing these requests  the
outcome that we want from effective caching. After an hour though, the resource become stale.
Now, when a request arrives for a stale resource, the cache forwards it to the origin server  with
a If-None-Match directive along with the Etag to enquire if the resource, in our case the
weather report, is still valid. This is known as revalidation.
   There are two possible responses to this request.

1.	If the Etag in the request matches the value associated with the resource in the services,
the cached value is still valid. The origin server can therefore return a 304 (Not
Modified) response, as shown in Figure 56. No response body is needed as the cached
value is still current, thus saving bandwidth, especially for large resources. The response
may also include new cache directives to update the freshness of the cached resource.
2.	The origin server may ignore the revalidation request and respond with a 200 response
code and a response body and Etag representing the latest version of the weather report.
1.	Request:
2.	GET /upic.com/weather/Blackstone
3.	If-None-Match: "09:33:49
4.	Response:
5.	HTTP/1.1 304 Not Modified
    Figure 56 Validating an Etag
   In the service implementation, a mechanism is needed to support revalidation. In our
weather report example, one strategy is as follows:

1.	Generate new daily report: The weather report is constructed and stored in a database.
The service also creates a new cache entry that identifies the weather report resource and
associates an Etag with this version of the resource, for example {#resortname-weather,
Etag value}.
2.	GET requests: When a GET request arrives, return the weather report and the Etag.
This will also populate Web caches along the network response path,
3.	Conditional GET requests: Lookup the Etag value in cache at {#resortname-
weather} and return 304 if the value has not changed. If the cached Etag has changed,
return 200 along with the latest weather report and a new Etag value.
4.	Update weather report: A new version of the weather report is stored in the database
and the cached Etag value is modified to represent this new version of the response.

   When used effectively, Web caching can significantly reduce latencies and save network
bandwidth. This is especially true for large items such as images and documents. Further, as
Web caches handle requests rather than application services, this reduces the request load on
origin servers, creating additional capacity.
   Proxy caches such as Squid  and Varnish  are extensively deployed in the Internet. Scalable
applications therefore exploit the powerful facilities provided by HTTP caching to exploit his
caching infrastructure.
Summary and Further Reading
   Caching is an essential component of any scalable distribution. Caching stores information
that is requested by many clients in memory and serves this information as the results to client
requests. While the information is still valid, it can be served potentially millions of times
without the cost of recreation.
   Application caching using a distributed cache is the most common approach to caching in
scalable systems. This approach requires the application logic to check for cached values when a
client request arrives and return these if available. If the cache hit rate is high, with most
requests being satisfied with cached results, load on backend services and database can be
considerably reduced.
   The Internet also has a built in, multilevel caching infrastructure. Applications can exploit
this through the use of cache directives that are part of HTTP headers. These directives enable a
service to specify what information can be cached, for how long it should be cached, and
protocol for checking to see if a stale cache entry is still valid.

?

CHAPTER 7
__________________________

Asynchronous Messaging Systems
Introduction to Messaging
  Messaging Primitives

  Message Persistence

  Publish-Subscribe

Example: RabbitMQ
  Producer-Consumer example

  Persistent Messages

  Message Distribution

  Message Filtering

  RabbitMQ RPC
Clustering and Mirroring

Messaging Patterns
  Competing Consumers

  At Least Once Processing

  Poison Messages
Summary and Further Reading

?
CHAPTER 8
__________________________

Serverless Processing Systems
   Scalable systems experience widely varying patterns of usage. For some applications, load
may be high during business hours and low or non-existent during the evening. Other
applications, for example an online concert ticket sales system, might have low background
traffic for 99% of the time. When tickets for a major series of shows are released, the demand
can spike by 100x of normal load for a number of hours before dropping down to background
levels. Elastic load balancing, as described in Chapter XXX, is one approach for handling these
spikes. Another is cloud-based serverless computing, which well examine in this chapter.
The Attractions of Serverless
   The transition of major organizational IT systems from on-premise to public cloud
platforms deployments seems inexorable. Organizations from startups to Government agencies
to multinationals see clouds as digital transformation platforms and a foundational technology
to improve business continuity.
   Two of the great attractions of cloud platforms is their pay-as-you-go billing and ability to
rapidly scale up (and down) virtual resources to meet fluctuating workloads and data volumes.
This ability to scale of course doesnt come for free. Your applications need to be architected to
leverage the scalable services provided by cloud platforms. And of course, as we discussed in
Chapter XXX, cost and scale are indelibly connected. The more resources a system utilizes for
extended periods, the larger the cloud bills will be at the end of the month.
   And monthly clouds bills can be big. Really big. Even worse, unexpectedly big! Cases of
sticker shock for significant cloud overspend are rife  in one survey 69% of respondents
regularly overspent on their cloud budget by more than 25%.  One well known case spent
$500K on an Azure task before it was noticed. Reasons attributed for overspending are many,
including lack of deployment of auto-scaling solutions, poor long-term capacity planning, and
inadequate exploitation of cloud architectures leading to sub-optimal system footprints.
   On a cloud platform, architects are confronted with a myriad of architectural decisions.
These are broad, in terms of the overall architectural pattern or style the systems adopts  e.g.
microservices, n-tier, event driven  and narrow, specific to individual components and the
cloud services that the system is built upon. In this sense, architecturally significant decisions
pervade all aspects of the system design and deployment on the cloud. And the consequences of
these decisions are highly apparent when you receive your monthly cloud spending bill.
   Traditionally, cloud applications have been deployed on an Infrastructure-as-a-Service (IaaS)
platform utilizing virtual machines (VMs). In this case, you pay for the resources you deploy
regardless of how highly utilized they are. If load increases, the application spins up new virtual
machines to increase capacity, typically using the cloud-provided load balancing service. Your
costs are essentially proportional to the type of VMs you choose, the duration they are deployed
for, and the amount of data the application stores and transmits.
   Since around 2016, major cloud providers have offered an alternative to explicitly
provisioning virtual processing resources. Known as serverless platforms, they do not require any
compute resources to be statically provisioned. Using technologies such as AWS Lambda or
Google App Engine (GAE), the application code is loaded and executed on demand, when
requests arrive. If there are no active requests, there are essentially no resources in use and no
charges to meet. Serverless platforms also manage autoscaling (up and down) for you. As
simultaneous requests arrive, additional processing capacity is created to handle requests and,
ideally, provide consistently low response times. When request loads drop, additional processing
capacity is decommissioned, and no charges are incurred.
   Every serverless platform varies in the details of its implementation. For example, a limited
number of mainstream programming languages and application server frameworks are typically
supported. Platforms also provide several configuration settings that can be used to balance
performance, scalability and costs. In general, costs are proportional to the type of server
instance chosen to execute a request, the number of requests and processing duration for each
request, and/or how long each application server instance remains resident on the serverless
infrastructure.
   Welcome to the world of cloud deployments. Every platform is proprietary and different is
subtle ways. The devil is, as usual, in the details. So lets explore some of those devilish details
for the Google App Engine and AWS Lambda platforms
Google App Engine
  The Basics
   Google App Engine (GAE) was the first offering from Google as part of what is now
known as the Google Cloud Platform. It has been in general release since 2011, and enables
developers to upload and execute HTTP-based application services on Googles managed cloud
infrastructure.
   GAE supports developing applications in Go, Java, Python,  Node.js,  PHP, .NET, and
Ruby. To build an application on GAE, developers can utilize common HTTP-based
application frameworks that are built with the GAE runtime libraries provided by Google. For
example, in Python, applications can utilize Flask, Django and web2py, and in Java the primary
supported platform is servlets built on the Jetty JEE web container.
   Application execution is managed dynamically by GAE, which launches compute resources
to meet request demand. Applications generally access a managed persistent storage platform
such as Googles Firestore  or Google Cloud SQL , or interact with a messaging service like
Googles Cloud PubSub .
   GAE comes in two flavors, known as the standard environment and the flexible
environment. The basic difference is that the standard environment is more closely managed by
GAE, with development restrictions in terms of language versions supported, but is able to
scale rapidly in response to increased loads. The flexible environment, as its name hints at, gives
more options in terms of development capabilities that can be used, but is not as suitable to
rapid scaling. The next two subsections expand on these two alternatives, and in the rest of
chapter, well focus on the highly scalable standard environment.
  GAE Standard Environment
   In the standard environment, developers upload their application code to a GAE project
that is associated with a base URL. This code must define HTTP endpoints that can be invoked
by clients making requests to the project URL. When a request is received, GAE will route it to
a processing instance to execute the application code. These are known as resident instances for
the application and are the major component of the cost incurred for utilizing GAE.
   Each project configuration can specify a collection of parameters that control when GAE
loads a new instance and invokes a resident instance. The two simplest settings control the
minimum and maximum instances that GAE will have resident. The minimum can be zero,
which is perfect for applications that have periods of inactivity, as this incurs no costs. When a
request arrives, GAE dynamically loads an application instance and invokes the processing for
the endpoint. Multiple simultaneous requests can be sent to the same instance, up to some
configured limit (more on this when we discuss auto-scaling). GAE will then load additional
instances on demand until the specified maximum instances value is reached. By setting the
maximum, an application can put a lid on costs, albeit with the potential for increased latencies
if load continues to grow.
   Standard environment applications can be built in Go, Java, Python,  Node.js,  PHP, and
Ruby.  As GAE itself is responsible for loading the runtime environment for an application, it
restricts the supported versions  to a small number per programming language. The language
used also affects the time to load a new instance on GAE. For example, a lightweight runtime
environment such as Go will start on a new instance in less than a second. In comparison, a
heavier weight Java Virtual Machine is of the order of 1-3 seconds on average. This load time is
also influenced by the number of external libraries that the application incorporates.
    Hence, while there is variability across languages, loading new instances is uniformly fast.
Much faster than booting a virtual machine. This makes the standard environment extremely
well suited for applications that experience rapid spikes in load. GAE is able to quickly add new
resident instances as request volumes increase.  Requests are dynamically routed to instances
based on load, and hence assume a purely stateless application model to support effective load
distribution. Subsequently, instances are released with little delay once the load drops, again
reducing costs.
   GAE is an extremely powerful platform for scalable applications, and one well explore in
much more detail in the case study later in this chapter.

  GAE Flexible Environment

  AutoScaling
   Autoscaling is an option that you specify in the app.yaml file that is passed to GAE when
you upload your server code. An autoscaled application is managed by GAE according to a
collection of default parameter values, which you can override in your app.yaml. The basic
scheme is shown in Figure 1.
   GAE basically manages the number of deployed server instances for an application
based on incoming traffic load. If there are no incoming requests, then GAE will not
schedule any instances and you will pay nothing. When a request arrives, GAE deploys an
instance to process the request.
   Deploying an instance can take anything from a few 100 milliseconds to a few seconds
depending on the programming language you are using. This means latency can be high if
there are no resident instances. To mitigate the instance loading latency effects, you can
specify a minimum number of instances to keep available for processing requests. This of
course costs money.
   As the request load grows, the GAE scheduler will dynamically load more instances to
handle requests. Three parameters control precisely how this operates, namely:
   Target CPU Utilization:	Sets the CPU utilization threshold above which more
instances will be started to handle traffic. The range is 0.5 (50%) to 0.95 (95%). The default
is 0.6 (60%).
   Maximum Concurrent Requests: Sets the maximum number of concurrent requests an
instance can accept before the scheduler spawns a new instance. The default value is 10,
and the maximum is 80. The documentation doesnt state the minimum allowed value, but
presumably 1 would define a single-threaded service.
   Target Throughput Utilization:	This is used in conjunction with the value specified
for maximum concurrent requests to specify when a new instance is started. The range is
0.5 (50%) to 0.95 (95%). The default is 0.6 (60%). It works like this - when the number of
concurrent requests for an instance reaches a value equal to maximum concurrent requests
value multiplied by the target throughput utilization, the scheduler tries to start a new
instance.
   Got that? ??
   So by default, an instance will handle 10 x 0.6 = 6 concurrent requests before a new
instance is created. And if these 6 (or less) requests cause the CPU utilization for an
instance to go over 60%, the scheduler will also try and create a new instance.
   I think.

   Figure 1 GAE Autoscaling
   But wait, theres more!
   You can also specify values to control when GAE scales instances up based on the time
requests spend in the request pending queue, waiting to be dispatched to an instance for
processing. This maximum pending latency parameter is the maximum amount of time that
App Engine should allow a request to wait in the pending queue before starting additional
instances to handle requests to reduce latency. The default value is 30ms. The lower the
value, the quicker an application will scale up. And the more it will probably cost you.
   Theres also a minimum pending latency parameter, with a default value of zero. If you
are brave, how the minimum and maximum values work together is explained here.
   These autoscaling parameter settings give us the ability to fine tune a services behavior
to balance performance and cost. Which is great, isnt it?

AWS Lambda
Case Study: Balancing Scalability and Costs
  The Basic Conundrum
  Dont Accept the Defaults
  Choosing Parameter Values
  Comparing Configurations
  Lessons Learned
Summary and Further reading

?
ABOUT THE AUTHOR

   Insert author bio text here. Insert author bio text here Insert author bio text here Insert
author bio text here Insert author bio text here Insert author bio text here Insert author bio text

?
?
   Random words section
   Database connections for a single database are also a limitation. Each open connection takes
database resources, and even if we restrict each server accessing the database to opening 50
connections maximum, the 4000 available on our GCP SQL server will be exhausted when we
deploy 80 application servers. Were hitting limits in scalability for a single database instance.

   Its usually recommended that clients follow something akin to an exponential backoff
algorithm as they see errors. The client blocks for a brief initial wait time on the first failure, but
as the operation continues to fail, it waits proportionally to 2n, where n is the number of failures
that have occurred. By backing off exponentially, we can ensure that clients arent hammering
on a downed server and contributing to the problem.

   Exponential backoff has a long and interesting history in computer networking.

   Furthermore, its also a good idea to mix in an element of randomness. If a problem with a
server causes a large number of clients to fail at close to the same time, then even with back off,
their retry schedules could be aligned closely enough that the retries will hammer the troubled
server. This is known as the thundering herd problem.

   We can address thundering herd by adding some amount of random jitter to each clients
wait time. This will space out requests across all clients, and give the server some breathing
room to recover.

   Another solution is to keep the per-session data in a database. Generally, this is bad for
performance because it increases the load on the database: the database is best used to store
information less transient than per-session data. To prevent a database from becoming a single
point of failure, and to improve scalability, the database is often replicated across multiple
machines, and load balancing is used to spread the query load across those replicas. Microsoft's
ASP.net State Server technology is an example of a session database. All servers in a web farm
store their session data on State Server and any server in the farm can retrieve the data.

       https://www.youtube.com/watch?v=eNliOm9NtCM
       https://engineering.fb.com/data-infrastructure/scribe/
       https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-
in-a-single-repository/fulltext
      https://www.pornhub.com/insights/2019-year-in-review
       https://kogo.iheart.com/content/2020-04-23-youtube-celebrates-15th-anniversary-by-
featuring-first-video-ever-posted/
       http://pcmuseum.tripod.com/comphis4.html
       https://www.internetsociety.org/internet/history-internet/brief-history-internet/
       https://en.wikipedia.org/wiki/Usenet
       https://en.wikipedia.org/wiki/History_of_the_World_Wide_Web
       https://www.nngroup.com/articles/100-million-websites/
       https://en.wikipedia.org/wiki/Dot-com_bubble
       https://www.internetworldstats.com/stats.htm
       https://blog.twitter.com/engineering/en_us/topics/infrastructure/2017/the-infrastructure-
behind-twitter-scale.html
       https://en.wikipedia.org/wiki/Sydney_Harbour_Bridge
       https://en.wikipedia.org/wiki/Sydney_Harbour_Tunnel
       https://en.wikipedia.org/wiki/Auckland_Harbour_Bridge
       https://en.wikipedia.org/wiki/Auckland_Harbour_Bridge#'Nippon_clip-ons'
       https://en.wikipedia.org/wiki/HipHop_for_PHP
       https://www.bloomberg.com/news/articles/2014-09-24/obamacare-website-costs-exceed-2-
billion-study-finds
       http://www.informationweek.com/healthcare/policy-and-regulation/oregon-dumps-failed-
health-insurance-exchange/d/d-id/1234875

       https://www.researchgate.net/publication/318049054_Chapter_2_Hyperscalability_-
_The_Changing_Face_of_Software_Architecture
       https://en.wikipedia.org/wiki/Flask_(web_framework)
       Mark Richards and Neal Ford, Fundamentals of Software Architecture: An Engineering
Approach 1st Edition, OReilly Media, 2020
       https://aws.amazon.com/ec2/instance-types/
       https://en.wikipedia.org/wiki/Reverse_proxy
       https://redis.io/
       https://memcached.org/
       https://www.allthingsdistributed.com/2019/08/modern-applications-at-aws.html
       https://samnewman.io/patterns/architectural/bff/
       https://en.wikipedia.org/wiki/Intel_80386
       https://en.wikipedia.org/wiki/Amdahl%27s_law
       https://en.wikipedia.org/wiki/Dining_philosophers_problem
       https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/package-summary.html
       Except Vector and HashTable, which are legacy classes, thread safe and slow!
       https://en.wikipedia.org/wiki/Wavelength-
division_multiplexing#:~:text=In%20fiber%2Doptic%20communications%2C%20wavelength,%2C%20
colors)%20of%20laser%20light.
       https://en.wikipedia.org/wiki/Core_router
       https://en.wikipedia.org/wiki/Tier_1_network#List_of_Tier_1_networks
       https://www.google.com/intl/en/ipv6/statistics.html
       http://www.opengroup.org/dce/
       http://www.corba.org
       https://docs.oracle.com/javase/9/docs/specs/rmi/index.html
       http://erlang.org/doc/man/rpc.html
       https://golang.org/pkg/net/rpc/
       Fielding, Roy Thomas (2000). "Architectural Styles and the Design of Network-based Software
Architectures". Dissertation. University of California, Irvine.
       https://en.wikipedia.org/wiki/Packet_loss
  https://medium.com/baseds/modes-of-failure-part-1-6687504bfed6#
       https://en.wikipedia.org/wiki/Two_Generals%27_Problem
       Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson. 1985. Impossibility of distributed
consensus with one faulty process. J. ACM 32, 2 (April 1985), 374382.
       https://medium.com/@chrshmmmr/consensus-in-blockchain-systems-in-short-691fc7d1fefe
       https://ieeexplore.ieee.org/abstract/document/8123011
       www.ntp.org

       https://chrony.tuxfamily.org/
       https://engineering.fb.com/production-engineering/ntp-service/
       https://en.wikipedia.org/wiki/Roy_Fielding
       https://app.swaggerhub.com/help/tutorials/openapi-3-tutorial
       https://app.swaggerhub.com
       Node.js is a notable exception here as it is single threaded. However, it employs an
asynchronous programming model for blocking I-Os that supports handling many simultaneous
requests.
       https://tools.ietf.org/html/rfc6265
       https://www.oracle.com/java/technologies/java-ee-glance.html
       https://expressjs.com/
       https://palletsprojects.com/p/flask/
       http://tomcat.apache.org/
       See https://tomcat.apache.org/tomcat-9.0-doc/config/executor.html for default Tomcat
Executor configuration settings
       https://en.wikipedia.org/wiki/Java_Management_Extensions
       https://en.wikipedia.org/wiki/JConsole
       https://github.com/javamelody/javamelody/wiki
       https://en.wikipedia.org/wiki/OSI_model
       http://cbonte.github.io/haproxy-dconv/2.3/intro.html#3.3.5
       http://cbonte.github.io/haproxy-dconv/2.3/intro.html#3.3.6
       https://www.nginx.com/blog/microservices-at-netflix-architectural-best-practices/
       https://blog.twitter.com/engineering/en_us/topics/infrastructure/2017/the-infrastructure-
behind-twitter-scale.html
       https://memcached.org/
       https://redis.io/
       https://www.ehcache.org/documentation/3.3/caching-patterns.html#cache-aside
       https://www.alachisoft.com/resources/docs/ncache/prog-guide/server-side-api-
programming.html
        https://aws.amazon.com/dynamodb/dax/
       http://www.squid-cache.org/
       https://varnish-cache.org/
       https://cloud.google.com/firestore, formerly known as Google Cloud DataStore.
       https://cloud.google.com/sql/docs
       https://cloud.google.com/pubsub
       https://cloud.google.com/appengine/docs/the-appengine-environments